{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and plot basic investigation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import SWOT_Step_2_make_files as step_2\n",
    "import SOFiA as SOFiA\n",
    "import cmocean\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python packages\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy as cy\n",
    "from matplotlib.dates import DateFormatter\n",
    "import cartopy.crs as ccrs \n",
    "import cmocean\n",
    "import netCDF4 as nc\n",
    "import glob as glob\n",
    "import pandas as pd\n",
    "from scipy.interpolate import griddata\n",
    "# Binning (Jonathan)\n",
    "from scipy.stats import binned_statistic_2d\n",
    "import cartopy.feature as cfeature\n",
    "import cmcrameri as cmc\n",
    "# Other\n",
    "from scipy.fft import fft, fftfreq\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.linalg import solve\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import ftplib\n",
    "from getpass import getpass\n",
    "#import cartopy.crs as ccrs\n",
    "import cartopy.feature as cft\n",
    "import cartopy.mpl.geoaxes as cmplgeo\n",
    "import cartopy.mpl.gridliner as cmplgrid\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import SWOT_Step_2_make_files as step_2\n",
    "import SOFiA as SOFiA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' FUNCTIONS PART 1 : '''\n",
    "'''  1) Count of no-nan in ssh_karin and ssh_karin_2 '''                                             # DIMS: (num_lines,num_pixels)\n",
    "def count_no_nan(ds, var_type, correction_type, mask_type, smoothing):\n",
    "    #ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    #masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables_C0_C2_all_own_ssha(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    ''' NUMBER OF GOOD DATA POINTS : '''\n",
    "    count = np.sum(~np.isnan(masked_d), axis=0)\n",
    "    return count, lat, lon\n",
    "'''  2) Standard deviation of ssh_karin_2 without geoid (still tides in it) '''                      # DIMS: (num_lines,num_pixels)\n",
    "def standard_deviation(ds, var_type, correction_type, mask_type, smoothing):\n",
    "    #ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    #masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables_C0_C2_all_own_ssha(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    ''' STANDARD DEVIATION OF VARIABLE : '''\n",
    "    std = np.nanstd(masked_d, axis=0)\n",
    "    return std, lat, lon\n",
    "'''  3) Mean (temporal over cycles) of ssh_karin(_2) removed geoid and tides and dac --> MDT '''     # DIMS: (num_lines,num_pixels)\n",
    "def temporal_nanmean(ds, var_type, correction_type, mask_type, smoothing):\n",
    "    #ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    #masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables_C0_C2_all_own_ssha(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    ''' MEAN OF VARIABLE : '''\n",
    "    mean = np.nanmean(masked_d, axis=0)\n",
    "    return mean, lat, lon\n",
    "'''  4) Mean of slope (dSSH/dx) of MDT (over) --> Magnitude = sqrt(x² + y²) '''                      # DIMS: (num_lines,num_pixels)\n",
    "def mean_slope_ssh(ds, var_type, correction_type, mask_type, smoothing):\n",
    "    #ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    #masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables_C0_C2_all_own_ssha(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    ''' GET SLA_DX AND SLA_DY : '''\n",
    "    sla_diff_x = np.gradient(masked_d, axis=2)\n",
    "    sla_diff_y = np.gradient(masked_d, axis=1)\n",
    "    ''' dx, dy : '''\n",
    "    dx = 2000  # m\n",
    "    dy = 2000  # m \n",
    "    ''' SLOPE : '''\n",
    "    slope_x = (sla_diff_x / dx)\n",
    "    slope_y = (sla_diff_y / dy)\n",
    "    ''' MAGNITUDE OF SLOPE : '''\n",
    "    magnitude = np.sqrt(slope_x**2 + slope_y**2)\n",
    "    ''' MEAN OF CYCLES : '''\n",
    "    mean_magnitude = np.nanmean(magnitude, axis=0)\n",
    "    ''' NORMALIZED ? '''\n",
    "    # Make bigger to avoid inf : \n",
    "    #mean_to_norm = mean_magnitude*1e6\n",
    "    #mean_magnitude_norm = (mean_to_norm-np.nanmin(mean_to_norm))/(np.nanmax(mean_to_norm)-np.nanmin(mean_to_norm))\n",
    "    return mean_magnitude, lat, lon\n",
    "'''  5) Mean of slope (dH/dx) of depth_or_elevation --> Magnitude = sqrt(x² + y²) '''                # DIMS: (num_lines,num_pixels)\n",
    "def mean_slope_bath(ds):\n",
    "    #ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    ''' Mask only Ocean data : '''\n",
    "    good_data = ds['ancillary_surface_classification_flag'] > 1\n",
    "    # GET variable : \n",
    "    variable = ds['depth_or_elevation']\n",
    "    ''' GET SLA_DX AND SLA_DY : '''\n",
    "    sla_diff_x = np.gradient(variable, axis=2)\n",
    "    sla_diff_y = np.gradient(variable, axis=1)\n",
    "    ''' dx, dy : '''\n",
    "    dx = 2000  # m\n",
    "    dy = 2000  # m \n",
    "    ''' SLOPE : '''\n",
    "    slope_x = (sla_diff_x / dx)\n",
    "    slope_y = (sla_diff_y / dy)\n",
    "    ''' MAGNITUDE OF SLOPE . '''\n",
    "    magnitude = np.sqrt(slope_x**2 + slope_y**2)\n",
    "    # Apply the mask :\n",
    "    masked_1 = np.ma.masked_where(good_data, magnitude)\n",
    "    masked   = np.ma.filled(masked_1, np.nan)\n",
    "    ''' MEAN OF CYCLES : '''\n",
    "    mean_magnitude = np.nanmean(masked, axis=0)\n",
    "    ''' NORMALIZED ? '''\n",
    "    mean_magnitude_norm = (mean_magnitude-np.nanmin(mean_magnitude))/(np.nanmax(mean_magnitude)-np.nanmin(mean_magnitude))\n",
    "    return mean_magnitude, mean_magnitude_norm\n",
    "\n",
    "#ds.mean_dynamic_topography[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CREATE VARIABLES PART 1 : '''\n",
    "def part_1_variables(ds):\n",
    "    count_ssh_karin, lat, lon               = count_no_nan(ds,          var_type='ssh_karin',   correction_type='ssh',      mask_type='all', smoothing=None)\n",
    "    count_ssh_karin_2, lat, lon             = count_no_nan(ds,          var_type='ssh_karin_2', correction_type='ssh',      mask_type='all', smoothing=None)\n",
    "    std_ssh_karin_2, lat, lon               = standard_deviation(ds,    var_type='ssh_karin_2', correction_type='ssh',      mask_type='all', smoothing=None)\n",
    "    mean_ssh_karin_2_tides, lat, lon        = temporal_nanmean(ds,      var_type='ssh_karin_2', correction_type='ssh_tide', mask_type='all', smoothing=None)\n",
    "    mean_slope_ssh_karin_2_tides, lat, lon  = mean_slope_ssh(ds,        var_type='ssh_karin_2', correction_type='ssh_tide', mask_type='all', smoothing=None)\n",
    "    mean_slope_bathy, mean_slope_bathy_norm = mean_slope_bath(ds)\n",
    "    mean_dynamic_topography_model = ds.mean_dynamic_topography[0]\n",
    "    ''' Fix the Longitude : '''\n",
    "    new_lon = np.where(lon > 180, lon - 360, lon)\n",
    "    return lat, new_lon, count_ssh_karin, count_ssh_karin_2, std_ssh_karin_2, mean_ssh_karin_2_tides, mean_slope_ssh_karin_2_tides, mean_slope_bathy, mean_slope_bathy_norm, mean_dynamic_topography_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_1_NetCDF(path_merged,pass_nr):\n",
    "    ''' OPEN DS : '''\n",
    "    ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    ''' GET VARIABLES : '''\n",
    "    lat, new_lon, count_ssh_karin, count_ssh_karin_2, std_ssh_karin_2, mean_ssh_karin_2_tides, mean_slope_ssh_karin_2_tides, mean_slope_bathy, mean_slope_bathy_norm, mean_dynamic_topography_model = part_1_variables(ds)\n",
    "    ''' Num_lines, Num_pixels '''\n",
    "    num_lines  = lat.shape[0]\n",
    "    num_pixels = lat.shape[1]\n",
    "    ''' CREATE DataArrays '''\n",
    "    count1   = xr.DataArray(count_ssh_karin,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    count2   = xr.DataArray(count_ssh_karin_2,             dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    std2     = xr.DataArray(std_ssh_karin_2,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    mean2    = xr.DataArray(mean_ssh_karin_2_tides,        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    slope2   = xr.DataArray(mean_slope_ssh_karin_2_tides,  dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    slopeb   = xr.DataArray(mean_slope_bathy,              dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    slopeb2  = xr.DataArray(mean_slope_bathy_norm,         dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    mdt      = xr.DataArray(mean_dynamic_topography_model, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    latc     = xr.DataArray(lat,                           dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    new_lonc = xr.DataArray(new_lon,                       dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ''' CREATE DataSet '''\n",
    "    dataset  = xr.Dataset({'count_ssh_karin'               : count1, \n",
    "                           'count_ssh_karin_2'             : count2, \n",
    "                           'std_ssh_karin_2'               : std2, \n",
    "                           'mean_ssh_karin_2_tides'        : mean2, \n",
    "                           'mean_slope_ssh_karin_2_tides'  : slope2, \n",
    "                           'mean_slope_bathy'              : slopeb, \n",
    "                           'mean_slope_bathy_norm'         : slopeb2, \n",
    "                           'mean_dynamic_topography_model' : mdt,\n",
    "                           'latitude'                      : latc, \n",
    "                           'longitude'                     : new_lonc})\n",
    "    ''' RETURN THE DataSet '''\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH : \n",
    "path_merged = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SWOT_1_RAW_MERGED/version_2/'\n",
    "pass_nr = 'pass_003'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(path_merged+pass_nr+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.concat_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = part_1_NetCDF(path_merged,pass_nr)\n",
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataset1.mean_dynamic_topography_model, vmin=-0.1, vmax=0.1)\n",
    "plt.ylim(500)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(10, 10))\n",
    "a=axs[0, 0].imshow(dataset1.count_ssh_karin)\n",
    "axs[0, 0].set_title('count_ssh_karin')\n",
    "axs[0, 0].set_ylim(200)\n",
    "plt.colorbar(a)\n",
    "\n",
    "b=axs[0, 1].imshow(dataset1.count_ssh_karin_2)\n",
    "axs[0, 1].set_title('count_ssh_karin_2')\n",
    "axs[0, 1].set_ylim(200)\n",
    "plt.colorbar(b)\n",
    "\n",
    "c=axs[0, 2].imshow(dataset1.std_ssh_karin_2, vmin=0.5, vmax=0.8)\n",
    "axs[0, 2].set_title('std_ssh_karin_2')\n",
    "axs[0, 2].set_ylim(200)\n",
    "plt.colorbar(c)\n",
    "\n",
    "d=axs[1, 0].imshow(dataset1.mean_ssh_karin_2_tides, vmin=-0.3, vmax=0.3, cmap=cmocean.cm.balance)\n",
    "axs[1, 0].set_title('mean_ssh_karin_2_tides')\n",
    "axs[1, 0].set_ylim(200)\n",
    "plt.colorbar(d)\n",
    "\n",
    "e=axs[1, 1].imshow(dataset1.mean_slope_ssh_karin_2_tides, vmin=0, vmax=0.00001, cmap=cmocean.cm.balance)\n",
    "axs[1, 1].set_title('mean_slope_ssh_karin_2_tides')\n",
    "axs[1, 1].set_ylim(200)\n",
    "plt.colorbar(e)\n",
    "\n",
    "f=axs[1, 2].imshow(dataset1.mean_slope_bathy)\n",
    "axs[1, 2].set_title('mean_slope_bathy')\n",
    "axs[1, 2].set_ylim(200)\n",
    "plt.colorbar(f)\n",
    "\n",
    "g=axs[2, 0].imshow(dataset1.mean_slope_bathy_norm)\n",
    "axs[2, 0].set_title('mean_slope_bathy_norm')\n",
    "axs[2, 0].set_ylim(200)\n",
    "plt.colorbar(g)\n",
    "\n",
    "h=axs[2, 1].imshow(dataset1.latitude, vmin=45, vmax=50)\n",
    "axs[2, 1].set_title('latitude')\n",
    "axs[2, 1].set_ylim(200)\n",
    "plt.colorbar(h)\n",
    "\n",
    "i=axs[2, 2].imshow(dataset1.longitude, vmin=-20, vmax=-15)\n",
    "axs[2, 2].set_title('longitude')\n",
    "axs[2, 2].set_ylim(200)\n",
    "plt.colorbar(i)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' FUNCTIONS PART 2 : '''\n",
    "'''  6) Standard deviation of SSHA '''                                                               # DIMS: (num_lines,num_pixels)\n",
    "def standard_deviation(ds, var_type, correction_type, mask_type, smoothing):\n",
    "    #ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    #masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables_C0_C2_all_own_ssha(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    ''' STANDARD DEVIATION OF VARIABLE : '''\n",
    "    std = np.nanstd(masked_d, axis=0)\n",
    "    return std, lat, lon\n",
    "'''  7) Mean (temporal over cycles) of ssha_karin_2 '''                                              # DIMS: (num_lines,num_pixels)\n",
    "def temporal_nanmean(ds, var_type, correction_type, mask_type, smoothing):\n",
    "    #ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    #masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables_C0_C2_all_own_ssha(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    ''' MEAN OF VARIABLE : '''\n",
    "    mean = np.nanmean(masked_d, axis=0)\n",
    "    return mean, lat, lon\n",
    "#'''  8) mean_dynamic_topography '''                                                                  # DIMS: (num_lines,num_pixels)\n",
    "#ds.mean_dynamic_topography[0]\n",
    "'''  9) EKE removed mean (SSHA) '''                                                                  # DIMS: (num_lines,num_pixels)\n",
    "def calculate_ug_vg(masked_dataset, lat, lon):\n",
    "    ''' GET SLA_DX AND SLA_DY : '''\n",
    "    sla_diff_x = np.gradient(masked_dataset, axis=2)\n",
    "    sla_diff_y = np.gradient(masked_dataset, axis=1)\n",
    "    ''' dx, dy : '''\n",
    "    dx = 2000\n",
    "    dy = 2000\n",
    "    ''' Tetha, g, f : '''\n",
    "    tetha = np.deg2rad(lat)\n",
    "    g     = 9.81\n",
    "    f     = 2 * 7.2921e-5 * np.sin(tetha)\n",
    "    ''' ug, vg : '''\n",
    "    ug  = - (g / f) * (sla_diff_y / dy)\n",
    "    vg  =   (g / f) * (sla_diff_x / dx)\n",
    "    ''' RETURN UG AND VG : '''\n",
    "    return ug, vg\n",
    "def eke_normal(ds, var_type, correction_type, mask_type, smoothing):\n",
    "    #ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    #masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables_C0_C2_all_own_ssha(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    ''' CALCULATE UG AND VG : '''\n",
    "    ug, vg = calculate_ug_vg(masked_d, lat, lon)\n",
    "    ''' MEAN OF ug AND vg : '''\n",
    "    ug_mean = np.nanmean(ug, axis=0)\n",
    "    vg_mean = np.nanmean(vg, axis=0)\n",
    "    ''' EKE : '''\n",
    "    ''' METHOD 1 : REMOVE MEAN BEFORE SQUARING : '''\n",
    "    # EKE Jonathan : EKE = 1/2*[<(u-<u>)^2>+<(v-<v>)^2>] \n",
    "    ug_diff_mean = ug - ug_mean\n",
    "    vg_diff_mean = vg - vg_mean\n",
    "    ug_diff_sq = ug_diff_mean**2\n",
    "    vg_diff_sq = vg_diff_mean**2\n",
    "    eke_jon = 0.5 * (np.nanmean(ug_diff_sq, axis=0) + np.nanmean(vg_diff_sq, axis=0))\n",
    "    return eke_jon, lat, lon \n",
    "''' 10) EKE not removed mean (SSHA) '''                                                              # DIMS: (num_lines,num_pixels)\n",
    "def eke_normal_not_rem_mean(ds, var_type, correction_type, mask_type, smoothing):\n",
    "    #ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    #masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables_C0_C2_all_own_ssha(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    ''' CALCULATE UG AND VG : '''\n",
    "    ug, vg = calculate_ug_vg(masked_d, lat, lon)\n",
    "    ''' EKE : '''\n",
    "    eke_not_removed   = 0.5 * (np.nanmean(ug**2, axis=0) + np.nanmean(vg**2, axis=0))\n",
    "    return eke_not_removed, lat, lon \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CREATE VARIABLES PART 2 : '''\n",
    "def part_2_variables(ds):\n",
    "    std_ssha_karin_2, lat, lon          = standard_deviation(ds,        var_type='ssh_karin_2', correction_type='ssha_own', mask_type='all', smoothing=2)\n",
    "    mean_ssha_karin_2, lat, lon         = temporal_nanmean(ds,          var_type='ssh_karin_2', correction_type='ssha_own', mask_type='all', smoothing=2)\n",
    "    eke_ssha_karin_2_normal, lat, lon   = eke_normal(ds,                var_type='ssh_karin_2', correction_type='ssha_own', mask_type='all', smoothing=2)\n",
    "    eke_ssha_karin_2_option_2, lat, lon = eke_normal_not_rem_mean(ds,   var_type='ssh_karin_2', correction_type='ssha_own', mask_type='all', smoothing=2)\n",
    "    ''' Fix the Longitude : '''\n",
    "    new_lon = np.where(lon > 180, lon - 360, lon)\n",
    "    return lat, new_lon, std_ssha_karin_2, mean_ssha_karin_2, eke_ssha_karin_2_normal, eke_ssha_karin_2_option_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_2_NetCDF(path_merged,pass_nr):\n",
    "    ''' OPEN DS : '''\n",
    "    ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    ''' GET VARIABLES : '''\n",
    "    lat, new_lon, std_ssha_karin_2, mean_ssha_karin_2, eke_ssha_karin_2_normal, eke_ssha_karin_2_option_2 = part_2_variables(ds)\n",
    "    ''' Num_lines, Num_pixels '''\n",
    "    num_lines  = lat.shape[0]\n",
    "    num_pixels = lat.shape[1]\n",
    "    ''' CREATE DataArrays '''\n",
    "    std   = xr.DataArray(std_ssha_karin_2,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    mean   = xr.DataArray(mean_ssha_karin_2,             dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    eke1     = xr.DataArray(eke_ssha_karin_2_normal,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    eke2    = xr.DataArray(eke_ssha_karin_2_option_2,        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    latc     = xr.DataArray(lat,                           dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    new_lonc = xr.DataArray(new_lon,                       dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ''' CREATE DataSet '''\n",
    "    dataset  = xr.Dataset({'std_ssha_karin_2'              : std, \n",
    "                           'mean_ssha_karin_2'             : mean, \n",
    "                           'eke_ssha_karin_2_normal'       : eke1, \n",
    "                           'eke_ssha_karin_2_option_2'     : eke2, \n",
    "                           'latitude'                      : latc, \n",
    "                           'longitude'                     : new_lonc})\n",
    "    ''' RETURN THE DataSet '''\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH : \n",
    "path_merged = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SWOT_1_RAW_MERGED/version_2/'\n",
    "pass_nr = 'pass_003'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = part_2_NetCDF(path_merged,pass_nr)\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(10, 10))\n",
    "a=axs[0, 0].imshow(dataset2.std_ssha_karin_2, vmin=0, vmax=0.2)\n",
    "axs[0, 0].set_title('std_ssha_karin_2')\n",
    "axs[0, 0].set_ylim(200)\n",
    "plt.colorbar(a)\n",
    "\n",
    "b=axs[0, 1].imshow(dataset2.mean_ssha_karin_2, vmin=-0.3, vmax=0.3, cmap=cmocean.cm.balance)\n",
    "axs[0, 1].set_title('mean_ssha_karin_2')\n",
    "axs[0, 1].set_ylim(200)\n",
    "plt.colorbar(b)\n",
    "\n",
    "c=axs[0, 2].imshow(dataset2.eke_ssha_karin_2_normal, vmin=0, vmax=0.3, cmap=cmocean.cm.matter)\n",
    "axs[0, 2].set_title('eke_ssha_karin_2_normal')\n",
    "axs[0, 2].set_ylim(200)\n",
    "plt.colorbar(c)\n",
    "\n",
    "d=axs[1, 0].imshow(dataset2.eke_ssha_karin_2_option_2, vmin=0, vmax=0.3, cmap=cmocean.cm.matter)\n",
    "axs[1, 0].set_title('eke_ssha_karin_2_option_2')\n",
    "axs[1, 0].set_ylim(200)\n",
    "plt.colorbar(d)\n",
    "\n",
    "e=axs[1, 1].imshow(dataset2.latitude, vmin=45, vmax=50)\n",
    "axs[1, 1].set_title('latitude')\n",
    "axs[1, 1].set_ylim(200)\n",
    "plt.colorbar(e)\n",
    "\n",
    "f=axs[1, 2].imshow(dataset2.longitude, vmin=-20, vmax=-15)\n",
    "axs[1, 2].set_title('longitude')\n",
    "axs[1, 2].set_ylim(200)\n",
    "plt.colorbar(f)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 3 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' FUNCTIONS PART 3 : '''\n",
    "''' 11) Mean ssha Winter-Summer '''                                                                # DIMS: (num_lines,num_pixels)\n",
    "def temporal_nanmean_season(masked_d, lat, lon, Winter_24, Summer_24):#ds, var_type, correction_type, mask_type, smoothing):\n",
    "    #ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    #masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    ''' GET WINTER 24 AND SUMMER 24 : '''\n",
    "    w_24 = masked_d[Winter_24]\n",
    "    s_24 = masked_d[Summer_24]\n",
    "    ''' MEAN OF VARIABLE : '''\n",
    "    mean_w_24 = np.nanmean(w_24, axis=0)\n",
    "    mean_s_24 = np.nanmean(s_24, axis=0)\n",
    "    ''' Winter-Summer : '''\n",
    "    mean = mean_w_24-mean_s_24\n",
    "    return mean, lat, lon\n",
    "''' 12) ug and vg (rotated) Winter-Summer --> speed? '''                                             # DIMS: (num_lines,num_pixels)\n",
    "def calculate_ug_vg(masked_dataset, lat, lon):\n",
    "    ''' GET SLA_DX AND SLA_DY : '''\n",
    "    sla_diff_x = np.gradient(masked_dataset, axis=2)\n",
    "    sla_diff_y = np.gradient(masked_dataset, axis=1)\n",
    "    ''' dx, dy : '''\n",
    "    dx = 2000\n",
    "    dy = 2000\n",
    "    ''' Tetha, g, f : '''\n",
    "    tetha = np.deg2rad(lat)\n",
    "    g     = 9.81\n",
    "    f     = 2 * 7.2921e-5 * np.sin(tetha)\n",
    "    ''' ug, vg : '''\n",
    "    ug  = - (g / f) * (sla_diff_y / dy)\n",
    "    vg  =   (g / f) * (sla_diff_x / dx)\n",
    "    ''' RETURN UG AND VG : '''\n",
    "    return ug, vg\n",
    "def rotate_dataset_original_grid_all_cycles(lat, lon, u, v):\n",
    "    TETHA = lon\n",
    "    PHI   = lat\n",
    "    num_lines  = PHI.shape[0]\n",
    "    num_pixels = PHI.shape[1]\n",
    "    TETHA      = np.where(TETHA > 180, TETHA - 360, TETHA)\n",
    "    # because num_lines is y and num_pixels is x\n",
    "    delta_PHI        = np.diff(PHI,   axis=1)\n",
    "    delta_TETHA      = np.diff(TETHA, axis=1)\n",
    "    delta_PHI        = np.append(delta_PHI, np.zeros((num_lines, 1)), axis=1)\n",
    "    delta_TETHA      = np.append(delta_TETHA, np.zeros((num_lines, 1)), axis=1)\n",
    "    tetha_rad        = np.deg2rad(TETHA)\n",
    "    phi_rad          = np.deg2rad(PHI)\n",
    "    delta_tetha_rad  = np.deg2rad(delta_TETHA)\n",
    "    delta_phi_rad    = np.deg2rad(delta_PHI)\n",
    "    cos_PHI          = np.cos(phi_rad)\n",
    "    curly_TETHA      = np.arctan2(delta_phi_rad,cos_PHI*delta_tetha_rad)\n",
    "    u_rotated        = u*np.cos(curly_TETHA) - v*np.sin(curly_TETHA)\n",
    "    v_rotated        = u*np.sin(curly_TETHA) + v*np.cos(curly_TETHA)\n",
    "    u_rotated_by_phi = u_rotated/cos_PHI\n",
    "    return u_rotated, v_rotated\n",
    "def ug_vg_season(masked_d, lat, lon, Winter_24, Summer_24):#ds, var_type, correction_type, mask_type, smoothing):\n",
    "    #ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    #masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    w_24 = masked_d[Winter_24]\n",
    "    s_24 = masked_d[Summer_24]\n",
    "    ''' MEAN OF VARIABLE : '''\n",
    "    mean_w_24 = np.nanmean(w_24, axis=0)\n",
    "    mean_s_24 = np.nanmean(s_24, axis=0)\n",
    "    ''' Winter-Summer : '''\n",
    "    mean = mean_w_24-mean_s_24\n",
    "    ''' CALCULATE UG AND VG : '''\n",
    "    sla_diff_x = np.gradient(mean, axis=1)\n",
    "    sla_diff_y = np.gradient(mean, axis=0)\n",
    "    ''' dx, dy : '''\n",
    "    dx = 2000\n",
    "    dy = 2000\n",
    "    ''' Tetha, g, f : '''\n",
    "    tetha = np.deg2rad(lat)\n",
    "    g     = 9.81\n",
    "    f     = 2 * 7.2921e-5 * np.sin(tetha)\n",
    "    ''' ug, vg : '''\n",
    "    ug_w_s  = - (g / f) * (sla_diff_y / dy)\n",
    "    vg_w_s  =   (g / f) * (sla_diff_x / dx)\n",
    "    ''' CALCULATE UG AND VG : '''\n",
    "    ug, vg = calculate_ug_vg(masked_d, lat, lon)\n",
    "    ''' GET THE SEASONAL INDICES : '''\n",
    "    ug_w = ug[Winter_24]\n",
    "    vg_w = vg[Winter_24]\n",
    "    ug_s = ug[Summer_24]\n",
    "    vg_s = vg[Summer_24]\n",
    "    ''' MEAN OF ug AND vg  SEASONAL : '''\n",
    "    ug_w_mean = np.nanmean(ug_w, axis=0)\n",
    "    vg_w_mean = np.nanmean(vg_w, axis=0)\n",
    "    ug_s_mean = np.nanmean(ug_s, axis=0)\n",
    "    vg_s_mean = np.nanmean(vg_s, axis=0)\n",
    "    ''' SPEED : '''\n",
    "    speed_w = np.sqrt(ug_w_mean**2 + vg_w_mean**2)\n",
    "    speed_s = np.sqrt(ug_s_mean**2 + vg_s_mean**2)\n",
    "    ''' Winter-Summer : '''\n",
    "    #ug_w_s = ug_w_mean-ug_s_mean\n",
    "    #vg_w_s = vg_w_mean-vg_s_mean\n",
    "    ''' ROTATE THEM : '''\n",
    "    ug_w_s_r, vg_w_s_r = rotate_dataset_original_grid_all_cycles(lat, lon, ug_w_s, vg_w_s)\n",
    "    ''' SPEED : '''\n",
    "    #speed = np.sqrt(ug_w_s_r**2 + vg_w_s_r**2)\n",
    "    speed = speed_w-speed_s\n",
    "    return speed, ug_w_s_r, vg_w_s_r, lat, lon\n",
    "''' 13) EKE Winter-Summer '''                                                                        # DIMS: (num_lines,num_pixels)\n",
    "def eke_season(masked_d, lat, lon, Winter_24, Summer_24):#ds, var_type, correction_type, mask_type, smoothing):\n",
    "    #ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    #masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    ''' CALCULATE UG AND VG : '''\n",
    "    ug, vg = calculate_ug_vg(masked_d, lat, lon)\n",
    "    ''' GET THE SEASONAL INDICES : '''\n",
    "    ug_w = ug[Winter_24]\n",
    "    vg_w = vg[Winter_24]\n",
    "    ug_s = ug[Summer_24]\n",
    "    vg_s = vg[Summer_24]\n",
    "    ''' MEAN OF ug AND vg  SEASONAL : '''\n",
    "    ug_w_mean = np.nanmean(ug_w, axis=0)\n",
    "    vg_w_mean = np.nanmean(vg_w, axis=0)\n",
    "    ug_s_mean = np.nanmean(ug_s, axis=0)\n",
    "    vg_s_mean = np.nanmean(vg_s, axis=0)\n",
    "    ''' EKE : '''\n",
    "    # WINTER : \n",
    "    ug_diff_mean_w = ug_w - ug_w_mean\n",
    "    vg_diff_mean_w = vg_w - vg_w_mean\n",
    "    ug_diff_sq_w = ug_diff_mean_w**2\n",
    "    vg_diff_sq_w = vg_diff_mean_w**2\n",
    "    eke_jon_w = 0.5 * (np.nanmean(ug_diff_sq_w, axis=0) + np.nanmean(vg_diff_sq_w, axis=0))\n",
    "    # SUMMER : \n",
    "    ug_diff_mean_s = ug_s - ug_s_mean\n",
    "    vg_diff_mean_s = vg_s - vg_s_mean\n",
    "    ug_diff_sq_s = ug_diff_mean_s**2\n",
    "    vg_diff_sq_s = vg_diff_mean_s**2\n",
    "    eke_jon_s = 0.5 * (np.nanmean(ug_diff_sq_s, axis=0) + np.nanmean(vg_diff_sq_s, axis=0))\n",
    "    ''' Winter-Summer : '''\n",
    "    eke_seasonal = eke_jon_w-eke_jon_s\n",
    "    return eke_seasonal, lat, lon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CREATE VARIABLES PART 3 : '''\n",
    "def part_3_variables(ds):\n",
    "    #masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type='ssha_karin_2', correction_type='ssha', mask_type='all', smoothing=None)\n",
    "    #masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type='ssha_karin_2', correction_type='ssha', mask_type='all', smoothing=1)\n",
    "    masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables_C0_C2_all_own_ssha(ds, var_type='ssh_karin_2', correction_type='ssha_own', mask_type='all', smoothing=2)\n",
    "    mean_ssha_karin_2_W_S, lat, lon                      = temporal_nanmean_season(masked_d, lat, lon, Winter_24, Summer_24)#ds,  var_type='ssha_karin_2', correction_type='ssha', mask_type='all', smoothing=None)\n",
    "    speed_ssha_karin_2_W_S, ug_w_s_r, vg_w_s_r, lat, lon = ug_vg_season(masked_d, lat, lon, Winter_24, Summer_24)#ds,             var_type='ssha_karin_2', correction_type='ssha', mask_type='all', smoothing=None)\n",
    "    eke_ssha_karin_2_W_S, lat, lon                       = eke_season(masked_d, lat, lon, Winter_24, Summer_24)#ds,               var_type='ssha_karin_2', correction_type='ssha', mask_type='all', smoothing=None)\n",
    "    ''' Fix the Longitude : '''\n",
    "    new_lon = np.where(lon > 180, lon - 360, lon)\n",
    "    return lat, new_lon, mean_ssha_karin_2_W_S, speed_ssha_karin_2_W_S, ug_w_s_r, vg_w_s_r, eke_ssha_karin_2_W_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_3_NetCDF(path_merged,pass_nr):\n",
    "    ''' OPEN DS : '''\n",
    "    ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    ''' GET VARIABLES : '''\n",
    "    lat, new_lon, mean_ssha_karin_2_W_S, speed_ssha_karin_2_W_S, ug_w_s_r, vg_w_s_r, eke_ssha_karin_2_W_S = part_3_variables(ds)\n",
    "    ''' Num_lines, Num_pixels '''\n",
    "    num_lines  = lat.shape[0]\n",
    "    num_pixels = lat.shape[1]\n",
    "    ''' CREATE DataArrays '''\n",
    "    mean     = xr.DataArray(mean_ssha_karin_2_W_S,  dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    speed    = xr.DataArray(speed_ssha_karin_2_W_S, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ug       = xr.DataArray(ug_w_s_r,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    vg       = xr.DataArray(vg_w_s_r,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    eke      = xr.DataArray(eke_ssha_karin_2_W_S,   dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    latc     = xr.DataArray(lat,                    dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    new_lonc = xr.DataArray(new_lon,                dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ''' CREATE DataSet '''\n",
    "    dataset  = xr.Dataset({'mean_ssha_karin_2_W_S'  : mean, \n",
    "                           'speed_ssha_karin_2_W_S' : speed, \n",
    "                           'ug_w_s_r'               : ug, \n",
    "                           'vg_w_s_r'               : vg, \n",
    "                           'eke_ssha_karin_2_W_S'   : eke, \n",
    "                           'latitude'               : latc, \n",
    "                           'longitude'              : new_lonc})\n",
    "    ''' RETURN THE DataSet '''\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH : \n",
    "path_merged = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SWOT_1_RAW_MERGED/version_2/'\n",
    "pass_nr = 'pass_003'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = part_3_NetCDF(path_merged,pass_nr)\n",
    "dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(10, 10))\n",
    "a=axs[0, 0].imshow(dataset3.mean_ssha_karin_2_W_S, vmin=-0.2, vmax=0.2, cmap=cmocean.cm.balance)\n",
    "axs[0, 0].set_title('mean_ssha_karin_2_W_S')\n",
    "axs[0, 0].set_ylim(200)\n",
    "plt.colorbar(a)\n",
    "\n",
    "b=axs[0, 1].imshow(dataset3.speed_ssha_karin_2_W_S, vmin=0, vmax=0.5, cmap=cmocean.cm.speed)\n",
    "axs[0, 1].set_title('speed_ssha_karin_2_W_S')\n",
    "axs[0, 1].set_ylim(200)\n",
    "plt.colorbar(b)\n",
    "\n",
    "c=axs[0, 2].imshow(dataset3.ug_w_s_r, vmin=-0.3, vmax=0.3, cmap=cmocean.cm.matter)\n",
    "axs[0, 2].set_title('ug_w_s_r')\n",
    "axs[0, 2].set_ylim(200)\n",
    "plt.colorbar(c)\n",
    "\n",
    "d=axs[1, 0].imshow(dataset3.vg_w_s_r, vmin=-0.3, vmax=0.3, cmap=cmocean.cm.balance)\n",
    "axs[1, 0].set_title('vg_w_s_r')\n",
    "axs[1, 0].set_ylim(200)\n",
    "plt.colorbar(d)\n",
    "\n",
    "e=axs[1, 1].imshow(dataset3.eke_ssha_karin_2_W_S, vmin=0, vmax=0.5, cmap=cmocean.cm.matter)\n",
    "axs[1, 1].set_title('eke_ssha_karin_2_W_S')\n",
    "axs[1, 1].set_ylim(200)\n",
    "plt.colorbar(e)\n",
    "\n",
    "f=axs[1, 2].imshow(dataset3.longitude, vmin=-20, vmax=-15)\n",
    "axs[1, 2].set_title('longitude')\n",
    "axs[1, 2].set_ylim(200)\n",
    "plt.colorbar(f)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/Investigation_part_1/'\n",
    "dataset_part_3 = part_3_NetCDF(path_merged,pass_nr)\n",
    "dataset_part_3.to_netcdf(save_path+'testpart_3_'+pass_nr+'.nc')\n",
    "print(save_path+'part_3_'+pass_nr+'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 4 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' FUNCTIONS PART 4 : '''\n",
    "def calculate_ug_vg(masked_dataset, lat, lon):\n",
    "    ''' GET SLA_DX AND SLA_DY : '''\n",
    "    sla_diff_x = np.gradient(masked_dataset, axis=2)\n",
    "    sla_diff_y = np.gradient(masked_dataset, axis=1)\n",
    "    ''' dx, dy : '''\n",
    "    dx = 2000\n",
    "    dy = 2000\n",
    "    ''' Tetha, g, f : '''\n",
    "    tetha = np.deg2rad(lat)\n",
    "    g     = 9.81\n",
    "    f     = 2 * 7.2921e-5 * np.sin(tetha)\n",
    "    ''' ug, vg : '''\n",
    "    ug  = - (g / f) * (sla_diff_y / dy)\n",
    "    vg  =   (g / f) * (sla_diff_x / dx)\n",
    "    ''' RETURN UG AND VG : '''\n",
    "    return ug, vg\n",
    "def rotate_dataset_original_grid_all_cycles(lat, lon, u, v):\n",
    "    TETHA = lon\n",
    "    PHI   = lat\n",
    "    num_lines  = PHI.shape[0]\n",
    "    num_pixels = PHI.shape[1]\n",
    "    TETHA      = np.where(TETHA > 180, TETHA - 360, TETHA)\n",
    "    # because num_lines is y and num_pixels is x\n",
    "    delta_PHI        = np.diff(PHI,   axis=1)\n",
    "    delta_TETHA      = np.diff(TETHA, axis=1)\n",
    "    delta_PHI        = np.append(delta_PHI, np.zeros((num_lines, 1)), axis=1)\n",
    "    delta_TETHA      = np.append(delta_TETHA, np.zeros((num_lines, 1)), axis=1)\n",
    "    tetha_rad        = np.deg2rad(TETHA)\n",
    "    phi_rad          = np.deg2rad(PHI)\n",
    "    delta_tetha_rad  = np.deg2rad(delta_TETHA)\n",
    "    delta_phi_rad    = np.deg2rad(delta_PHI)\n",
    "    cos_PHI          = np.cos(phi_rad)\n",
    "    curly_TETHA      = np.arctan2(delta_phi_rad,cos_PHI*delta_tetha_rad)\n",
    "    u_rotated        = u*np.cos(curly_TETHA) - v*np.sin(curly_TETHA)\n",
    "    v_rotated        = u*np.sin(curly_TETHA) + v*np.cos(curly_TETHA)\n",
    "    u_rotated_by_phi = u_rotated/cos_PHI\n",
    "    return u_rotated, v_rotated\n",
    "''' STANDARD DEVIATION OF UG AND VG - TRACK : '''\n",
    "def STD_ug_vg_track(masked_d, lat, lon):\n",
    "    ''' CALCULATE UG AND VG : '''\n",
    "    ug, vg = calculate_ug_vg(masked_d, lat, lon)\n",
    "    ''' MEAN OF ug AND vg : '''\n",
    "    #ug_mean = np.nanmean(ug, axis=0)\n",
    "    #vg_mean = np.nanmean(vg, axis=0)\n",
    "    ''' STD : '''\n",
    "    std_ug = np.nanstd(ug, axis=0)\n",
    "    std_vg = np.nanstd(vg, axis=0)\n",
    "    return std_ug, std_vg\n",
    "''' STANDARD DEVIATION OF UG AND VG - ZONAL AND MERIDIONAL : '''\n",
    "def STD_ug_vg_zon_mer(masked_d, lat, lon):\n",
    "    ''' CALCULATE UG AND VG : '''\n",
    "    ug, vg = calculate_ug_vg(masked_d, lat, lon)\n",
    "    ''' MEAN OF ug AND vg : '''\n",
    "    #ug_mean = np.nanmean(ug, axis=0)\n",
    "    #vg_mean = np.nanmean(vg, axis=0)\n",
    "    ''' ROTATE TO ZONAL AND MERIDIONAL : '''\n",
    "    ug_rot, vg_rot = rotate_dataset_original_grid_all_cycles(lat, lon, ug, vg)\n",
    "    ''' STD : '''\n",
    "    std_ug = np.nanstd(ug_rot, axis=0)\n",
    "    std_vg = np.nanstd(vg_rot, axis=0)\n",
    "    return std_ug, std_vg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CREATE VARIABLES PART 4 : '''\n",
    "def part_4_variables(ds):\n",
    "    #masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type='ssha_karin_2', correction_type='ssha', mask_type='all', smoothing=None)\n",
    "    masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables_C0_C2_all_own_ssha(ds, var_type='ssh_karin_2', correction_type='ssha_own', mask_type='all', smoothing=None)\n",
    "    #masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type='ssha_karin_2', correction_type='ssha', mask_type='all', smoothing=1)\n",
    "    std_ug_T, std_vg_T = STD_ug_vg_track(masked_d, lat, lon)\n",
    "    std_ug_G, std_vg_G = STD_ug_vg_zon_mer(masked_d, lat, lon)\n",
    "    ''' Fix the Longitude : '''\n",
    "    new_lon = np.where(lon > 180, lon - 360, lon)\n",
    "    return lat, new_lon, std_ug_T, std_vg_T, std_ug_G, std_vg_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_4_NetCDF(path_merged,pass_nr):\n",
    "    ''' OPEN DS : '''\n",
    "    ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    ''' GET VARIABLES : '''\n",
    "    lat, new_lon, std_ug_T, std_vg_T, std_ug_G, std_vg_G = part_4_variables(ds)\n",
    "    ''' Num_lines, Num_pixels '''\n",
    "    num_lines  = lat.shape[0]\n",
    "    num_pixels = lat.shape[1]\n",
    "    ''' CREATE DataArrays '''\n",
    "    stdugt   = xr.DataArray(std_ug_T, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    stdvgt   = xr.DataArray(std_vg_T, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    stdugg   = xr.DataArray(std_ug_G, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    stdvgg   = xr.DataArray(std_vg_G, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    latc     = xr.DataArray(lat,      dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    new_lonc = xr.DataArray(new_lon,  dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ''' CREATE DataSet '''\n",
    "    dataset  = xr.Dataset({'std_ug_zomer' : stdugg, \n",
    "                           'std_vg_zomer' : stdvgg, \n",
    "                           'std_ug_track' : stdugt, \n",
    "                           'std_vg_track' : stdvgt, \n",
    "                           'latitude'     : latc, \n",
    "                           'longitude'    : new_lonc})\n",
    "    ''' RETURN THE DataSet '''\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH : \n",
    "path_merged = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SWOT_1_RAW_MERGED/version_2/'\n",
    "pass_nr = 'pass_003'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = part_4_NetCDF(path_merged,pass_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(dataset.std_ug_zomer,vmin=0,vmax=0.5)\n",
    "#plt.ylim(250,500)\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "plt.imshow(dataset.std_vg_zomer,vmin=0,vmax=0.5)\n",
    "plt.ylim(400,500)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "#plt.imshow(dataset.std_ug_track,vmin=0,vmax=0.5)\n",
    "#plt.ylim(250,500)\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "plt.imshow(dataset.std_vg_track,vmin=0,vmax=0.5)\n",
    "plt.ylim(400,500)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(dataset.std_vg_zomer-dataset.std_vg_track,cmap=cmocean.cm.balance,vmin=-0.2,vmax=0.2)\n",
    "plt.ylim(400,500)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 5 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' FUNCTIONS PART 5 : '''\n",
    "def calculate_ug_vg(masked_dataset, lat, lon):\n",
    "    ''' GET SLA_DX AND SLA_DY : '''\n",
    "    sla_diff_x = np.gradient(masked_dataset, axis=2)\n",
    "    sla_diff_y = np.gradient(masked_dataset, axis=1)\n",
    "    ''' dx, dy : '''\n",
    "    dx = 2000\n",
    "    dy = 2000\n",
    "    ''' Tetha, g, f : '''\n",
    "    tetha = np.deg2rad(lat)\n",
    "    g     = 9.81\n",
    "    f     = 2 * 7.2921e-5 * np.sin(tetha)\n",
    "    ''' ug, vg : '''\n",
    "    ug  = - (g / f) * (sla_diff_y / dy)\n",
    "    vg  =   (g / f) * (sla_diff_x / dx)\n",
    "    ''' RETURN UG AND VG : '''\n",
    "    return ug, vg\n",
    "def rotate_dataset_original_grid_all_cycles(lat, lon, u, v):\n",
    "    TETHA = lon\n",
    "    PHI   = lat\n",
    "    num_lines  = PHI.shape[0]\n",
    "    num_pixels = PHI.shape[1]\n",
    "    TETHA      = np.where(TETHA > 180, TETHA - 360, TETHA)\n",
    "    # because num_lines is y and num_pixels is x\n",
    "    delta_PHI        = np.diff(PHI,   axis=1)\n",
    "    delta_TETHA      = np.diff(TETHA, axis=1)\n",
    "    delta_PHI        = np.append(delta_PHI, np.zeros((num_lines, 1)), axis=1)\n",
    "    delta_TETHA      = np.append(delta_TETHA, np.zeros((num_lines, 1)), axis=1)\n",
    "    tetha_rad        = np.deg2rad(TETHA)\n",
    "    phi_rad          = np.deg2rad(PHI)\n",
    "    delta_tetha_rad  = np.deg2rad(delta_TETHA)\n",
    "    delta_phi_rad    = np.deg2rad(delta_PHI)\n",
    "    cos_PHI          = np.cos(phi_rad)\n",
    "    curly_TETHA      = np.arctan2(delta_phi_rad,cos_PHI*delta_tetha_rad)\n",
    "    u_rotated        = u*np.cos(curly_TETHA) - v*np.sin(curly_TETHA)\n",
    "    v_rotated        = u*np.sin(curly_TETHA) + v*np.cos(curly_TETHA)\n",
    "    u_rotated_by_phi = u_rotated/cos_PHI\n",
    "    return u_rotated, v_rotated\n",
    "''' Coefficient of Variation : '''\n",
    "def CV_ug_vg_zon_mer(masked_d, lat, lon):\n",
    "    ''' CALCULATE UG AND VG : '''\n",
    "    ug, vg = calculate_ug_vg(masked_d, lat, lon)\n",
    "    ''' ROTATE TO ZONAL AND MERIDIONAL : '''\n",
    "    ug_rot, vg_rot = rotate_dataset_original_grid_all_cycles(lat, lon, ug, vg)\n",
    "    ''' MEAN OF ug AND vg : '''\n",
    "    ug_mean = np.nanmean(ug_rot, axis=0)\n",
    "    vg_mean = np.nanmean(vg_rot, axis=0)\n",
    "    ''' STD : '''\n",
    "    std_ug = np.nanstd(ug_rot, axis=0)\n",
    "    std_vg = np.nanstd(vg_rot, axis=0)\n",
    "    ''' CV : '''\n",
    "    CV_ug = (std_ug+1)/(np.abs(ug_mean)+1)\n",
    "    CV_vg = (std_vg+1)/(np.abs(vg_mean)+1)\n",
    "    return CV_ug, CV_vg, ug_mean, vg_mean, std_ug, std_vg#np.sqrt(CV_ug), np.sqrt(CV_vg), ug_mean, vg_mean#CV_ug/20, CV_vg/20#\n",
    "\n",
    "def CV_speed_func(masked_d, lat, lon):\n",
    "    ''' CALCULATE UG AND VG : '''\n",
    "    ug, vg = calculate_ug_vg(masked_d, lat, lon)\n",
    "    ''' ROTATE TO ZONAL AND MERIDIONAL : '''\n",
    "    ug_rot, vg_rot = rotate_dataset_original_grid_all_cycles(lat, lon, ug, vg)\n",
    "    ''' MEAN OF ug AND vg : '''\n",
    "    ug_mean = np.nanmean(ug_rot, axis=0)\n",
    "    vg_mean = np.nanmean(vg_rot, axis=0)\n",
    "    ''' SPEED : '''\n",
    "    speed = np.sqrt(ug_rot**2 + vg_rot**2)\n",
    "    #mspeed = np.sqrt(ug_mean**2 + vg_mean**2)\n",
    "    m2speed = np.nanmean(speed, axis=0)\n",
    "    ''' STD : '''\n",
    "    #std_ug = np.nanstd(ug_rot, axis=0)\n",
    "    #std_vg = np.nanstd(vg_rot, axis=0)\n",
    "    std_speed = np.nanstd(speed, axis=0)\n",
    "    ''' CV : '''\n",
    "    #CV_ug = (std_ug+1)/(np.abs(ug_mean)+1)\n",
    "    #CV_vg = (std_vg+1)/(np.abs(vg_mean)+1)\n",
    "    CV_speed = std_speed/m2speed\n",
    "    #ug_diff_mean = ug - ug_mean\n",
    "    #vg_diff_mean = vg - vg_mean\n",
    "    #ug_diff_sq = ug_diff_mean**2\n",
    "    #vg_diff_sq = vg_diff_mean**2\n",
    "    #eke_jon = 0.5 * (np.nanmean(ug_diff_sq, axis=0) + np.nanmean(vg_diff_sq, axis=0))\n",
    "    return  m2speed, std_speed, CV_speed\n",
    "\n",
    "\n",
    "''' Spatial Standard deviation : '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CREATE VARIABLES PART 3 : '''\n",
    "def part_5_variables(ds):\n",
    "    #masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type='ssha_karin_2', correction_type='ssha', mask_type='all', smoothing=None)\n",
    "    masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables_C0_C2_all_own_ssha(ds, var_type='ssh_karin_2', correction_type='ssha_own', mask_type='all', smoothing=None)\n",
    "    #masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type='ssha_karin_2', correction_type='ssha', mask_type='all', smoothing=1)\n",
    "    #CV_ug, CV_vg, ug_mean, vg_mean, std_ug, std_vg = CV_ug_vg_zon_mer(masked_d, lat, lon)\n",
    "    m2speed, std_speed, CV_speed = CV_speed_func(masked_d, lat, lon)\n",
    "    ''' Fix the Longitude : '''\n",
    "    new_lon = np.where(lon > 180, lon - 360, lon)\n",
    "    return lat, new_lon, m2speed, std_speed, CV_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_5_NetCDF_old(path_merged,pass_nr):\n",
    "    ''' OPEN DS : '''\n",
    "    ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    ''' GET VARIABLES : '''\n",
    "    lat, new_lon, CV_ug, CV_vg, ug_mean, vg_mean, std_ug, std_vg = part_5_variables(ds)\n",
    "    ''' Num_lines, Num_pixels '''\n",
    "    num_lines  = lat.shape[0]\n",
    "    num_pixels = lat.shape[1]\n",
    "    ''' CREATE DataArrays '''\n",
    "    CVug     = xr.DataArray(CV_ug, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    CVvg     = xr.DataArray(CV_vg, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    mug      = xr.DataArray(ug_mean, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    mvg      = xr.DataArray(vg_mean, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    latc     = xr.DataArray(lat,      dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    new_lonc = xr.DataArray(new_lon,  dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ''' CREATE DataSet '''\n",
    "    dataset  = xr.Dataset({'CV_ug'     : CVug, \n",
    "                           'CV_vg'     : CVvg, \n",
    "                           'ug_mean'   : mug, \n",
    "                           'vg_mean'   : mvg, \n",
    "                           'latitude'  : latc, \n",
    "                           'longitude' : new_lonc})\n",
    "    ''' RETURN THE DataSet '''\n",
    "    return dataset\n",
    "def part_5_NetCDF(path_merged,pass_nr):\n",
    "    ''' OPEN DS : '''\n",
    "    ds = xr.open_dataset(path_merged+pass_nr+'.nc')\n",
    "    ''' GET VARIABLES : '''\n",
    "    lat, new_lon, m2speed, std_speed, CV_speed = part_5_variables(ds)\n",
    "    ''' Num_lines, Num_pixels '''\n",
    "    num_lines  = lat.shape[0]\n",
    "    num_pixels = lat.shape[1]\n",
    "    ''' CREATE DataArrays '''\n",
    "    ms       = xr.DataArray(m2speed,   dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    stds     = xr.DataArray(std_speed, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    cvs      = xr.DataArray(CV_speed,  dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    latc     = xr.DataArray(lat,       dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    new_lonc = xr.DataArray(new_lon,   dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ''' CREATE DataSet '''\n",
    "    dataset  = xr.Dataset({'mean_speed' : ms, \n",
    "                           'std_speed'  : stds, \n",
    "                           'CV_speed'   : cvs, \n",
    "                           'latitude'   : latc, \n",
    "                           'longitude'  : new_lonc})\n",
    "    ''' RETURN THE DataSet '''\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH : \n",
    "path_merged = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SWOT_1_RAW_MERGED/version_2/'\n",
    "pass_nr = 'pass_003'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = part_5_NetCDF(path_merged,pass_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataset.mean_speed,vmin=0,vmax=0.5)\n",
    "plt.ylim(400,500)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(dataset.std_speed,vmin=0,vmax=0.5)\n",
    "plt.ylim(400,500)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(dataset.CV_speed,vmin=0,vmax=1.5)\n",
    "plt.ylim(400,500)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make NetCDF-files : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passes = ['pass_003', 'pass_005', 'pass_007', 'pass_008', 'pass_009', 'pass_010', 'pass_012', 'pass_014', 'pass_016', 'pass_018',\n",
    "          'pass_031', 'pass_033', 'pass_035', 'pass_036', 'pass_037', 'pass_038', 'pass_040', 'pass_042', 'pass_044', 'pass_046',\n",
    "          'pass_059', 'pass_061', 'pass_063', 'pass_064', 'pass_065', 'pass_066', 'pass_068', 'pass_070', 'pass_072', 'pass_074',\n",
    "          'pass_087', 'pass_089', 'pass_091', 'pass_092', 'pass_093', 'pass_094', 'pass_096', 'pass_098', 'pass_100',\n",
    "          'pass_113', 'pass_115', 'pass_117', 'pass_119', 'pass_120', 'pass_121', 'pass_122', 'pass_124', 'pass_126', 'pass_128',\n",
    "          'pass_141', 'pass_143', 'pass_145', 'pass_147', 'pass_148', 'pass_149', 'pass_150', 'pass_152', 'pass_154', 'pass_156',\n",
    "          'pass_169', 'pass_171', 'pass_173', 'pass_175', 'pass_176', 'pass_177', 'pass_178', 'pass_180', 'pass_182', 'pass_184',\n",
    "          'pass_197', 'pass_199', 'pass_201', 'pass_203', 'pass_204', 'pass_205', 'pass_206', 'pass_208', 'pass_210', 'pass_212',\n",
    "          'pass_225', 'pass_227', 'pass_229', 'pass_231', 'pass_232', 'pass_233', 'pass_234', 'pass_236', 'pass_238', 'pass_240',\n",
    "          'pass_253', 'pass_255', 'pass_257', 'pass_259', 'pass_260', 'pass_262', 'pass_264', 'pass_266', 'pass_268',\n",
    "          'pass_281', 'pass_283', 'pass_285', 'pass_286', 'pass_287', 'pass_288', 'pass_290', 'pass_292', 'pass_294', 'pass_296',\n",
    "          'pass_309', 'pass_311', 'pass_313', 'pass_314', 'pass_315', 'pass_316', 'pass_318', 'pass_320', 'pass_322', 'pass_324',\n",
    "          'pass_337', 'pass_339', 'pass_341', 'pass_342', 'pass_343', 'pass_344', 'pass_346', 'pass_348', 'pass_350', 'pass_352',\n",
    "          'pass_365', 'pass_367', 'pass_369', 'pass_370', 'pass_371', 'pass_372', 'pass_374', 'pass_376', 'pass_378', 'pass_391',\n",
    "          'pass_393', 'pass_395', 'pass_397', 'pass_398', 'pass_399', 'pass_400', 'pass_402', 'pass_404', 'pass_406', 'pass_419',\n",
    "          'pass_421', 'pass_423', 'pass_425', 'pass_426', 'pass_427', 'pass_428', 'pass_430', 'pass_432', 'pass_434', 'pass_447',\n",
    "          'pass_449', 'pass_451', 'pass_453', 'pass_454', 'pass_455', 'pass_456', 'pass_458', 'pass_460', 'pass_462', 'pass_475',\n",
    "          'pass_477', 'pass_479', 'pass_481', 'pass_482', 'pass_483', 'pass_484', 'pass_486', 'pass_488', 'pass_490', 'pass_503',\n",
    "          'pass_505', 'pass_507', 'pass_509', 'pass_510', 'pass_511', 'pass_512', 'pass_514', 'pass_516', 'pass_518', 'pass_531',\n",
    "          'pass_533', 'pass_535', 'pass_536', 'pass_537', 'pass_538', 'pass_539', 'pass_540', 'pass_542', 'pass_544', 'pass_546', 'pass_559',\n",
    "          'pass_561', 'pass_563', 'pass_564', 'pass_565', 'pass_566', 'pass_568', 'pass_570', 'pass_572', 'pass_574']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passes = ['pass_477', 'pass_479', 'pass_481', 'pass_482', 'pass_483', 'pass_484', 'pass_486', 'pass_488', 'pass_490', 'pass_503',\n",
    "          'pass_505', 'pass_507', 'pass_509', 'pass_510', 'pass_511', 'pass_512', 'pass_514', 'pass_516', 'pass_518', 'pass_531',\n",
    "          'pass_533', 'pass_535', 'pass_536', 'pass_537', 'pass_538', 'pass_539', 'pass_540', 'pass_542', 'pass_544', 'pass_546', 'pass_559',\n",
    "          'pass_561', 'pass_563', 'pass_564', 'pass_565', 'pass_566', 'pass_568', 'pass_570', 'pass_572', 'pass_574']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passes = ['pass_568', 'pass_570', 'pass_572', 'pass_574']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path   = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/Investigation_part_1/'\n",
    "#save_path  = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/Investigation_part_1/Smoothed_1_time/'\n",
    "path_merged = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SWOT_1_RAW_MERGED/version_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pass_nr in passes: \n",
    "    print(pass_nr)\n",
    "    dataset_part_1 = part_1_NetCDF(path_merged,pass_nr)\n",
    "    dataset_part_1.to_netcdf(save_path+'part_1/'+'part_1_'+pass_nr+'.nc')\n",
    "    print(save_path+'part_1/'+'part_1_'+pass_nr+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pass_nr in passes: \n",
    "    print(pass_nr)\n",
    "    dataset_part_2 = part_2_NetCDF(path_merged,pass_nr)\n",
    "    #dataset_part_2.to_netcdf(save_path+'part_2/'+'part_2_'+pass_nr+'.nc')\n",
    "    dataset_part_2.to_netcdf(save_path+'part_2_sm_2/'+'part_2_'+pass_nr+'.nc')\n",
    "    print(save_path+'part_2/'+'part_2_'+pass_nr+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pass_nr in passes: \n",
    "    print(pass_nr)\n",
    "    dataset_part_3 = part_3_NetCDF(path_merged,pass_nr)\n",
    "    #dataset_part_3.to_netcdf(save_path+'part_3/'+'part_3_'+pass_nr+'.nc')\n",
    "    dataset_part_3.to_netcdf(save_path+'part_3_sm_2/'+'part_3_'+pass_nr+'.nc')\n",
    "    print(save_path+'part_3/'+'part_3_'+pass_nr+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pass_nr in passes: \n",
    "    print(pass_nr)\n",
    "    dataset_part_4 = part_4_NetCDF(path_merged,pass_nr)\n",
    "    dataset_part_4.to_netcdf(save_path+'part_4/'+'part_4_'+pass_nr+'.nc')\n",
    "    print(save_path+'part_4/'+'part_4_'+pass_nr+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pass_nr in passes: \n",
    "    print(pass_nr)\n",
    "    dataset_part_5 = part_5_NetCDF(path_merged,pass_nr)\n",
    "    dataset_part_5.to_netcdf(save_path+'part_5/'+'part_5_'+pass_nr+'.nc')\n",
    "    print(save_path+'part_5/'+'part_5_'+pass_nr+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/Investigation_part_1/'\n",
    "pass_nr = 'pass_003'\n",
    "ds = xr.open_dataset(save_path+'part_1/'+'part_1_'+pass_nr+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/Investigation_part_1/'\n",
    "pass_nr = 'pass_003'\n",
    "ds = xr.open_dataset(save_path+'part_2/'+'part_2_'+pass_nr+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/Investigation_part_1/'\n",
    "pass_nr = 'pass_003'\n",
    "ds = xr.open_dataset(save_path+'part_3/'+'part_3_'+pass_nr+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/Investigation_part_1/'\n",
    "pass_nr = 'pass_003'\n",
    "ds = xr.open_dataset(save_path+'part_4/'+'part_4_'+pass_nr+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/Investigation_part_1/'\n",
    "pass_nr = 'pass_003'\n",
    "ds = xr.open_dataset(save_path+'part_5/'+'part_5_'+pass_nr+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unravel and create CSV-files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passes = ['pass_003', 'pass_005', 'pass_007', 'pass_008', 'pass_009', 'pass_010', 'pass_012', 'pass_014', 'pass_016', 'pass_018',\n",
    "          'pass_031', 'pass_033', 'pass_035', 'pass_036', 'pass_037', 'pass_038', 'pass_040', 'pass_042', 'pass_044', 'pass_046',\n",
    "          'pass_059', 'pass_061', 'pass_063', 'pass_064', 'pass_065', 'pass_066', 'pass_068', 'pass_070', 'pass_072', 'pass_074',\n",
    "          'pass_087', 'pass_089', 'pass_091', 'pass_092', 'pass_093', 'pass_094', 'pass_096', 'pass_098', 'pass_100',\n",
    "          'pass_113', 'pass_115', 'pass_117', 'pass_119', 'pass_120', 'pass_121', 'pass_122', 'pass_124', 'pass_126', 'pass_128',\n",
    "          'pass_141', 'pass_143', 'pass_145', 'pass_147', 'pass_148', 'pass_149', 'pass_150', 'pass_152', 'pass_154', 'pass_156',\n",
    "          'pass_169', 'pass_171', 'pass_173', 'pass_175', 'pass_176', 'pass_177', 'pass_178', 'pass_180', 'pass_182', 'pass_184',\n",
    "          'pass_197', 'pass_199', 'pass_201', 'pass_203', 'pass_204', 'pass_205', 'pass_206', 'pass_208', 'pass_210', 'pass_212',\n",
    "          'pass_225', 'pass_227', 'pass_229', 'pass_231', 'pass_232', 'pass_233', 'pass_234', 'pass_236', 'pass_238', 'pass_240',\n",
    "          'pass_253', 'pass_255', 'pass_257', 'pass_259', 'pass_260', 'pass_262', 'pass_264', 'pass_266', 'pass_268',\n",
    "          'pass_281', 'pass_283', 'pass_285', 'pass_286', 'pass_287', 'pass_288', 'pass_290', 'pass_292', 'pass_294', 'pass_296',\n",
    "          'pass_309', 'pass_311', 'pass_313', 'pass_314', 'pass_315', 'pass_316', 'pass_318', 'pass_320', 'pass_322', 'pass_324',\n",
    "          'pass_337', 'pass_339', 'pass_341', 'pass_342', 'pass_343', 'pass_344', 'pass_346', 'pass_348', 'pass_350', 'pass_352',\n",
    "          'pass_365', 'pass_367', 'pass_369', 'pass_370', 'pass_371', 'pass_372', 'pass_374', 'pass_376', 'pass_378', 'pass_391',\n",
    "          'pass_393', 'pass_395', 'pass_397', 'pass_398', 'pass_399', 'pass_400', 'pass_402', 'pass_404', 'pass_406', 'pass_419',\n",
    "          'pass_421', 'pass_423', 'pass_425', 'pass_426', 'pass_427', 'pass_428', 'pass_430', 'pass_432', 'pass_434', 'pass_447',\n",
    "          'pass_449', 'pass_451', 'pass_453', 'pass_454', 'pass_455', 'pass_456', 'pass_458', 'pass_460', 'pass_462', 'pass_475',\n",
    "          'pass_477', 'pass_479', 'pass_481', 'pass_482', 'pass_483', 'pass_484', 'pass_486', 'pass_488', 'pass_490', 'pass_503',\n",
    "          'pass_505', 'pass_507', 'pass_509', 'pass_510', 'pass_511', 'pass_512', 'pass_514', 'pass_516', 'pass_518', 'pass_531',\n",
    "          'pass_533', 'pass_535', 'pass_536', 'pass_537', 'pass_538', 'pass_539', 'pass_540', 'pass_542', 'pass_544', 'pass_546', 'pass_559',\n",
    "          'pass_561', 'pass_563', 'pass_564', 'pass_565', 'pass_566', 'pass_568', 'pass_570', 'pass_572', 'pass_574']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/Investigation_part_1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_values_part_1(files_ds):    \n",
    "    # GET VALUES :\n",
    "    c1     = files_ds['count_ssh_karin']\n",
    "    c2     = files_ds['count_ssh_karin_2']\n",
    "    std    = files_ds['std_ssh_karin_2']\n",
    "    meant  = files_ds['mean_ssh_karin_2_tides']\n",
    "    means  = files_ds['mean_slope_ssh_karin_2_tides']\n",
    "    meanb  = files_ds['mean_slope_bathy']\n",
    "    meanb2 = files_ds['mean_slope_bathy_norm']\n",
    "    mdtm   = files_ds['mean_dynamic_topography_model']\n",
    "    lat    = files_ds['latitude']\n",
    "    lon    = files_ds['longitude']\n",
    "    # FIX LONGITUDE (IF NEEDED) : \n",
    "    #lon = np.where(lon>180.0,lon-360,lon)\n",
    "    # UNRAVEL : \n",
    "    c1     = np.ravel(c1)\n",
    "    c2     = np.ravel(c2)\n",
    "    std    = np.ravel(std)\n",
    "    meant  = np.ravel(meant)\n",
    "    means  = np.ravel(means)\n",
    "    meanb  = np.ravel(meanb)\n",
    "    meanb2 = np.ravel(meanb2)\n",
    "    mdtm   = np.ravel(mdtm)\n",
    "    lat    = np.ravel(lat)\n",
    "    lon    = np.ravel(lon)\n",
    "    # Return values :\n",
    "    return c1, c2, std, meant, means, meanb, meanb2, mdtm, lat, lon\n",
    "\n",
    "def unravel_values_part_2(files_ds):    \n",
    "    # GET VALUES :\n",
    "    std   = files_ds['std_ssha_karin_2']\n",
    "    mean  = files_ds['mean_ssha_karin_2']\n",
    "    eke1  = files_ds['eke_ssha_karin_2_normal']\n",
    "    eke2  = files_ds['eke_ssha_karin_2_option_2']\n",
    "    lat   = files_ds['latitude']\n",
    "    lon   = files_ds['longitude']\n",
    "    # FIX LONGITUDE (IF NEEDED) : \n",
    "    #lon = np.where(lon>180.0,lon-360,lon)\n",
    "    # UNRAVEL : \n",
    "    std   = np.ravel(std)\n",
    "    mean  = np.ravel(mean)\n",
    "    eke1  = np.ravel(eke1)\n",
    "    eke2  = np.ravel(eke2)\n",
    "    lat   = np.ravel(lat)\n",
    "    lon   = np.ravel(lon)\n",
    "    # Return values :\n",
    "    return std, mean, eke1, eke2, lat, lon\n",
    "\n",
    "def unravel_values_part_3(files_ds):    \n",
    "    # GET VALUES :\n",
    "    mean  = files_ds['mean_ssha_karin_2_W_S']\n",
    "    speed = files_ds['speed_ssha_karin_2_W_S']\n",
    "    ug    = files_ds['ug_w_s_r']\n",
    "    vg    = files_ds['vg_w_s_r']\n",
    "    eke   = files_ds['eke_ssha_karin_2_W_S']\n",
    "    lat   = files_ds['latitude']\n",
    "    lon   = files_ds['longitude']\n",
    "    # FIX LONGITUDE (IF NEEDED) : \n",
    "    #lon = np.where(lon>180.0,lon-360,lon)\n",
    "    # UNRAVEL : \n",
    "    mean  = np.ravel(mean)\n",
    "    speed = np.ravel(speed)\n",
    "    ug    = np.ravel(ug)\n",
    "    vg    = np.ravel(vg)\n",
    "    eke   = np.ravel(eke)\n",
    "    lat   = np.ravel(lat)\n",
    "    lon   = np.ravel(lon)\n",
    "    # Return values :\n",
    "    return mean, speed, ug, vg, eke, lat, lon\n",
    "\n",
    "def unravel_values_part_4(files_ds):   \n",
    "    # GET VALUES :\n",
    "    stdugg = files_ds['std_ug_zomer']\n",
    "    stdvgg = files_ds['std_vg_zomer']\n",
    "    stdugt = files_ds['std_ug_track']\n",
    "    stdvgt = files_ds['std_vg_track']\n",
    "    lat    = files_ds['latitude']\n",
    "    lon    = files_ds['longitude']\n",
    "    # FIX LONGITUDE (IF NEEDED) : \n",
    "    #lon = np.where(lon>180.0,lon-360,lon)\n",
    "    # UNRAVEL : \n",
    "    stdugg = np.ravel(stdugg)\n",
    "    stdvgg = np.ravel(stdvgg)\n",
    "    stdugt = np.ravel(stdugt)\n",
    "    stdvgt = np.ravel(stdvgt)\n",
    "    lat    = np.ravel(lat)\n",
    "    lon    = np.ravel(lon)\n",
    "    # Return values :\n",
    "    return stdugg, stdvgg, stdugt, stdvgt, lat, lon\n",
    "\n",
    "def unravel_values_part_5(files_ds):   \n",
    "    # GET VALUES :\n",
    "    ms   = files_ds['mean_speed']\n",
    "    stds = files_ds['std_speed']\n",
    "    cvs  = files_ds['CV_speed']\n",
    "    lat  = files_ds['latitude']\n",
    "    lon  = files_ds['longitude']\n",
    "    # FIX LONGITUDE (IF NEEDED) : \n",
    "    #lon = np.where(lon>180.0,lon-360,lon)\n",
    "    # UNRAVEL : \n",
    "    ms   = np.ravel(ms)\n",
    "    stds = np.ravel(stds)\n",
    "    cvs  = np.ravel(cvs)\n",
    "    lat  = np.ravel(lat)\n",
    "    lon  = np.ravel(lon)\n",
    "    # Return values :\n",
    "    return ms, stds, cvs, lat, lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_1(save_path,pass_nr):\n",
    "    # Get NetCDF file : \n",
    "    files_ds = xr.open_dataset(save_path+'part_1/'+'part_1_'+pass_nr+'.nc')\n",
    "    # Unravel data : \n",
    "    c1, c2, std, meant, means, meanb, meanb2, mdtm, lat, lon = unravel_values_part_1(files_ds)\n",
    "    # CREATE DATAFRAME :\n",
    "    df = pd.DataFrame({'count_ssh_karin'                : c1, \n",
    "                       'count_ssh_karin_2'              : c2, \n",
    "                       'std_ssh_karin_2'                : std, \n",
    "                       'mean_ssh_karin_2_tides'         : meant, \n",
    "                       'mean_slope_ssh_karin_2_tides'   : means, \n",
    "                       'mean_slope_bathy'               : meanb, \n",
    "                       'mean_slope_bathy_norm'          : meanb2, \n",
    "                       'mean_dynamic_topography_model'  : mdtm, \n",
    "                       'lat'                            : lat, \n",
    "                       'lon'                            : lon})\n",
    "    # Return dataset :\n",
    "    return df\n",
    "\n",
    "def create_dataset_2(save_path,pass_nr):\n",
    "    # Get NetCDF file : \n",
    "    #files_ds = xr.open_dataset(save_path+'part_2/'+'part_2_'+pass_nr+'.nc')\n",
    "    files_ds = xr.open_dataset(save_path+'part_2_sm_2/'+'part_2_'+pass_nr+'.nc')\n",
    "    # Unravel data : \n",
    "    std, mean, eke1, eke2, lat, lon = unravel_values_part_2(files_ds)\n",
    "    # CREATE DATAFRAME :\n",
    "    df = pd.DataFrame({'std_ssha_karin_2'          : std, \n",
    "                       'mean_ssha_karin_2'         : mean, \n",
    "                       'eke_ssha_karin_2_normal'   : eke1, \n",
    "                       'eke_ssha_karin_2_option_2' : eke2, \n",
    "                       'lat'                       : lat, \n",
    "                       'lon'                       : lon})\n",
    "    # Return dataset :\n",
    "    return df\n",
    "\n",
    "def create_dataset_3(save_path,pass_nr):\n",
    "    # Get NetCDF file : \n",
    "    #files_ds = xr.open_dataset(save_path+'part_3/'+'part_3_'+pass_nr+'.nc')\n",
    "    files_ds = xr.open_dataset(save_path+'part_3_sm_2/'+'part_3_'+pass_nr+'.nc')\n",
    "    # Unravel data : \n",
    "    mean, speed, ug, vg, eke, lat, lon = unravel_values_part_3(files_ds)\n",
    "    # CREATE DATAFRAME :\n",
    "    df = pd.DataFrame({'mean_ssha_karin_2_W_S'  : mean, \n",
    "                       'speed_ssha_karin_2_W_S' : speed, \n",
    "                       'ug_w_s_r'               : ug, \n",
    "                       'vg_w_s_r'               : vg, \n",
    "                       'eke_ssha_karin_2_W_S'   : eke, \n",
    "                       'lat'                    : lat, \n",
    "                       'lon'                    : lon})\n",
    "    # Return dataset :\n",
    "    return df\n",
    "\n",
    "def create_dataset_4(save_path,pass_nr):\n",
    "    # Get NetCDF file : \n",
    "    files_ds = xr.open_dataset(save_path+'part_4/'+'part_4_'+pass_nr+'.nc')\n",
    "    # Unravel data : \n",
    "    stdugg, stdvgg, stdugt, stdvgt, lat, lon = unravel_values_part_4(files_ds)\n",
    "    # CREATE DATAFRAME :\n",
    "    df = pd.DataFrame({'std_ug_zomer' : stdugg, \n",
    "                       'std_vg_zomer' : stdvgg, \n",
    "                       'std_ug_track' : stdugt, \n",
    "                       'std_vg_track' : stdvgt, \n",
    "                       'lat'          : lat, \n",
    "                       'lon'          : lon})\n",
    "    # Return dataset :\n",
    "    return df\n",
    "\n",
    "def create_dataset_5(save_path,pass_nr):\n",
    "    # Get NetCDF file : \n",
    "    files_ds = xr.open_dataset(save_path+'part_5/'+'part_5_'+pass_nr+'.nc')\n",
    "    # Unravel data : \n",
    "    ms, stds, cvs, lat, lon = unravel_values_part_5(files_ds)\n",
    "    # CREATE DATAFRAME :\n",
    "    df = pd.DataFrame({'mean_speed' : ms, \n",
    "                       'std_speed'  : stds, \n",
    "                       'CV_speed'   : cvs, \n",
    "                       'lat'        : lat, \n",
    "                       'lon'        : lon})\n",
    "    # Return dataset :\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pass_nr in passes:\n",
    "    print('Pass number : ', pass_nr)\n",
    "    # Get datasets : \n",
    "    df = create_dataset_1(save_path,pass_nr)\n",
    "    # Save the dataframe to a csv file\n",
    "    csv_name  = 'part_1_'+pass_nr+'.csv'\n",
    "    # FILE PATHS :\n",
    "    save_to_csv = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/Investigation_part_1/part_1_csv/'+csv_name\n",
    "    # SAVE :\n",
    "    df.to_csv(save_to_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pass_nr in passes:\n",
    "    print('Pass number : ', pass_nr)\n",
    "    # Get datasets : \n",
    "    df = create_dataset_2(save_path,pass_nr)\n",
    "    # Save the dataframe to a csv file\n",
    "    csv_name  = 'part_2_'+pass_nr+'.csv'\n",
    "    # FILE PATHS :\n",
    "    #save_to_csv = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/Investigation_part_1/part_2_csv/'+csv_name\n",
    "    save_to_csv = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/Investigation_part_1/part_2_csv_sm_2/'+csv_name\n",
    "    # SAVE :\n",
    "    df.to_csv(save_to_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pass_nr in passes:\n",
    "    print('Pass number : ', pass_nr)\n",
    "    # Get datasets : \n",
    "    df = create_dataset_3(save_path,pass_nr)\n",
    "    # Save the dataframe to a csv file\n",
    "    csv_name  = 'part_3_'+pass_nr+'.csv'\n",
    "    # FILE PATHS :\n",
    "    #save_to_csv = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/Investigation_part_1/part_3_csv/'+csv_name\n",
    "    save_to_csv = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/Investigation_part_1/part_3_csv_sm_2/'+csv_name\n",
    "    #save_to_csv = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/Investigation_part_1/Smoothed_1_time/part_3_csv/'+csv_name\n",
    "    # SAVE :\n",
    "    df.to_csv(save_to_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pass_nr in passes:\n",
    "    print('Pass number : ', pass_nr)\n",
    "    # Get datasets : \n",
    "    df = create_dataset_4(save_path,pass_nr)\n",
    "    # Save the dataframe to a csv file\n",
    "    csv_name  = 'part_4_'+pass_nr+'.csv'\n",
    "    # FILE PATHS :\n",
    "    save_to_csv = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/Investigation_part_1/part_4_csv/'+csv_name\n",
    "    # SAVE :\n",
    "    df.to_csv(save_to_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pass_nr in passes:\n",
    "    print('Pass number : ', pass_nr)\n",
    "    # Get datasets : \n",
    "    df = create_dataset_5(save_path,pass_nr)\n",
    "    # Save the dataframe to a csv file\n",
    "    csv_name  = 'part_5_'+pass_nr+'.csv'\n",
    "    # FILE PATHS :\n",
    "    save_to_csv = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/Investigation_part_1/part_5_csv/'+csv_name\n",
    "    # SAVE :\n",
    "    df.to_csv(save_to_csv, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
