{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>SOFiA - Create Files</center></h1>\n",
    "\n",
    "<h5><center>Ingvild Olden Bjerkelund</center></h5>\n",
    "\n",
    "<h1><center> </center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Choose parameters, correction, smoothing, delta-lat-lon, etc. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python packages\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy as cy\n",
    "from matplotlib.dates import DateFormatter\n",
    "import cartopy.crs as ccrs \n",
    "import cmocean\n",
    "import netCDF4 as nc\n",
    "import glob as glob\n",
    "import pandas as pd\n",
    "from scipy.interpolate import griddata\n",
    "# SOFI\n",
    "#import SWOT_Oceanography_Functions_Ingvild as sofi\n",
    "# Binning (Jonathan)\n",
    "from scipy.stats import binned_statistic_2d\n",
    "import cartopy.feature as cfeature\n",
    "import cmcrameri as cmc\n",
    "\n",
    "import SOFiA as SOFiA\n",
    "\n",
    "import SWOT_PLOT_Step_2_Func as plt_s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Create $<\\bar{ug}>$ and $<\\bar{vg}>$, and Bathymetry : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_lat_lon  = 0.1 # This is approx 11.1 km so small enough !!\n",
    "extent_of_area = [-80, 30, 45, 78]\n",
    "lon1 = extent_of_area[0]\n",
    "lon2 = extent_of_area[1]\n",
    "lat1 = extent_of_area[2]\n",
    "lat2 = extent_of_area[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passes = ['pass_003', 'pass_005', 'pass_007', 'pass_008', 'pass_009', 'pass_010', 'pass_012', 'pass_014', 'pass_016', 'pass_018',\n",
    "          'pass_031', 'pass_033', 'pass_035', 'pass_036', 'pass_037', 'pass_038', 'pass_040', 'pass_042', 'pass_044', 'pass_046',\n",
    "          'pass_059', 'pass_061', 'pass_063', 'pass_064', 'pass_065', 'pass_066', 'pass_068', 'pass_070', 'pass_072', 'pass_074',\n",
    "          'pass_087', 'pass_089', 'pass_091', 'pass_092', 'pass_093', 'pass_094', 'pass_096', 'pass_098', 'pass_100',\n",
    "          'pass_113', 'pass_115', 'pass_117', 'pass_119', 'pass_120', 'pass_121', 'pass_122', 'pass_124', 'pass_126', 'pass_128',\n",
    "          'pass_141', 'pass_143', 'pass_145', 'pass_147', 'pass_148', 'pass_149', 'pass_150', 'pass_152', 'pass_154', 'pass_156',\n",
    "          'pass_169', 'pass_171', 'pass_173', 'pass_175', 'pass_176', 'pass_177', 'pass_178', 'pass_180', 'pass_182', 'pass_184',\n",
    "          'pass_197', 'pass_199', 'pass_201', 'pass_203', 'pass_204', 'pass_205', 'pass_206', 'pass_208', 'pass_210', 'pass_212',\n",
    "          'pass_225', 'pass_227', 'pass_229', 'pass_231', 'pass_232', 'pass_233', 'pass_234', 'pass_236', 'pass_238', 'pass_240',\n",
    "          'pass_253', 'pass_255', 'pass_257', 'pass_259', 'pass_260', 'pass_262', 'pass_264', 'pass_266', 'pass_268',\n",
    "          'pass_281', 'pass_283', 'pass_285', 'pass_286', 'pass_287', 'pass_288', 'pass_290', 'pass_292', 'pass_294', 'pass_296',\n",
    "          'pass_309', 'pass_311', 'pass_313', 'pass_314', 'pass_315', 'pass_316', 'pass_318', 'pass_320', 'pass_322', 'pass_324',\n",
    "          'pass_337', 'pass_339', 'pass_341', 'pass_342', 'pass_343', 'pass_344', 'pass_346', 'pass_348', 'pass_350', 'pass_352',\n",
    "          'pass_365', 'pass_367', 'pass_369', 'pass_370', 'pass_371', 'pass_372', 'pass_374', 'pass_376', 'pass_378', 'pass_391',\n",
    "          'pass_393', 'pass_395', 'pass_397', 'pass_398', 'pass_399', 'pass_400', 'pass_402', 'pass_404', 'pass_406', 'pass_419',\n",
    "          'pass_421', 'pass_423', 'pass_425', 'pass_426', 'pass_427', 'pass_428', 'pass_430', 'pass_432', 'pass_434', 'pass_447',\n",
    "          'pass_449', 'pass_451', 'pass_453', 'pass_454', 'pass_455', 'pass_456', 'pass_458', 'pass_460', 'pass_462', 'pass_475',\n",
    "          'pass_477', 'pass_479', 'pass_481', 'pass_482', 'pass_483', 'pass_484', 'pass_486', 'pass_488', 'pass_490', 'pass_503',\n",
    "          'pass_505', 'pass_507', 'pass_509', 'pass_510', 'pass_511', 'pass_512', 'pass_514', 'pass_516', 'pass_518', 'pass_531',\n",
    "          'pass_533', 'pass_535', 'pass_536', 'pass_537', 'pass_538', 'pass_539', 'pass_540', 'pass_542', 'pass_544', 'pass_546', 'pass_559',\n",
    "          'pass_561', 'pass_563', 'pass_564', 'pass_565', 'pass_566', 'pass_568', 'pass_570', 'pass_572', 'pass_574']\n",
    "\n",
    "passes_sub1  = ['pass_003', 'pass_005', 'pass_007', 'pass_008', 'pass_009', 'pass_010', 'pass_012', 'pass_014', 'pass_016', 'pass_018',\n",
    "                'pass_031', 'pass_033', 'pass_035', 'pass_036', 'pass_037', 'pass_038', 'pass_040', 'pass_042', 'pass_044', 'pass_046',\n",
    "                'pass_059', 'pass_061', 'pass_063', 'pass_064', 'pass_065', 'pass_066', 'pass_068', 'pass_070', 'pass_072', 'pass_074',\n",
    "                'pass_087', 'pass_089', 'pass_091', 'pass_092', 'pass_093', 'pass_094', 'pass_096', 'pass_098', 'pass_100',\n",
    "                'pass_113', 'pass_115', 'pass_117', 'pass_119', 'pass_120', 'pass_121', 'pass_122', 'pass_124', 'pass_126', 'pass_128',\n",
    "                'pass_141', 'pass_143', 'pass_145', 'pass_147', 'pass_148', 'pass_149', 'pass_150', 'pass_152', 'pass_154', 'pass_156',\n",
    "                'pass_169', 'pass_171', 'pass_173', 'pass_175', 'pass_176', 'pass_177', 'pass_178', 'pass_180', 'pass_182', 'pass_184',\n",
    "                'pass_197', 'pass_199', 'pass_201', 'pass_203', 'pass_204', 'pass_205', 'pass_206', 'pass_208', 'pass_210', 'pass_212',\n",
    "                'pass_225', 'pass_227', 'pass_229', 'pass_231', 'pass_232', 'pass_233', 'pass_234', 'pass_236', 'pass_238', 'pass_240',\n",
    "                'pass_253', 'pass_255', 'pass_257', 'pass_259', 'pass_260']\n",
    "\n",
    "passes_sub2  = ['pass_262', 'pass_264', 'pass_266', 'pass_268',\n",
    "                'pass_281', 'pass_283', 'pass_285', 'pass_286', 'pass_287', 'pass_288', 'pass_290', 'pass_292', 'pass_294', 'pass_296',\n",
    "                'pass_309', 'pass_311', 'pass_313', 'pass_314', 'pass_315', 'pass_316', 'pass_318', 'pass_320', 'pass_322', 'pass_324',\n",
    "                'pass_337', 'pass_339', 'pass_341', 'pass_342', 'pass_343', 'pass_344', 'pass_346', 'pass_348', 'pass_350', 'pass_352',\n",
    "                'pass_365', 'pass_367', 'pass_369', 'pass_370', 'pass_371', 'pass_372', 'pass_374', 'pass_376', 'pass_378', 'pass_391',\n",
    "                'pass_393', 'pass_395', 'pass_397', 'pass_398', 'pass_399', 'pass_400', 'pass_402', 'pass_404', 'pass_406', 'pass_419',\n",
    "                'pass_421', 'pass_423', 'pass_425', 'pass_426', 'pass_427', 'pass_428', 'pass_430', 'pass_432', 'pass_434', 'pass_447',\n",
    "                'pass_449', 'pass_451', 'pass_453', 'pass_454', 'pass_455', 'pass_456', 'pass_458', 'pass_460', 'pass_462', 'pass_475',\n",
    "                'pass_477', 'pass_479', 'pass_481', 'pass_482', 'pass_483', 'pass_484', 'pass_486', 'pass_488', 'pass_490', 'pass_503',\n",
    "                'pass_505', 'pass_507', 'pass_509', 'pass_510', 'pass_511', 'pass_512', 'pass_514', 'pass_516', 'pass_518', 'pass_531',\n",
    "                'pass_533', 'pass_535', 'pass_536', 'pass_537', 'pass_538', 'pass_539', 'pass_540', 'pass_542', 'pass_544', 'pass_546', 'pass_559',\n",
    "                'pass_561', 'pass_563', 'pass_564', 'pass_565', 'pass_566', 'pass_568', 'pass_570', 'pass_572', 'pass_574']\n",
    "\n",
    "passes_ascending = ['pass_003', 'pass_005', 'pass_007', 'pass_009',\n",
    "                    'pass_031', 'pass_033', 'pass_035', 'pass_037', \n",
    "                    'pass_059', 'pass_061', 'pass_063', 'pass_065', \n",
    "                    'pass_087', 'pass_089', 'pass_091', 'pass_093', \n",
    "                    'pass_113', 'pass_115', 'pass_117', 'pass_119', 'pass_121', \n",
    "                    'pass_141', 'pass_143', 'pass_145', 'pass_147', 'pass_149',\n",
    "                    'pass_169', 'pass_171', 'pass_173', 'pass_175', 'pass_177', \n",
    "                    'pass_197', 'pass_199', 'pass_201', 'pass_203', 'pass_205', \n",
    "                    'pass_225', 'pass_227', 'pass_229', 'pass_231', 'pass_233', \n",
    "                    'pass_253', 'pass_255', 'pass_257', 'pass_259', \n",
    "                    'pass_281', 'pass_283', 'pass_285', 'pass_287', \n",
    "                    'pass_309', 'pass_311', 'pass_313', 'pass_315', \n",
    "                    'pass_337', 'pass_339', 'pass_341', 'pass_343', \n",
    "                    'pass_365', 'pass_367', 'pass_369', 'pass_371', 'pass_391',\n",
    "                    'pass_393', 'pass_395', 'pass_397', 'pass_399', 'pass_419',\n",
    "                    'pass_421', 'pass_423', 'pass_425', 'pass_427', 'pass_447',\n",
    "                    'pass_449', 'pass_451', 'pass_453', 'pass_455', 'pass_475',\n",
    "                    'pass_477', 'pass_479', 'pass_481', 'pass_483', 'pass_503',\n",
    "                    'pass_505', 'pass_507', 'pass_509', 'pass_511', 'pass_531',\n",
    "                    'pass_533', 'pass_535', 'pass_537', 'pass_539', 'pass_559',\n",
    "                    'pass_561', 'pass_563', 'pass_565', ]\n",
    "\n",
    "passes_descending = ['pass_008', 'pass_010', 'pass_012', 'pass_014', 'pass_016', 'pass_018',\n",
    "                     'pass_036', 'pass_038', 'pass_040', 'pass_042', 'pass_044', 'pass_046',\n",
    "                     'pass_064', 'pass_066', 'pass_068', 'pass_070', 'pass_072', 'pass_074',\n",
    "                     'pass_092', 'pass_094', 'pass_096', 'pass_098', 'pass_100',\n",
    "                     'pass_120', 'pass_122', 'pass_124', 'pass_126', 'pass_128',\n",
    "                     'pass_148', 'pass_150', 'pass_152', 'pass_154', 'pass_156',\n",
    "                     'pass_176', 'pass_178', 'pass_180', 'pass_182', 'pass_184',\n",
    "                     'pass_204', 'pass_206', 'pass_208', 'pass_210', 'pass_212',\n",
    "                     'pass_232', 'pass_234', 'pass_236', 'pass_238', 'pass_240',\n",
    "                     'pass_260', 'pass_262', 'pass_264', 'pass_266', 'pass_268',\n",
    "                     'pass_286', 'pass_288', 'pass_290', 'pass_292', 'pass_294', 'pass_296',\n",
    "                     'pass_314', 'pass_316', 'pass_318', 'pass_320', 'pass_322', 'pass_324',\n",
    "                     'pass_342', 'pass_344', 'pass_346', 'pass_348', 'pass_350', 'pass_352',\n",
    "                     'pass_370', 'pass_372', 'pass_374', 'pass_376', 'pass_378', \n",
    "                     'pass_398', 'pass_400', 'pass_402', 'pass_404', 'pass_406', \n",
    "                     'pass_426', 'pass_428', 'pass_430', 'pass_432', 'pass_434', \n",
    "                     'pass_454', 'pass_456', 'pass_458', 'pass_460', 'pass_462',\n",
    "                     'pass_482', 'pass_484', 'pass_486', 'pass_488', 'pass_490',\n",
    "                     'pass_510', 'pass_512', 'pass_514', 'pass_516', 'pass_518', \n",
    "                     'pass_536', 'pass_538', 'pass_540', 'pass_542', 'pass_544', 'pass_546',\n",
    "                     'pass_564', 'pass_566', 'pass_568', 'pass_570', 'pass_572', 'pass_574']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_passes = ['pass_003', 'pass_005', 'pass_007', 'pass_008']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_passes = passes\n",
    "passes_name    = 'all_passes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Temporal before spatial : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_average_for_each_pass_to_be_used(ds,pass_nr):\n",
    "    ''' TEMPORAL MEAN OF UG_ROT AND VG_ROT : '''\n",
    "    UG_ROT_TEMP_AVG = np.nanmean(ds.ug_rot_SSHA_c, axis=0)\n",
    "    VG_ROT_TEMP_AVG = np.nanmean(ds.vg_rot_SSHA_c, axis=0)\n",
    "    ''' UNRAVEL SPEED VALUES : '''\n",
    "    UG_ROT      = np.ravel(UG_ROT_TEMP_AVG)\n",
    "    VG_ROT      = np.ravel(VG_ROT_TEMP_AVG)\n",
    "    LATITUDE    = np.ravel(ds.latitude[0])\n",
    "    LONGITUDE   = np.ravel(ds.longitude[0])\n",
    "    ''' GET RID OF NAN VALUES : '''\n",
    "    inval = ~np.isnan(UG_ROT) & ~np.isnan(VG_ROT)\n",
    "    lat_df        = LATITUDE[inval]\n",
    "    lon_df        = LONGITUDE[inval]\n",
    "    var_ug_df     = UG_ROT[inval]\n",
    "    var_vg_df     = VG_ROT[inval]\n",
    "    ''' 6 ) Save as temporary df for each pass : '''\n",
    "    df_temp = pd.DataFrame({'latitude'        : lat_df,\n",
    "                            'longitude'       : lon_df,\n",
    "                            'temp_avg_ug'     : var_ug_df,\n",
    "                            'temp_avg_vg'     : var_vg_df})\n",
    "    return df_temp\n",
    "\n",
    "def spatial_average_from_temporal_averaged(df_all_pass,extent_of_area,delta_lat_lon):\n",
    "    ''' FUNCTION TO SPATIALLY AVERAGE OR BIN (2D-STATISTIC) OF ALREADY TEMPORAL AVERAGED VELOCITIES '''\n",
    "    # Get the extent : \n",
    "    left_lon  = extent_of_area[0]\n",
    "    right_lon = extent_of_area[1]\n",
    "    down_lat  = extent_of_area[2]\n",
    "    up_lat    = extent_of_area[3]\n",
    "    # Get variables : \n",
    "    lat_var_for_binning    = df_all_pass['latitude'].values\n",
    "    lon_var_for_binning    = df_all_pass['longitude'].values\n",
    "    ug_var_for_binning     = df_all_pass['temp_avg_ug'].values\n",
    "    vg_var_for_binning     = df_all_pass['temp_avg_vg'].values\n",
    "    # Define Bins : \n",
    "    dlatlon = delta_lat_lon\n",
    "    lonbins = np.arange(left_lon, right_lon, dlatlon)\n",
    "    latbins = np.arange(down_lat, up_lat, dlatlon)\n",
    "    # Compute Statistics (mean) :\n",
    "    spatial_and_temporal_ug     = binned_statistic_2d(lat_var_for_binning, lon_var_for_binning, ug_var_for_binning,     bins=[latbins, lonbins], statistic='mean')\n",
    "    spatial_and_temporal_vg     = binned_statistic_2d(lat_var_for_binning, lon_var_for_binning, vg_var_for_binning,     bins=[latbins, lonbins], statistic='mean')\n",
    "    # Return : \n",
    "    return lonbins, latbins, spatial_and_temporal_ug, spatial_and_temporal_vg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE_NETCDF_2 = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/A_SSHA_CORR_NETCDF/'\n",
    "SAVE_NETCDF_2 = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/A_SSHA_CORR_NETCDF_2/'\n",
    "for pass_nr in passes:\n",
    "    print('Pass number : ', pass_nr)\n",
    "    ds = xr.open_dataset(SAVE_NETCDF_2+pass_nr+'.nc')\n",
    "    ''' TEMPORAL MEAN OF UG_ROT AND VG_ROT : '''\n",
    "    df_temp = temporal_average_for_each_pass_to_be_used(ds,pass_nr)\n",
    "    ''' STORE IN TEMP FOLDER : '''\n",
    "    #save_temp_path = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/AA_Total_Local_bulk_new/temp_spat_average/temp_files/'\n",
    "    save_temp_path = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/AA_Total_Local_bulk_new_2/temp_spat_average/temp_files/'\n",
    "    temp_file_name = 'temp_' + pass_nr + '.csv'\n",
    "    df_temp.to_csv(save_temp_path+temp_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Concatenate all DataFrames in the list into a single DataFrame : '''\n",
    "#temp_read_in_files = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/AA_Total_Local_bulk_new/temp_spat_average/temp_files/temp_*.csv'\n",
    "temp_read_in_files = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/AA_Total_Local_bulk_new_2/temp_spat_average/temp_files/temp_*.csv'\n",
    "df_all_pass = pd.concat([pd.read_csv(f) for f in glob.glob(temp_read_in_files)], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 9 ) Now make the Spatial average by binning into boxes of delta_lat_lon degree : '''\n",
    "lonbins, latbins, spatial_and_temporal_ug, spatial_and_temporal_vg = spatial_average_from_temporal_averaged(df_all_pass,extent_of_area,delta_lat_lon)\n",
    "''' 10 ) Create a dataframe for the spatially and temporally averaged velocities : '''\n",
    "# Extract the bin edges --- Assume all have same bins, as they should!\n",
    "lat_edges = spatial_and_temporal_ug.y_edge\n",
    "lon_edges = spatial_and_temporal_ug.x_edge\n",
    "# Compute midpoints for latitude and longitude bins\n",
    "lat_mids = 0.5 * (lat_edges[:-1] + lat_edges[1:])\n",
    "lon_mids = 0.5 * (lon_edges[:-1] + lon_edges[1:])\n",
    "# Create a meshgrid of lat and lon midpoints\n",
    "lon_mid_mesh, lat_mid_mesh = np.meshgrid(lat_mids, lon_mids)\n",
    "# Flatten the arrays for DataFrame\n",
    "lat_mid_flat = lat_mid_mesh.flatten()\n",
    "lon_mid_flat = lon_mid_mesh.flatten()\n",
    "spatial_and_temporal_ug_flat     = spatial_and_temporal_ug.statistic.flatten()\n",
    "spatial_and_temporal_vg_flat     = spatial_and_temporal_vg.statistic.flatten()\n",
    "# Create the DataFrame --- Could have saved in grid, but griddata needs unraveled later : \n",
    "df_temp_spat_avg = pd.DataFrame({'latitude'                    : lat_mid_flat,\n",
    "                                 'longitude'                   : lon_mid_flat,\n",
    "                                 'spatial_and_temporal_ug'     : spatial_and_temporal_ug_flat,\n",
    "                                 'spatial_and_temporal_vg'     : spatial_and_temporal_vg_flat})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_spat_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_spat_avg_nonan = df_temp_spat_avg.dropna(axis=0)\n",
    "df_temp_spat_avg_nonan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE PATH : \n",
    "path_temp_spat_avg = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/AA_Total_Local_bulk_new_2/temp_spat_average/'\n",
    "\n",
    "temp_spat_avg_full  = 'temp_spat_avg_full'+'_delta_lat_lon_'+str(delta_lat_lon)+'_extent_of_area_'+str(lon1)+'_'+str(lon2)+'_'+str(lat1)+'_'+str(lat2)+'_'+passes_name+'.csv'\n",
    "temp_spat_avg_nonan = 'temp_spat_avg_nonan'+'_delta_lat_lon_'+str(delta_lat_lon)+'_extent_of_area_'+str(lon1)+'_'+str(lon2)+'_'+str(lat1)+'_'+str(lat2)+'_'+passes_name+'.csv'\n",
    "\n",
    "df_temp_spat_avg.to_csv(path_temp_spat_avg+temp_spat_avg_full, index=False)\n",
    "df_temp_spat_avg_nonan.to_csv(path_temp_spat_avg+temp_spat_avg_nonan, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot and see : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binned_statistics_from_unraveled_df(df, variable_name, left_lon, right_lon, down_lat, up_lat, delta_lat_lon=0.2, statistic='count'):\n",
    "    # GET LAT, LON AND VARIABLE VALUES :\n",
    "    lat = df['lat'].values\n",
    "    lon = df['lon'].values\n",
    "    var = df[variable_name].values\n",
    "    # DEFINE BINS :\n",
    "    dlatlon = delta_lat_lon\n",
    "    lonbins = np.arange(left_lon, right_lon, dlatlon)\n",
    "    latbins = np.arange(down_lat, up_lat, dlatlon)\n",
    "    # COMPUTE STATISTICS :\n",
    "    mean_var = binned_statistic_2d(lat, lon, var, bins=[latbins, lonbins], statistic=statistic)\n",
    "    # RETURN :\n",
    "    return mean_var, lonbins, latbins\n",
    "def map_setup(figsize,projection):\n",
    "     fig = plt.figure(figsize=figsize)\n",
    "     ax = fig.add_subplot(projection=projection)\n",
    "     ax.add_feature(cfeature.LAND, facecolor='grey', zorder=100)\n",
    "     ax.add_feature(cfeature.COASTLINE, zorder=100)\n",
    "     #ax.gridlines(draw_labels=True)  \n",
    "     #ax.set_ylim(-80,80)\n",
    "     #ax.set_xlim(45,89)\n",
    "     return fig, ax\n",
    "def plot_binned_data_title(binned_data, lonbins, latbins, var_type, title, d_lat_lon, unit_t, cmap, vmin, vmax, cbar_aspect, figextent, bath_color, projection=ccrs.PlateCarree(), figsize=(10,10), bath2=None, bat_min=3700, bat_max=-20, bat_lvl=4, grid_p=True, pre_def_lvls=2, bath_line_stye='dashed', bath_line_width=1.5, bath_or_not=True):\n",
    "    full_title = title + f', resolution={d_lat_lon}'\n",
    "    cbar_title = title + f' unit={unit_t}'\n",
    "    if unit_t==None:\n",
    "        cbar_title = title\n",
    "    fig, ax = map_setup(figsize, projection)\n",
    "    plt.title(full_title, fontsize=28, fontweight='bold')\n",
    "    ax.set_extent(figextent, ccrs.PlateCarree())\n",
    "    im = ax.pcolormesh(lonbins, latbins, binned_data, cmap=cmap, transform=ccrs.PlateCarree(), vmin=vmin, vmax=vmax, shading='flat')\n",
    "    cbar = plt.colorbar(im, ax=ax, orientation='horizontal', fraction=0.05, pad=0.05, aspect=cbar_aspect, shrink=1)\n",
    "    cbar.set_label(cbar_title, fontsize=20)\n",
    "    if bath_or_not == True: \n",
    "        if pre_def_lvls == False:\n",
    "            levels = np.linspace(bat_min, bat_max, bat_lvl)\n",
    "        else:\n",
    "            levels = pre_def_lvls\n",
    "        # NEW BATHYMETRY : \n",
    "        #left_lon  = figextent[0]\n",
    "        #right_lon = figextent[1]\n",
    "        #down_lat  = figextent[2]\n",
    "        #up_lat    = figextent[3]\n",
    "        #new_test = crop_and_iterp_bat_etopo(bath2, left_lon, right_lon, down_lat, up_lat, delta_lat_lon=d_lat_lon)\n",
    "        new_test = bath2\n",
    "        plt.contour(new_test['lon'], new_test['lat'], new_test['z'], levels=levels, colors=bath_color, linestyles=bath_line_stye, linewidths=bath_line_width, zorder=10, transform=ccrs.PlateCarree())\n",
    "    if grid_p == True:\n",
    "        ax.gridlines(draw_labels=grid_p)\n",
    "    plt.show()\n",
    "def plot_binned_data(binned_data, lonbins, latbins, font_size, title, d_lat_lon, unit_t, cmap, vmin, vmax, cbar_aspect, figextent, bath_color, projection=ccrs.PlateCarree(), figsize=(10,10), bath2=None, bat_min=3700, bat_max=-20, bat_lvl=4, grid_p=True, pre_def_lvls=2, bath_line_stye='dashed', bath_line_width=1.5, bath_or_not=True):\n",
    "    #full_title = title + f', resolution={d_lat_lon}'\n",
    "    cbar_title = title + f'{unit_t}'\n",
    "    if unit_t==None:\n",
    "        cbar_title = title\n",
    "    fig, ax = map_setup(figsize, projection)\n",
    "    #plt.title(full_title, fontsize=28, fontweight='bold')\n",
    "    ax.set_extent(figextent, ccrs.PlateCarree())\n",
    "    im = ax.pcolormesh(lonbins, latbins, binned_data, cmap=cmap, transform=ccrs.PlateCarree(), vmin=vmin, vmax=vmax, shading='flat')\n",
    "    cbar = plt.colorbar(im, ax=ax, orientation='horizontal', fraction=0.05, pad=0.005, aspect=cbar_aspect, shrink=0.95)\n",
    "    cbar.set_label(cbar_title, fontsize=font_size)\n",
    "    if bath_or_not == True: \n",
    "        if pre_def_lvls == False:\n",
    "            levels = np.linspace(bat_min, bat_max, bat_lvl)\n",
    "        else:\n",
    "            levels = pre_def_lvls\n",
    "        # NEW BATHYMETRY : \n",
    "        #left_lon  = figextent[0]\n",
    "        #right_lon = figextent[1]\n",
    "        #down_lat  = figextent[2]\n",
    "        #up_lat    = figextent[3]\n",
    "        #new_test = crop_and_iterp_bat_etopo(bath2, left_lon, right_lon, down_lat, up_lat, delta_lat_lon=d_lat_lon)\n",
    "        new_test = bath2\n",
    "        plt.contour(new_test['lon'], new_test['lat'], new_test['z'], levels=levels, colors=bath_color, linestyles=bath_line_stye, linewidths=bath_line_width, zorder=10, transform=ccrs.PlateCarree())\n",
    "    if grid_p == True:\n",
    "        ax.gridlines(draw_labels=grid_p)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binned_data(spatial_and_temporal_ug.statistic, lonbins, latbins, 15, 'None', delta_lat_lon, None, cmap=cmocean.cm.balance, vmin=-0.05, vmax=0.05, cbar_aspect=50, figextent=[-70, 16, 45, 79], bath_color='white', projection=ccrs.PlateCarree(), figsize=(15,10), bath2=None, bat_min=3700, bat_max=-20, bat_lvl=4, grid_p=True, pre_def_lvls=2, bath_line_stye='dashed', bath_line_width=1.5, bath_or_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binned_data(spatial_and_temporal_ug.statistic, lonbins, latbins, 15, 'None', delta_lat_lon, None, cmap=cmocean.cm.balance, vmin=-0.05, vmax=0.05, cbar_aspect=50, figextent=[-70, 16, 45, 79], bath_color='white', projection=ccrs.PlateCarree(), figsize=(15,10), bath2=None, bat_min=3700, bat_max=-20, bat_lvl=4, grid_p=True, pre_def_lvls=2, bath_line_stye='dashed', bath_line_width=1.5, bath_or_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16,10])\n",
    "plt.scatter(x=df_temp_spat_avg['longitude'],y=df_temp_spat_avg['latitude'],c=df_temp_spat_avg['spatial_and_temporal_vg'],cmap=cmocean.cm.balance, vmin=-0.05, vmax=0.05,s=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Spatial before temporal : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_average_from_one_cycle_averaged(df_all_pass,extent_of_area,delta_lat_lon):\n",
    "    ''' FUNCTION TO SPATIALLY AVERAGE OR BIN (2D-STATISTIC) OF ALREADY TEMPORAL AVERAGED VELOCITIES '''\n",
    "    # Get the extent : \n",
    "    left_lon  = extent_of_area[0]\n",
    "    right_lon = extent_of_area[1]\n",
    "    down_lat  = extent_of_area[2]\n",
    "    up_lat    = extent_of_area[3]\n",
    "    # Get variables : \n",
    "    lat_var_for_binning    = df_all_pass['lat'].values\n",
    "    lon_var_for_binning    = df_all_pass['lon'].values\n",
    "    ug_var_for_binning     = df_all_pass['UG_ROT'].values\n",
    "    vg_var_for_binning     = df_all_pass['VG_ROT'].values\n",
    "    # Define Bins : \n",
    "    dlatlon = delta_lat_lon\n",
    "    lonbins = np.arange(left_lon, right_lon, dlatlon)\n",
    "    latbins = np.arange(down_lat, up_lat, dlatlon)\n",
    "    # Compute Statistics (mean) :\n",
    "    spatial_and_temporal_ug     = binned_statistic_2d(lat_var_for_binning, lon_var_for_binning, ug_var_for_binning,     bins=[latbins, lonbins], statistic='mean')\n",
    "    spatial_and_temporal_vg     = binned_statistic_2d(lat_var_for_binning, lon_var_for_binning, vg_var_for_binning,     bins=[latbins, lonbins], statistic='mean')\n",
    "    # Return : \n",
    "    return lonbins, latbins, spatial_and_temporal_ug, spatial_and_temporal_vg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_cycle_no_nan(path_cycles,cycle_nr):\n",
    "    df_cyc_files = '*_cyc_'+cycle_nr+'_*.csv'\n",
    "    df_all_pass = pd.concat([pd.read_csv(f) for f in glob.glob(path_cycles+df_cyc_files)], ignore_index = True)\n",
    "    df_all_pass2 = df_all_pass.dropna(axis=0)\n",
    "    ''' BINNING : '''\n",
    "    lonbins, latbins, spatial_and_temporal_ug, spatial_and_temporal_vg = spatial_average_from_one_cycle_averaged(df_all_pass2,extent_of_area,delta_lat_lon)\n",
    "    ''' Create a dataframe for the spatially and temporally averaged velocities : '''\n",
    "    # Extract the bin edges --- Assume all have same bins, as they should!\n",
    "    lat_edges = spatial_and_temporal_ug.y_edge\n",
    "    lon_edges = spatial_and_temporal_ug.x_edge\n",
    "    # Compute midpoints for latitude and longitude bins\n",
    "    lat_mids = 0.5 * (lat_edges[:-1] + lat_edges[1:])\n",
    "    lon_mids = 0.5 * (lon_edges[:-1] + lon_edges[1:])\n",
    "    # Create a meshgrid of lat and lon midpoints\n",
    "    lon_mid_mesh, lat_mid_mesh = np.meshgrid(lat_mids, lon_mids)\n",
    "    # Flatten the arrays for DataFrame\n",
    "    lat_mid_flat = lat_mid_mesh.flatten()\n",
    "    lon_mid_flat = lon_mid_mesh.flatten()\n",
    "    spatial_and_temporal_ug_flat     = spatial_and_temporal_ug.statistic.flatten()\n",
    "    spatial_and_temporal_vg_flat     = spatial_and_temporal_vg.statistic.flatten()\n",
    "    # Create the DataFrame --- Could have saved in grid, but griddata needs unraveled later : \n",
    "    df_temp_spat_avg = pd.DataFrame({'latitude'               : lat_mid_flat,\n",
    "                                    'longitude'               : lon_mid_flat,\n",
    "                                    'spatial_and_temporal_ug' : spatial_and_temporal_ug_flat,\n",
    "                                    'spatial_and_temporal_vg' : spatial_and_temporal_vg_flat})\n",
    "    return df_temp_spat_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path over all files : \n",
    "path_ug_vg_corected_per_cycle = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/A_SSHA_UG_VG_CSV/'\n",
    "\n",
    "list_of_cycles = ['000','001','002','003','004','005','006','007','008','009',\n",
    "                  '010','011','012','013','014','015','016','017','018','019',\n",
    "                  '020','021','022','023','024','025','026','027','028']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cyc_01 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[1])\n",
    "df_cyc_02 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[2])\n",
    "df_cyc_03 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[3])\n",
    "df_cyc_04 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[4])\n",
    "df_cyc_05 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[5])\n",
    "df_cyc_06 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[6])\n",
    "df_cyc_07 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[7])\n",
    "df_cyc_08 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[8])\n",
    "df_cyc_09 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[9])\n",
    "df_cyc_10 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[10])\n",
    "df_cyc_11 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[11])\n",
    "df_cyc_12 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[12])\n",
    "df_cyc_13 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[13])\n",
    "df_cyc_14 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[14])\n",
    "df_cyc_15 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[15])\n",
    "df_cyc_16 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[16])\n",
    "df_cyc_17 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[17])\n",
    "df_cyc_18 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[18])\n",
    "df_cyc_19 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[19])\n",
    "df_cyc_20 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[20])\n",
    "df_cyc_21 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[21])\n",
    "df_cyc_22 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[22])\n",
    "df_cyc_23 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[23])\n",
    "df_cyc_24 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[24])\n",
    "df_cyc_25 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[25])\n",
    "df_cyc_26 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[26])\n",
    "df_cyc_27 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[27])\n",
    "df_cyc_28 = get_one_cycle_no_nan(path_ug_vg_corected_per_cycle,list_of_cycles[28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cyc_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_ug_vg_cycles(dataframe_cyc):\n",
    "    dataframe_cyc_d = dataframe_cyc.drop('latitude', axis=1)\n",
    "    dataframe_cyc_d2 = dataframe_cyc_d.drop('longitude', axis=1)\n",
    "    dataframe_cyc_ug = dataframe_cyc_d2.drop('spatial_and_temporal_vg', axis=1)\n",
    "    dataframe_cyc_vg = dataframe_cyc_d2.drop('spatial_and_temporal_ug', axis=1)\n",
    "    return dataframe_cyc_ug, dataframe_cyc_vg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cyc_01_ug, df_cyc_01_vg = drop_ug_vg_cycles(df_cyc_01)\n",
    "df_cyc_02_ug, df_cyc_02_vg = drop_ug_vg_cycles(df_cyc_02)\n",
    "df_cyc_03_ug, df_cyc_03_vg = drop_ug_vg_cycles(df_cyc_03)\n",
    "df_cyc_04_ug, df_cyc_04_vg = drop_ug_vg_cycles(df_cyc_04)\n",
    "df_cyc_05_ug, df_cyc_05_vg = drop_ug_vg_cycles(df_cyc_05)\n",
    "df_cyc_06_ug, df_cyc_06_vg = drop_ug_vg_cycles(df_cyc_06)\n",
    "df_cyc_07_ug, df_cyc_07_vg = drop_ug_vg_cycles(df_cyc_07)\n",
    "df_cyc_08_ug, df_cyc_08_vg = drop_ug_vg_cycles(df_cyc_08)\n",
    "df_cyc_09_ug, df_cyc_09_vg = drop_ug_vg_cycles(df_cyc_09)\n",
    "df_cyc_10_ug, df_cyc_10_vg = drop_ug_vg_cycles(df_cyc_10)\n",
    "df_cyc_11_ug, df_cyc_11_vg = drop_ug_vg_cycles(df_cyc_11)\n",
    "df_cyc_12_ug, df_cyc_12_vg = drop_ug_vg_cycles(df_cyc_12)\n",
    "df_cyc_13_ug, df_cyc_13_vg = drop_ug_vg_cycles(df_cyc_13)\n",
    "df_cyc_14_ug, df_cyc_14_vg = drop_ug_vg_cycles(df_cyc_14)\n",
    "df_cyc_15_ug, df_cyc_15_vg = drop_ug_vg_cycles(df_cyc_15)\n",
    "df_cyc_16_ug, df_cyc_16_vg = drop_ug_vg_cycles(df_cyc_16)\n",
    "df_cyc_17_ug, df_cyc_17_vg = drop_ug_vg_cycles(df_cyc_17)\n",
    "df_cyc_18_ug, df_cyc_18_vg = drop_ug_vg_cycles(df_cyc_18)\n",
    "df_cyc_19_ug, df_cyc_19_vg = drop_ug_vg_cycles(df_cyc_19)\n",
    "df_cyc_20_ug, df_cyc_20_vg = drop_ug_vg_cycles(df_cyc_20)\n",
    "df_cyc_21_ug, df_cyc_21_vg = drop_ug_vg_cycles(df_cyc_21)\n",
    "df_cyc_22_ug, df_cyc_22_vg = drop_ug_vg_cycles(df_cyc_22)\n",
    "df_cyc_23_ug, df_cyc_23_vg = drop_ug_vg_cycles(df_cyc_23)\n",
    "df_cyc_24_ug, df_cyc_24_vg = drop_ug_vg_cycles(df_cyc_24)\n",
    "df_cyc_25_ug, df_cyc_25_vg = drop_ug_vg_cycles(df_cyc_25)\n",
    "df_cyc_26_ug, df_cyc_26_vg = drop_ug_vg_cycles(df_cyc_26)\n",
    "df_cyc_27_ug, df_cyc_27_vg = drop_ug_vg_cycles(df_cyc_27)\n",
    "df_cyc_28_ug, df_cyc_28_vg = drop_ug_vg_cycles(df_cyc_28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_vg = pd.concat([df_cyc_01_vg,df_cyc_02_vg,df_cyc_03_vg,df_cyc_04_vg,df_cyc_05_vg,df_cyc_06_vg,df_cyc_07_vg,df_cyc_08_vg,df_cyc_09_vg,df_cyc_10_vg,df_cyc_11_vg,df_cyc_12_vg,df_cyc_13_vg,df_cyc_14_vg,df_cyc_15_vg,df_cyc_16_vg,df_cyc_17_vg,df_cyc_18_vg,df_cyc_19_vg,df_cyc_20_vg,df_cyc_21_vg,df_cyc_22_vg,df_cyc_23_vg,df_cyc_24_vg,df_cyc_25_vg,df_cyc_26_vg,df_cyc_27_vg,df_cyc_28_vg], axis=1)  # Combine along rows\n",
    "combined_df_vg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_ug = pd.concat([df_cyc_01_ug,df_cyc_02_ug,df_cyc_03_ug,df_cyc_04_ug,df_cyc_05_ug,df_cyc_06_ug,df_cyc_07_ug,df_cyc_08_ug,df_cyc_09_ug,df_cyc_10_ug,df_cyc_11_ug,df_cyc_12_ug,df_cyc_13_ug,df_cyc_14_ug,df_cyc_15_ug,df_cyc_16_ug,df_cyc_17_ug,df_cyc_18_ug,df_cyc_19_ug,df_cyc_20_ug,df_cyc_21_ug,df_cyc_22_ug,df_cyc_23_ug,df_cyc_24_ug,df_cyc_25_ug,df_cyc_26_ug,df_cyc_27_ug,df_cyc_28_ug], axis=1)  # Combine along rows\n",
    "combined_df_ug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanmean_ug = combined_df_ug.mean(axis=1)\n",
    "nanmean_ug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanmean_vg = combined_df_vg.mean(axis=1)\n",
    "nanmean_vg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude1 = df_cyc_01.drop('spatial_and_temporal_vg', axis=1)\n",
    "latitude2 = latitude1.drop('spatial_and_temporal_ug', axis=1)\n",
    "latitude  = latitude2.drop('longitude', axis=1)\n",
    "latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitude1 = df_cyc_01.drop('spatial_and_temporal_vg', axis=1)\n",
    "longitude2 = longitude1.drop('spatial_and_temporal_ug', axis=1)\n",
    "longitude  = longitude2.drop('latitude', axis=1)\n",
    "longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spat_temp_avg = pd.concat([latitude,longitude,nanmean_ug,nanmean_vg], axis=1)  # Combine along rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spat_temp_avg.columns = ['latitude', 'longitude', 'spat_temp_ug', 'spat_temp_vg']\n",
    "df_spat_temp_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spat_temp_avg_nonan = df_spat_temp_avg.dropna(axis=0)\n",
    "df_spat_temp_avg_nonan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE PATH : \n",
    "path_temp_spat_avg = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/AA_Total_Local_bulk_new/spat_temp_average/'\n",
    "\n",
    "temp_spat_avg_full  = 'spat_temp_avg_full'+'_delta_lat_lon_'+str(delta_lat_lon)+'_extent_of_area_'+str(lon1)+'_'+str(lon2)+'_'+str(lat1)+'_'+str(lat2)+'_'+passes_name+'.csv'\n",
    "temp_spat_avg_nonan = 'spat_temp_avg_nonan'+'_delta_lat_lon_'+str(delta_lat_lon)+'_extent_of_area_'+str(lon1)+'_'+str(lon2)+'_'+str(lat1)+'_'+str(lat2)+'_'+passes_name+'.csv'\n",
    "\n",
    "df_spat_temp_avg.to_csv(path_temp_spat_avg+temp_spat_avg_full, index=False)\n",
    "df_spat_temp_avg_nonan.to_csv(path_temp_spat_avg+temp_spat_avg_nonan, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot and see : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binned_statistics_from_unraveled_df(df, variable_name, left_lon, right_lon, down_lat, up_lat, delta_lat_lon=0.2, statistic='count'):\n",
    "    # GET LAT, LON AND VARIABLE VALUES :\n",
    "    lat = df['lat'].values\n",
    "    lon = df['lon'].values\n",
    "    var = df[variable_name].values\n",
    "    # DEFINE BINS :\n",
    "    dlatlon = delta_lat_lon\n",
    "    lonbins = np.arange(left_lon, right_lon, dlatlon)\n",
    "    latbins = np.arange(down_lat, up_lat, dlatlon)\n",
    "    # COMPUTE STATISTICS :\n",
    "    mean_var = binned_statistic_2d(lat, lon, var, bins=[latbins, lonbins], statistic=statistic)\n",
    "    # RETURN :\n",
    "    return mean_var, lonbins, latbins\n",
    "def map_setup(figsize,projection):\n",
    "     fig = plt.figure(figsize=figsize)\n",
    "     ax = fig.add_subplot(projection=projection)\n",
    "     ax.add_feature(cfeature.LAND, facecolor='grey', zorder=100)\n",
    "     ax.add_feature(cfeature.COASTLINE, zorder=100)\n",
    "     #ax.gridlines(draw_labels=True)  \n",
    "     #ax.set_ylim(-80,80)\n",
    "     #ax.set_xlim(45,89)\n",
    "     return fig, ax\n",
    "def plot_binned_data_title(binned_data, lonbins, latbins, var_type, title, d_lat_lon, unit_t, cmap, vmin, vmax, cbar_aspect, figextent, bath_color, projection=ccrs.PlateCarree(), figsize=(10,10), bath2=None, bat_min=3700, bat_max=-20, bat_lvl=4, grid_p=True, pre_def_lvls=2, bath_line_stye='dashed', bath_line_width=1.5, bath_or_not=True):\n",
    "    full_title = title + f', resolution={d_lat_lon}'\n",
    "    cbar_title = title + f' unit={unit_t}'\n",
    "    if unit_t==None:\n",
    "        cbar_title = title\n",
    "    fig, ax = map_setup(figsize, projection)\n",
    "    plt.title(full_title, fontsize=28, fontweight='bold')\n",
    "    ax.set_extent(figextent, ccrs.PlateCarree())\n",
    "    im = ax.pcolormesh(lonbins, latbins, binned_data, cmap=cmap, transform=ccrs.PlateCarree(), vmin=vmin, vmax=vmax, shading='flat')\n",
    "    cbar = plt.colorbar(im, ax=ax, orientation='horizontal', fraction=0.05, pad=0.05, aspect=cbar_aspect, shrink=1)\n",
    "    cbar.set_label(cbar_title, fontsize=20)\n",
    "    if bath_or_not == True: \n",
    "        if pre_def_lvls == False:\n",
    "            levels = np.linspace(bat_min, bat_max, bat_lvl)\n",
    "        else:\n",
    "            levels = pre_def_lvls\n",
    "        # NEW BATHYMETRY : \n",
    "        #left_lon  = figextent[0]\n",
    "        #right_lon = figextent[1]\n",
    "        #down_lat  = figextent[2]\n",
    "        #up_lat    = figextent[3]\n",
    "        #new_test = crop_and_iterp_bat_etopo(bath2, left_lon, right_lon, down_lat, up_lat, delta_lat_lon=d_lat_lon)\n",
    "        new_test = bath2\n",
    "        plt.contour(new_test['lon'], new_test['lat'], new_test['z'], levels=levels, colors=bath_color, linestyles=bath_line_stye, linewidths=bath_line_width, zorder=10, transform=ccrs.PlateCarree())\n",
    "    if grid_p == True:\n",
    "        ax.gridlines(draw_labels=grid_p)\n",
    "    plt.show()\n",
    "def plot_binned_data(binned_data, lonbins, latbins, font_size, title, d_lat_lon, unit_t, cmap, vmin, vmax, cbar_aspect, figextent, bath_color, projection=ccrs.PlateCarree(), figsize=(10,10), bath2=None, bat_min=3700, bat_max=-20, bat_lvl=4, grid_p=True, pre_def_lvls=2, bath_line_stye='dashed', bath_line_width=1.5, bath_or_not=True):\n",
    "    #full_title = title + f', resolution={d_lat_lon}'\n",
    "    cbar_title = title + f'{unit_t}'\n",
    "    if unit_t==None:\n",
    "        cbar_title = title\n",
    "    fig, ax = map_setup(figsize, projection)\n",
    "    #plt.title(full_title, fontsize=28, fontweight='bold')\n",
    "    ax.set_extent(figextent, ccrs.PlateCarree())\n",
    "    im = ax.pcolormesh(lonbins, latbins, binned_data, cmap=cmap, transform=ccrs.PlateCarree(), vmin=vmin, vmax=vmax, shading='flat')\n",
    "    cbar = plt.colorbar(im, ax=ax, orientation='horizontal', fraction=0.05, pad=0.005, aspect=cbar_aspect, shrink=0.95)\n",
    "    cbar.set_label(cbar_title, fontsize=font_size)\n",
    "    if bath_or_not == True: \n",
    "        if pre_def_lvls == False:\n",
    "            levels = np.linspace(bat_min, bat_max, bat_lvl)\n",
    "        else:\n",
    "            levels = pre_def_lvls\n",
    "        # NEW BATHYMETRY : \n",
    "        #left_lon  = figextent[0]\n",
    "        #right_lon = figextent[1]\n",
    "        #down_lat  = figextent[2]\n",
    "        #up_lat    = figextent[3]\n",
    "        #new_test = crop_and_iterp_bat_etopo(bath2, left_lon, right_lon, down_lat, up_lat, delta_lat_lon=d_lat_lon)\n",
    "        new_test = bath2\n",
    "        plt.contour(new_test['lon'], new_test['lat'], new_test['z'], levels=levels, colors=bath_color, linestyles=bath_line_stye, linewidths=bath_line_width, zorder=10, transform=ccrs.PlateCarree())\n",
    "    if grid_p == True:\n",
    "        ax.gridlines(draw_labels=grid_p)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binned_data(spatial_and_temporal_ug.statistic, lonbins, latbins, 15, 'None', delta_lat_lon, None, cmap=cmocean.cm.balance, vmin=-0.05, vmax=0.05, cbar_aspect=50, figextent=[-70, 16, 45, 79], bath_color='white', projection=ccrs.PlateCarree(), figsize=(15,10), bath2=None, bat_min=3700, bat_max=-20, bat_lvl=4, grid_p=True, pre_def_lvls=2, bath_line_stye='dashed', bath_line_width=1.5, bath_or_not=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16,10])\n",
    "plt.scatter(x=df_spat_temp_avg['longitude'],y=df_spat_temp_avg['latitude'],c=df_spat_temp_avg['spat_temp_vg'],cmap=cmocean.cm.balance, vmin=-0.05, vmax=0.05,s=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Calculate and save in files for each track eddy-part - Alignment, Topostrophy, Orthostrophy, Total, Local and Bulk Variance : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spat_temp_average_path = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/AA_Total_Local_bulk_new/spat_temp_average/'\n",
    "spat_temp_average_file = 'spat_temp_avg_nonan_delta_lat_lon_0.1_extent_of_area_-80_30_45_78_all_passes.csv'\n",
    "#spat_temp_average_file = 'spat_temp_avg_full_delta_lat_lon_0.1_extent_of_area_-80_30_45_78_all_passes.csv'\n",
    "temp_spat_average_path = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/AA_Total_Local_bulk_new/temp_spat_average/'\n",
    "temp_spat_average_file = 'temp_spat_avg_nonan_delta_lat_lon_0.1_extent_of_area_-80_30_45_78_all_passes.csv'\n",
    "#temp_spat_average_file = 'temp_spat_avg_full_delta_lat_lon_0.1_extent_of_area_-80_30_45_78_all_passes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_spat_average_path = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/AA_Total_Local_bulk_new_2/temp_spat_average/'\n",
    "temp_spat_average_file = 'temp_spat_avg_nonan_delta_lat_lon_0.1_extent_of_area_-80_30_45_78_all_passes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_spat_temp_avg_nonan = pd.read_csv(spat_temp_average_path+spat_temp_average_file)\n",
    "df_temp_spat_avg_nonan = pd.read_csv(temp_spat_average_path+temp_spat_average_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing : Look at them :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spat_temp_avg_nonan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_spat_avg_nonan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binned_statistics_from_unraveled_df(df, variable_name, left_lon, right_lon, down_lat, up_lat, delta_lat_lon=0.2, statistic='count'):\n",
    "    # GET LAT, LON AND VARIABLE VALUES :\n",
    "    lat = df['lat'].values\n",
    "    lon = df['lon'].values\n",
    "    var = df[variable_name].values\n",
    "    # DEFINE BINS :\n",
    "    dlatlon = delta_lat_lon\n",
    "    lonbins = np.arange(left_lon, right_lon, dlatlon)\n",
    "    latbins = np.arange(down_lat, up_lat, dlatlon)\n",
    "    # COMPUTE STATISTICS :\n",
    "    mean_var = binned_statistic_2d(lat, lon, var, bins=[latbins, lonbins], statistic=statistic)\n",
    "    # RETURN :\n",
    "    return mean_var, lonbins, latbins\n",
    "def map_setup(figsize,projection):\n",
    "     fig = plt.figure(figsize=figsize)\n",
    "     ax = fig.add_subplot(projection=projection)\n",
    "     ax.add_feature(cfeature.LAND, facecolor='grey', zorder=100)\n",
    "     ax.add_feature(cfeature.COASTLINE, zorder=100)\n",
    "     #ax.gridlines(draw_labels=True)  \n",
    "     #ax.set_ylim(-80,80)\n",
    "     #ax.set_xlim(45,89)\n",
    "     return fig, ax\n",
    "def plot_binned_data_title(binned_data, lonbins, latbins, var_type, title, d_lat_lon, unit_t, cmap, vmin, vmax, cbar_aspect, figextent, bath_color, projection=ccrs.PlateCarree(), figsize=(10,10), bath2=None, bat_min=3700, bat_max=-20, bat_lvl=4, grid_p=True, pre_def_lvls=2, bath_line_stye='dashed', bath_line_width=1.5, bath_or_not=True):\n",
    "    full_title = title + f', resolution={d_lat_lon}'\n",
    "    cbar_title = title + f' unit={unit_t}'\n",
    "    if unit_t==None:\n",
    "        cbar_title = title\n",
    "    fig, ax = map_setup(figsize, projection)\n",
    "    plt.title(full_title, fontsize=28, fontweight='bold')\n",
    "    ax.set_extent(figextent, ccrs.PlateCarree())\n",
    "    im = ax.pcolormesh(lonbins, latbins, binned_data, cmap=cmap, transform=ccrs.PlateCarree(), vmin=vmin, vmax=vmax, shading='flat')\n",
    "    cbar = plt.colorbar(im, ax=ax, orientation='horizontal', fraction=0.05, pad=0.05, aspect=cbar_aspect, shrink=1)\n",
    "    cbar.set_label(cbar_title, fontsize=20)\n",
    "    if bath_or_not == True: \n",
    "        if pre_def_lvls == False:\n",
    "            levels = np.linspace(bat_min, bat_max, bat_lvl)\n",
    "        else:\n",
    "            levels = pre_def_lvls\n",
    "        # NEW BATHYMETRY : \n",
    "        #left_lon  = figextent[0]\n",
    "        #right_lon = figextent[1]\n",
    "        #down_lat  = figextent[2]\n",
    "        #up_lat    = figextent[3]\n",
    "        #new_test = crop_and_iterp_bat_etopo(bath2, left_lon, right_lon, down_lat, up_lat, delta_lat_lon=d_lat_lon)\n",
    "        new_test = bath2\n",
    "        plt.contour(new_test['lon'], new_test['lat'], new_test['z'], levels=levels, colors=bath_color, linestyles=bath_line_stye, linewidths=bath_line_width, zorder=10, transform=ccrs.PlateCarree())\n",
    "    if grid_p == True:\n",
    "        ax.gridlines(draw_labels=grid_p)\n",
    "    plt.show()\n",
    "def plot_binned_data(binned_data, lonbins, latbins, font_size, title, d_lat_lon, unit_t, cmap, vmin, vmax, cbar_aspect, figextent, bath_color, projection=ccrs.PlateCarree(), figsize=(10,10), bath2=None, bat_min=3700, bat_max=-20, bat_lvl=4, grid_p=True, pre_def_lvls=2, bath_line_stye='dashed', bath_line_width=1.5, bath_or_not=True):\n",
    "    #full_title = title + f', resolution={d_lat_lon}'\n",
    "    cbar_title = title + f'{unit_t}'\n",
    "    if unit_t==None:\n",
    "        cbar_title = title\n",
    "    fig, ax = map_setup(figsize, projection)\n",
    "    #plt.title(full_title, fontsize=28, fontweight='bold')\n",
    "    ax.set_extent(figextent, ccrs.PlateCarree())\n",
    "    im = ax.pcolormesh(lonbins, latbins, binned_data, cmap=cmap, transform=ccrs.PlateCarree(), vmin=vmin, vmax=vmax, shading='flat')\n",
    "    cbar = plt.colorbar(im, ax=ax, orientation='horizontal', fraction=0.05, pad=0.005, aspect=cbar_aspect, shrink=0.95)\n",
    "    cbar.set_label(cbar_title, fontsize=font_size)\n",
    "    if bath_or_not == True: \n",
    "        if pre_def_lvls == False:\n",
    "            levels = np.linspace(bat_min, bat_max, bat_lvl)\n",
    "        else:\n",
    "            levels = pre_def_lvls\n",
    "        # NEW BATHYMETRY : \n",
    "        #left_lon  = figextent[0]\n",
    "        #right_lon = figextent[1]\n",
    "        #down_lat  = figextent[2]\n",
    "        #up_lat    = figextent[3]\n",
    "        #new_test = crop_and_iterp_bat_etopo(bath2, left_lon, right_lon, down_lat, up_lat, delta_lat_lon=d_lat_lon)\n",
    "        new_test = bath2\n",
    "        plt.contour(new_test['lon'], new_test['lat'], new_test['z'], levels=levels, colors=bath_color, linestyles=bath_line_stye, linewidths=bath_line_width, zorder=10, transform=ccrs.PlateCarree())\n",
    "    if grid_p == True:\n",
    "        ax.gridlines(draw_labels=grid_p)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(projection=ccrs.LambertConformal(central_longitude=-25))\n",
    "ax.add_feature(cfeature.LAND, facecolor='grey', zorder=100)\n",
    "ax.add_feature(cfeature.COASTLINE, zorder=100)\n",
    "ax.set_extent([-80,30,45,78], ccrs.PlateCarree())\n",
    "ax.scatter(df_spat_temp_avg_nonan['longitude'],y=df_spat_temp_avg_nonan['latitude'],\n",
    "               c=df_spat_temp_avg_nonan['spat_temp_vg'],cmap=cmocean.cm.balance, vmin=-0.07, vmax=0.07,s=0.1, transform=ccrs.PlateCarree())\n",
    "plt.show()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(projection=ccrs.LambertConformal(central_longitude=-25))\n",
    "ax.add_feature(cfeature.LAND, facecolor='grey', zorder=100)\n",
    "ax.add_feature(cfeature.COASTLINE, zorder=100)\n",
    "ax.set_extent([-80,30,45,78], ccrs.PlateCarree())\n",
    "ax.scatter(df_spat_temp_avg_nonan['longitude'],y=df_spat_temp_avg_nonan['latitude'],\n",
    "               c=np.abs(df_spat_temp_avg_nonan['spat_temp_vg'])-np.abs(df_temp_spat_avg_nonan['spatial_and_temporal_vg']),cmap=cmocean.cm.balance, vmin=-0.07, vmax=0.07,s=0.1, transform=ccrs.PlateCarree())\n",
    "plt.show()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(projection=ccrs.LambertConformal(central_longitude=-25))\n",
    "ax.add_feature(cfeature.LAND, facecolor='grey', zorder=100)\n",
    "ax.add_feature(cfeature.COASTLINE, zorder=100)\n",
    "ax.set_extent([-80,30,45,78], ccrs.PlateCarree())\n",
    "ax.scatter(df_spat_temp_avg_nonan['longitude'],y=df_spat_temp_avg_nonan['latitude'],\n",
    "               c=df_temp_spat_avg_nonan['spatial_and_temporal_vg'],cmap=cmocean.cm.balance, vmin=-0.07, vmax=0.07,s=0.1, transform=ccrs.PlateCarree())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(projection=ccrs.LambertConformal(central_longitude=-10))\n",
    "ax.add_feature(cfeature.LAND, facecolor='grey', zorder=100)\n",
    "ax.add_feature(cfeature.COASTLINE, zorder=100)\n",
    "ax.set_extent([-30,30,55,78], ccrs.PlateCarree())\n",
    "a=ax.scatter(df_spat_temp_avg_nonan['longitude'],y=df_spat_temp_avg_nonan['latitude'],\n",
    "               c=np.abs(df_spat_temp_avg_nonan['spat_temp_vg'])-np.abs(df_temp_spat_avg_nonan['spatial_and_temporal_vg']),cmap=cmocean.cm.balance, vmin=-0.03, vmax=0.03,s=0.25, transform=ccrs.PlateCarree())\n",
    "plt.colorbar(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16,10])\n",
    "plt.scatter(x=df_spat_temp_avg_nonan['longitude'],y=df_spat_temp_avg_nonan['latitude'],c=np.abs(df_spat_temp_avg_nonan['spat_temp_vg'])-np.abs(df_temp_spat_avg_nonan['spatial_and_temporal_vg']),cmap=cmocean.cm.balance, vmin=-0.05, vmax=0.05,s=0.1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16,10])\n",
    "plt.scatter(x=df_spat_temp_avg_nonan['longitude'],y=df_spat_temp_avg_nonan['latitude'],c=df_spat_temp_avg_nonan['spat_temp_vg'],cmap=cmocean.cm.balance, vmin=-0.07, vmax=0.07,s=0.1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16,10])\n",
    "plt.scatter(x=df_temp_spat_avg_nonan['longitude'],y=df_temp_spat_avg_nonan['latitude'],c=df_temp_spat_avg_nonan['spatial_and_temporal_vg'],cmap=cmocean.cm.balance, vmin=-0.07, vmax=0.07,s=0.1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check smoothing og bath : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_for_pass = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SWOT_1_RAW_MERGED/version_2'\n",
    "ds_origi_raw = xr.open_dataset(path_for_pass+'/'+'pass_003'+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bath_swot = ds_origi_raw.depth_or_elevation # Should be the same for all cycles, so only need one\n",
    "bath_swot = ds_origi_raw.isel(concat_dim=12).depth_or_elevation \n",
    "bath_lat  = ds_origi_raw.isel(concat_dim=12).latitude\n",
    "bath_lon  = ds_origi_raw.isel(concat_dim=12).longitude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bath_swot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_one_cycle = SOFiA.smooth_multiple_times(bath_swot, 2)\n",
    "cleaned_one_cycle  = SOFiA.clean_smoothing(smoothed_one_cycle, bath_swot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_one_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines  = len(bath_swot['num_lines'])\n",
    "num_pixels = len(bath_swot['num_pixels'])\n",
    "\n",
    "num_lines,num_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bath_smoothed = xr.DataArray(cleaned_one_cycle, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bath_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(bath_swot,vmin=-5000,vmax=-3000)\n",
    "plt.ylim(100,200)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(bath_smoothed,vmin=-5000,vmax=-3000)\n",
    "plt.ylim(100,200)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_H_num_pixels_rotated, grad_H_num_lines_rotated = get_bathymetry_from_SWOT_file_orig_grid_and_Earth_grid(ds_origi_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(grad_H_num_pixels_rotated)#,vmin=-5000,vmax=-3000)\n",
    "plt.ylim(100,200)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_spat_temp_Earth_grid_to_satellite_pass_grid(df_avg_nonan, velocity_var, lat_sat_o, lon_sat_o):\n",
    "    ''' FUNCTION TO INTERPOLATE THE SPATIAL AND TEMPORALY AVERAGED VELODITIEC BACK TO SATELLITE-TRACK-GRID '''\n",
    "    # GRID POINTS\n",
    "    lat_grid = df_avg_nonan['latitude'].values\n",
    "    lon_grid = df_avg_nonan['longitude'].values\n",
    "    grid_values = df_avg_nonan[velocity_var].values\n",
    "    # SATELLITE-TRACK POINTS\n",
    "    lat_sat = np.ravel(lat_sat_o)\n",
    "    lon_sat = np.ravel(lon_sat_o)\n",
    "    # NB! Correct longitude, if not done before : \n",
    "    #lon_sat = np.where(lon_sat>180.0,lon_sat-360,lon_sat)\n",
    "    # Make Earth-grid points :\n",
    "    original_points = np.column_stack((lon_grid, lat_grid))\n",
    "    original_values = grid_values\n",
    "    # Create a mask for valid satellite coordinates (where neither is NaN) :\n",
    "    valid_mask = ~np.isnan(lat_sat) & ~np.isnan(lon_sat)\n",
    "    valid_sat_coords = np.column_stack((lon_sat[valid_mask], lat_sat[valid_mask]))\n",
    "    # Perform griddata interpolation on valid satellite coordinates :\n",
    "    interpolated_values = griddata(original_points, original_values, valid_sat_coords, method='nearest')\n",
    "    # Initialize a full array with NaNs to represent the satellite's grid values :\n",
    "    satellite_values = np.full(lat_sat.shape, np.nan)\n",
    "    satellite_values[valid_mask] = interpolated_values\n",
    "    # Reshape back to original shape : \n",
    "    interpolated_values_original_shape = satellite_values.reshape((lat_sat_o.shape))\n",
    "    return interpolated_values_original_shape, interpolated_values\n",
    "\n",
    "def calculate_total_local_bulk_covariance_for_each_track(df_avg_nonan,lat,lon,ug_rotated,vg_rotated):\n",
    "    ''' 4 ) Calculate Temporal mean ug and vg (for each track) : '''\n",
    "    ug_temp_mean        = np.nanmean(ug_rotated, axis=0)\n",
    "    vg_temp_mean        = np.nanmean(vg_rotated, axis=0)\n",
    "    ''' 5 ) Calculate Spatial and Temporal mean ug and vg (for each track) : '''\n",
    "    ug_spat_temp_mean        , temp = interpolate_spat_temp_Earth_grid_to_satellite_pass_grid(df_avg_nonan, velocity_var='spatial_and_temporal_ug',     lat_sat_o=lat, lon_sat_o=lon)\n",
    "    vg_spat_temp_mean        , temp = interpolate_spat_temp_Earth_grid_to_satellite_pass_grid(df_avg_nonan, velocity_var='spatial_and_temporal_vg',     lat_sat_o=lat, lon_sat_o=lon)\n",
    "\n",
    "    #ug_spat_temp_mean        , temp = interpolate_spat_temp_Earth_grid_to_satellite_pass_grid(df_avg_nonan, velocity_var='spat_temp_ug',     lat_sat_o=lat, lon_sat_o=lon)\n",
    "    #vg_spat_temp_mean        , temp = interpolate_spat_temp_Earth_grid_to_satellite_pass_grid(df_avg_nonan, velocity_var='spat_temp_vg',     lat_sat_o=lat, lon_sat_o=lon)\n",
    "    ''' 6 ) CALCULATE COVARIANCE MATRICES : '''\n",
    "    # TOTAL : \n",
    "    total_covariance_cuu = np.nanmean(np.multiply(ug_rotated - ug_spat_temp_mean, ug_rotated - ug_spat_temp_mean), axis=0)\n",
    "    total_covariance_cvv = np.nanmean(np.multiply(vg_rotated - vg_spat_temp_mean, vg_rotated - vg_spat_temp_mean), axis=0)\n",
    "    total_covariance_cuv = np.nanmean(np.multiply(ug_rotated - ug_spat_temp_mean, vg_rotated - vg_spat_temp_mean), axis=0)\n",
    "    # LOCAL : \n",
    "    local_covariance_cuu = np.nanmean(np.multiply(ug_rotated - ug_temp_mean, ug_rotated - ug_temp_mean), axis=0)\n",
    "    local_covariance_cvv = np.nanmean(np.multiply(vg_rotated - vg_temp_mean, vg_rotated - vg_temp_mean), axis=0)\n",
    "    local_covariance_cuv = np.nanmean(np.multiply(ug_rotated - ug_temp_mean, vg_rotated - vg_temp_mean), axis=0)\n",
    "    # BULK : \n",
    "    bulk_covariance_cuu = np.multiply(ug_temp_mean - ug_spat_temp_mean, ug_temp_mean - ug_spat_temp_mean)\n",
    "    bulk_covariance_cvv = np.multiply(vg_temp_mean - vg_spat_temp_mean, vg_temp_mean - vg_spat_temp_mean)\n",
    "    bulk_covariance_cuv = np.multiply(ug_temp_mean - ug_spat_temp_mean, vg_temp_mean - vg_spat_temp_mean)\n",
    "    # NEEDS SPATIAL AVERAGE OR BINNING FOR ALL TO BE COMPLETE !!!\n",
    "    return total_covariance_cuu, total_covariance_cvv, total_covariance_cuv, local_covariance_cuu, local_covariance_cvv, local_covariance_cuv, bulk_covariance_cuu, bulk_covariance_cvv, bulk_covariance_cuv\n",
    "\n",
    "def calculate_det_trc_a_b_theta(cuu,cvv,cuv):\n",
    "    ''' FUNCTION TO CALCULATE DETERMINANT, TRACE, A, B, AND THETHA\n",
    "        Can be on track-level or on binned-level.'''\n",
    "    ''' CALCULATE DETC, TRC : '''\n",
    "    detc = np.real(cuu*cvv-cuv**2)   # determinant of covariance matrix\n",
    "    trc = cuu + cvv                  # trace of covariance matrix\n",
    "    ''' CALCULATE A, B, THETA : '''\n",
    "    a     = np.sqrt(trc/2+np.sqrt(trc**2-4*detc)/2)  # semi-major axis\n",
    "    b     = np.sqrt(trc/2-np.sqrt(trc**2-4*detc)/2)  # semi-minor axis\n",
    "    theta = np.arctan2(2*cuv,cuu-cvv)/2              # orientation angle\n",
    "    return detc, trc, a, b, theta\n",
    "\n",
    "def get_bathymetry_from_SWOT_file_orig_grid_and_Earth_grid(ds_orig_raw_merged):\n",
    "    # Get dept_or_elevation layer from the original SWOT dataset : \n",
    "    bath_swot = ds_orig_raw_merged.depth_or_elevation # Should be the same for all cycles, so only need one\n",
    "    bath_swot = ds_orig_raw_merged.isel(concat_dim=12).depth_or_elevation \n",
    "    bath_lat  = ds_orig_raw_merged.isel(concat_dim=12).latitude\n",
    "    bath_lon  = ds_orig_raw_merged.isel(concat_dim=12).longitude \n",
    "    # Smoothing ? \n",
    "    smoothed_one_cycle = SOFiA.smooth_multiple_times(bath_swot, 2)\n",
    "    cleaned_one_cycle  = SOFiA.clean_smoothing(smoothed_one_cycle, bath_swot)\n",
    "    # Store ass DataArray\n",
    "    num_lines  = len(bath_swot['num_lines'])\n",
    "    num_pixels = len(bath_swot['num_pixels'])\n",
    "    bath_smoothed = xr.DataArray(cleaned_one_cycle, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    # dx, dy 2 km : \n",
    "    dx = 2000\n",
    "    dy = 2000\n",
    "    # Get Grad_H\n",
    "    #grad_H_num_lines  = np.gradient(bath_swot, axis=0)/dy # y-axis - local SWOT-grid\n",
    "    #grad_H_num_pixels = np.gradient(bath_swot, axis=1)/dx # x-axis - local SWOT-grid\n",
    "    grad_H_num_lines  = np.gradient(bath_smoothed, axis=0)/dy # y-axis - local SWOT-grid\n",
    "    grad_H_num_pixels = np.gradient(bath_smoothed, axis=1)/dx # x-axis - local SWOT-grid\n",
    "    # Rotate to Earth-Grid : \n",
    "    grad_H_num_pixels_rotated, grad_H_num_lines_rotated, grad_H_num_pixels_rotated_by_phi =  SOFiA.rotate_dataset_original_grid_all_cycles(bath_lat, bath_lon, grad_H_num_pixels, grad_H_num_lines)\n",
    "    return grad_H_num_pixels_rotated, grad_H_num_lines_rotated\n",
    "\n",
    "def calculate_polarized_vector_and_rotated_bathymetry_vector(a, b, theta, ds_of_track_for_topogrphy):\n",
    "    ''' 1 ) Get polarized part of variance : '''\n",
    "    # Polarized  part of velocity variance ellipse : \n",
    "    polarized_magnitude = np.sqrt(a**2 - b**2)\n",
    "    # Define the unit vector in the direction of the major axis\n",
    "    N = np.array([np.cos(theta), np.sin(theta)])\n",
    "    # Compute the polarized component\n",
    "    polarized_component = polarized_magnitude * N\n",
    "    ''' 2 ) Get barthymetry from the track : '''\n",
    "    grad_H_num_pixels_rotated, grad_H_num_lines_rotated = get_bathymetry_from_SWOT_file_orig_grid_and_Earth_grid(ds_of_track_for_topogrphy)\n",
    "    # Gradient vector\n",
    "    gradH = np.array([grad_H_num_pixels_rotated, grad_H_num_lines_rotated])\n",
    "    # Apply 90-degree rotation manually\n",
    "    grad_H_rot_90 = np.array([-grad_H_num_lines_rotated, grad_H_num_pixels_rotated])\n",
    "    ''' Another way to calculate : '''\n",
    "    # Rotation matrix\n",
    "    R_90 = np.array([[0, -1], [1, 0]])\n",
    "    # Rotate each vector in batch across provided dimensions [2,x,y]\n",
    "    # Reshape and permute to apply batch rotation per vector (2, dimensions)\n",
    "    gradH_rotated = np.einsum('ij,jkl->ikl', R_90, gradH)\n",
    "    # Ensure matching dimensions for computation\n",
    "    dot_product1 = np.einsum('ijm,ijm->jm', gradH_rotated, polarized_component)\n",
    "    return polarized_component, grad_H_rot_90\n",
    "\n",
    "def calculate_alignment_topostrophy_orthostrophy_of_variance(vector_a, topography_vector):\n",
    "    ''' FUNCTION TO CALCULATE THE ALIGNMENT, TOPOSTROPHY, AND ORTHOSTROPHY\n",
    "        This function do not normalize, can normalize at later scale, \n",
    "        when spatially averaged, or right away if looking at small scale. \n",
    "        Just a basic calculation ... '''\n",
    "    polarized_component = vector_a\n",
    "    grad_H_rot_90       = topography_vector\n",
    "    ''' 1 ) Calculate the dot-product between the two vectors : '''\n",
    "    # Compute dot product\n",
    "    #dot_product = np.dot(grad_H_rot_90, polarized_component)\n",
    "    dot_product2 = (grad_H_rot_90[0]*polarized_component[0] + grad_H_rot_90[1]*polarized_component[1])\n",
    "    magnitude_polarized  = (np.sqrt(polarized_component[0]**2+polarized_component[1]**2))\n",
    "    magnitude_bathymetry = (np.sqrt(grad_H_rot_90[0]**2+grad_H_rot_90[1]**2))\n",
    "    # Iterate over batches and compute dot product\n",
    "    #dot_product3 = np.sum(grad_H_rot_90 * polarized_component, axis=0)  # Axis to align sum if needed with dimensions\n",
    "    ''' ALIGNMENT :  '''\n",
    "    alignment   = np.abs(dot_product2)/(np.sqrt(polarized_component[0]**2+polarized_component[1]**2)*np.sqrt(grad_H_rot_90[0]**2+grad_H_rot_90[1]**2))\n",
    "    theta_cos = np.arccos(alignment)\n",
    "    #theta_cos_smooth = SOFiA.smooth_multiple_times(theta_cos,1)\n",
    "    ''' PROJECT A ONTO B : '''\n",
    "    flow_norm = polarized_component #/ (np.sqrt(polarized_component[0]**2+polarized_component[1]**2))\n",
    "    dot_product_a_b = (grad_H_rot_90[0]*flow_norm[0] + grad_H_rot_90[1]*flow_norm[1])\n",
    "    # Calculate squared norm safely by summing along the first axis (axis=0)\n",
    "    norm_squared = np.sum(grad_H_rot_90**2, axis=0)  # Shape should be (2062, 69)\n",
    "    # Element-wise condition to avoid division by zero\n",
    "    is_nonzero = norm_squared != 0  # Shape: (2062, 69)\n",
    "    # Modify norm_squared shape for broadcasting to ensure compatibility across operations\n",
    "    norm_squared = norm_squared[np.newaxis, :, :]  # Shape: (1, 2062, 69)\n",
    "    # Safe projection calculation using broadcasting\n",
    "    Proj_a_onto_b = np.where(is_nonzero[np.newaxis, :, :], (dot_product_a_b / norm_squared) * grad_H_rot_90, np.zeros_like(grad_H_rot_90))\n",
    "    #Proj_a_onto_b = np.abs(dot_product_a_b)/(grad_H_rot_90[0]*grad_H_rot_90[0] + grad_H_rot_90[1]*grad_H_rot_90[1])*grad_H_rot_90\n",
    "    non_topostrophy_a = flow_norm - Proj_a_onto_b\n",
    "    ''' ORTHOSTROPHY : '''\n",
    "    orthostrophy = np.sqrt(non_topostrophy_a[0]**2 + non_topostrophy_a[1]**2)\n",
    "    ''' TOPOSTROPHY : '''\n",
    "    topostrophy1 = np.abs(dot_product2)/(np.sqrt(grad_H_rot_90[0]**2+grad_H_rot_90[1]**2))\n",
    "    topostrophy  = np.sqrt(Proj_a_onto_b[0]**2 + Proj_a_onto_b[1]**2)\n",
    "    return dot_product2, magnitude_polarized, magnitude_bathymetry, alignment, topostrophy1, topostrophy, orthostrophy\n",
    "\n",
    "def create_dataframe_for_each_pass_tot_loc_bulk_a_b_ali_topo_ortho(SAVE_NETCDF_2, path_for_pass, pass_number, df_avg_nonan):\n",
    "    # Open passes : \n",
    "    ds_corrected = xr.open_dataset(SAVE_NETCDF_2+pass_number+'.nc')\n",
    "    ds_origi_raw = xr.open_dataset(path_for_pass+'/'+pass_number+'.nc')\n",
    "    # Get ug_rot and vg_rot :\n",
    "    lat_c = ds_corrected.latitude[0]\n",
    "    lon_c = ds_corrected.longitude[0]\n",
    "    ug_rot = ds_corrected.ug_rot_SSHA_c\n",
    "    vg_rot = ds_corrected.vg_rot_SSHA_c\n",
    "    # Calculate Total, Local and Bulk covariance :\n",
    "    total_covariance_cuu,total_covariance_cvv,total_covariance_cuv,local_covariance_cuu,local_covariance_cvv,local_covariance_cuv,bulk_covariance_cuu,bulk_covariance_cvv,bulk_covariance_cuv=calculate_total_local_bulk_covariance_for_each_track(df_avg_nonan,lat_c,lon_c,ug_rot,vg_rot)\n",
    "    # Calculate detc, trace, a, b, and theta for Total, Local, and Bulk covariance : \n",
    "    detc_T, trc_T, a_T, b_T, theta_T = calculate_det_trc_a_b_theta(total_covariance_cuu,total_covariance_cvv,total_covariance_cuv)\n",
    "    detc_L, trc_L, a_L, b_L, theta_L = calculate_det_trc_a_b_theta(local_covariance_cuu,local_covariance_cvv,local_covariance_cuv)\n",
    "    detc_B, trc_B, a_B, b_B, theta_B = calculate_det_trc_a_b_theta(bulk_covariance_cuu, bulk_covariance_cvv, bulk_covariance_cuv)\n",
    "    # Calculate Polarized vector and Rotated Bathymetry vector :\n",
    "    total_polarized_component, grad_H_rot_90 = calculate_polarized_vector_and_rotated_bathymetry_vector(a_T, b_T, theta_T, ds_origi_raw)\n",
    "    local_polarized_component, grad_H_rot_90 = calculate_polarized_vector_and_rotated_bathymetry_vector(a_L, b_L, theta_L, ds_origi_raw)\n",
    "    bulk_polarized_component, grad_H_rot_90  = calculate_polarized_vector_and_rotated_bathymetry_vector(a_B, b_B, theta_B, ds_origi_raw)\n",
    "    # Calculate small scale alignment, topostrophy, orthostrophy, and dot-poduct between polarized and grad_H (and magnitude) : \n",
    "    dot_product2_T, magnitude_polarized_T, magnitude_bathymetry_T, alignment_T, topostrophy1_T, topostrophy_T, orthostrophy_T = calculate_alignment_topostrophy_orthostrophy_of_variance(total_polarized_component, grad_H_rot_90)\n",
    "    dot_product2_L, magnitude_polarized_L, magnitude_bathymetry_L, alignment_L, topostrophy1_L, topostrophy_L, orthostrophy_L = calculate_alignment_topostrophy_orthostrophy_of_variance(local_polarized_component, grad_H_rot_90)\n",
    "    dot_product2_B, magnitude_polarized_B, magnitude_bathymetry_B, alignment_B, topostrophy1_B, topostrophy_B, orthostrophy_B = calculate_alignment_topostrophy_orthostrophy_of_variance(bulk_polarized_component,  grad_H_rot_90)\n",
    "    ''' MAKE DATAFRAME : '''\n",
    "    # Fix the Longitude : \n",
    "    #new_lon = np.where(lon > 180, lon - 360, lon)\n",
    "    # Num_lines, Num_pixels :\n",
    "    num_lines  = lat_c.shape[0]\n",
    "    num_pixels = lat_c.shape[1]\n",
    "    ''' CREATE DataArrays '''\n",
    "    cuu_T       = xr.DataArray(total_covariance_cuu,         dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    cvv_T       = xr.DataArray(total_covariance_cvv,         dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    cuv_T       = xr.DataArray(total_covariance_cuv,         dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    cuu_L       = xr.DataArray(local_covariance_cuu,         dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    cvv_L       = xr.DataArray(local_covariance_cvv,         dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    cuv_L       = xr.DataArray(local_covariance_cuv,         dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    cuu_B       = xr.DataArray(bulk_covariance_cuu,          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    cvv_B       = xr.DataArray(bulk_covariance_cvv,          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    cuv_B       = xr.DataArray(bulk_covariance_cuv,          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    trac_T      = xr.DataArray(trc_T,                        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    trac_L      = xr.DataArray(trc_L,                        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    trac_B      = xr.DataArray(trc_B,                        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    tot_a       = xr.DataArray(a_T,                          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    loc_a       = xr.DataArray(a_L,                          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    bul_a       = xr.DataArray(a_B,                          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    tot_b       = xr.DataArray(b_T,                          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    loc_b       = xr.DataArray(b_L,                          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    bul_b       = xr.DataArray(b_B,                          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    tot_theta   = xr.DataArray(theta_T,                      dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    loc_theta   = xr.DataArray(theta_L,                      dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    bul_theta   = xr.DataArray(theta_B,                      dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    tot_pol_x   = xr.DataArray(total_polarized_component[0], dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    tot_pol_y   = xr.DataArray(total_polarized_component[1], dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    loc_pol_x   = xr.DataArray(local_polarized_component[0], dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    loc_pol_y   = xr.DataArray(local_polarized_component[1], dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    bul_pol_x   = xr.DataArray(bulk_polarized_component[0],  dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    bul_pol_y   = xr.DataArray(bulk_polarized_component[1],  dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    grad_H_90_x = xr.DataArray(grad_H_rot_90[0],             dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    grad_H_90_y = xr.DataArray(grad_H_rot_90[1],             dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    dot_prod_T  = xr.DataArray(dot_product2_T,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    dot_prod_L  = xr.DataArray(dot_product2_L,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    dot_prod_B  = xr.DataArray(dot_product2_B,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    mag_pol_T   = xr.DataArray(magnitude_polarized_T,        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    mag_pol_L   = xr.DataArray(magnitude_polarized_L,        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    mag_pol_B   = xr.DataArray(magnitude_polarized_B,        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    mag_bathy   = xr.DataArray(magnitude_bathymetry_L,       dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ali_T       = xr.DataArray(alignment_T,                  dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ali_L       = xr.DataArray(alignment_L,                  dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ali_B       = xr.DataArray(alignment_B,                  dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    topos1_T    = xr.DataArray(topostrophy1_T,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    topos1_L    = xr.DataArray(topostrophy1_L,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    topos1_B    = xr.DataArray(topostrophy1_B,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    topos_T     = xr.DataArray(topostrophy_T,                dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    topos_L     = xr.DataArray(topostrophy_L,                dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    topos_B     = xr.DataArray(topostrophy_B,                dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ortho_T     = xr.DataArray(orthostrophy_T,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ortho_L     = xr.DataArray(orthostrophy_L,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ortho_B     = xr.DataArray(orthostrophy_B,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    lat_df      = xr.DataArray(lat_c,                        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    new_lon_df  = xr.DataArray(lon_c,                        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ''' CREATE DataSet '''\n",
    "    dataset  = xr.Dataset({ 'cuu_T'       : cuu_T,\n",
    "                            'cvv_T'       : cvv_T,\n",
    "                            'cuv_T'       : cuv_T,\n",
    "                            'cuu_L'       : cuu_L,\n",
    "                            'cvv_L'       : cvv_L,\n",
    "                            'cuv_L'       : cuv_L,\n",
    "                            'cuu_B'       : cuu_B,\n",
    "                            'cvv_B'       : cvv_B,\n",
    "                            'cuv_B'       : cuv_B,\n",
    "                            'trac_T'      : trac_T,\n",
    "                            'trac_L'      : trac_L,\n",
    "                            'trac_B'      : trac_B,\n",
    "                            'tot_a'       : tot_a,\n",
    "                            'loc_a'       : loc_a,\n",
    "                            'bul_a'       : bul_a,\n",
    "                            'tot_b'       : tot_b,\n",
    "                            'loc_b'       : loc_b,\n",
    "                            'bul_b'       : bul_b,\n",
    "                            'tot_theta'   : tot_theta,\n",
    "                            'loc_theta'   : loc_theta,\n",
    "                            'bul_theta'   : bul_theta,\n",
    "                            'tot_pol_x'   : tot_pol_x,\n",
    "                            'tot_pol_y'   : tot_pol_y,\n",
    "                            'loc_pol_x'   : loc_pol_x,\n",
    "                            'loc_pol_y'   : loc_pol_y,\n",
    "                            'bul_pol_x'   : bul_pol_x,\n",
    "                            'bul_pol_y'   : bul_pol_y,\n",
    "                            'grad_H_90_x' : grad_H_90_x,\n",
    "                            'grad_H_90_y' : grad_H_90_y,\n",
    "                            'dot_prod_T'  : dot_prod_T,\n",
    "                            'dot_prod_L'  : dot_prod_L,\n",
    "                            'dot_prod_B'  : dot_prod_B,\n",
    "                            'mag_pol_T'   : mag_pol_T,\n",
    "                            'mag_pol_L'   : mag_pol_L,\n",
    "                            'mag_pol_B'   : mag_pol_B,\n",
    "                            'mag_bathy'   : mag_bathy,\n",
    "                            'ali_T'       : ali_T,\n",
    "                            'ali_L'       : ali_L,\n",
    "                            'ali_B'       : ali_B,\n",
    "                            'topos_T'     : topos1_T,\n",
    "                            'topos_L'     : topos1_L,\n",
    "                            'topos_B'     : topos1_B,\n",
    "                            'ortho_T'     : ortho_T,\n",
    "                            'ortho_L'     : ortho_L,\n",
    "                            'ortho_B'     : ortho_B,\n",
    "                            'lat_df'      : lat_df,\n",
    "                            'new_lon_df'  : new_lon_df})\n",
    "    ''' RETURN THE DataSet '''\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE_NETCDF_2 = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/A_SSHA_CORR_NETCDF/'\n",
    "SAVE_NETCDF_2 = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/A_SSHA_CORR_NETCDF_2/'\n",
    "path_for_pass = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SWOT_1_RAW_MERGED/version_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_NETCDF_TOT_LOC_BUL_SOFIA = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/AA_Total_Local_bulk_new/SOFiA_NETCDF/smoothed_0'\n",
    "save_NETCDF_TOT_LOC_BUL_SOFIA = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/AA_Total_Local_bulk_new_2/SOFiA_NETCDF'\n",
    "for pass_number in list_of_passes:\n",
    "    print(pass_number)\n",
    "    save_dataset_of_calc_NETCDF = save_NETCDF_TOT_LOC_BUL_SOFIA + '/' + 'SOFiA_temp_spat_' + pass_number + '.nc'\n",
    "    dataset = create_dataframe_for_each_pass_tot_loc_bulk_a_b_ali_topo_ortho(SAVE_NETCDF_2, path_for_pass, pass_number, df_temp_spat_avg_nonan)\n",
    "    dataset.to_netcdf(save_dataset_of_calc_NETCDF)\n",
    "    print(save_dataset_of_calc_NETCDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = xr.open_dataset(save_NETCDF_TOT_LOC_BUL_SOFIA + '/' + 'SOFiA_temp_spat_' + 'pass_003' + '.nc')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_SOFiA_values_into_3_dataset(dataset):\n",
    "    # GET VALUES : \n",
    "    cuu_T       = dataset['cuu_T']\n",
    "    cvv_T       = dataset['cvv_T']\n",
    "    cuv_T       = dataset['cuv_T']\n",
    "    cuu_L       = dataset['cuu_L']\n",
    "    cvv_L       = dataset['cvv_L']\n",
    "    cuv_L       = dataset['cuv_L']\n",
    "    cuu_B       = dataset['cuu_B']\n",
    "    cvv_B       = dataset['cvv_B']\n",
    "    cuv_B       = dataset['cuv_B']\n",
    "    trac_T      = dataset['trac_T']\n",
    "    trac_L      = dataset['trac_L']\n",
    "    trac_B      = dataset['trac_B']\n",
    "    tot_a       = dataset['tot_a']\n",
    "    loc_a       = dataset['loc_a']\n",
    "    bul_a       = dataset['bul_a']\n",
    "    tot_b       = dataset['tot_b']\n",
    "    loc_b       = dataset['loc_b']\n",
    "    bul_b       = dataset['bul_b']\n",
    "    tot_theta   = dataset['tot_theta']\n",
    "    loc_theta   = dataset['loc_theta']\n",
    "    bul_theta   = dataset['bul_theta']\n",
    "    tot_pol_x   = dataset['tot_pol_x']\n",
    "    tot_pol_y   = dataset['tot_pol_y']\n",
    "    loc_pol_x   = dataset['loc_pol_x']\n",
    "    loc_pol_y   = dataset['loc_pol_y']\n",
    "    bul_pol_x   = dataset['bul_pol_x']\n",
    "    bul_pol_y   = dataset['bul_pol_y']\n",
    "    grad_H_90_x = dataset['grad_H_90_x']\n",
    "    grad_H_90_y = dataset['grad_H_90_y']\n",
    "    dot_prod_T  = dataset['dot_prod_T']\n",
    "    dot_prod_L  = dataset['dot_prod_L']\n",
    "    dot_prod_B  = dataset['dot_prod_B']\n",
    "    mag_pol_T   = dataset['mag_pol_T']\n",
    "    mag_pol_L   = dataset['mag_pol_L']\n",
    "    mag_pol_B   = dataset['mag_pol_B']\n",
    "    mag_bathy   = dataset['mag_bathy']\n",
    "    ali_T       = dataset['ali_T']\n",
    "    ali_L       = dataset['ali_L']\n",
    "    ali_B       = dataset['ali_B']\n",
    "    topos_T     = dataset['topos_T']\n",
    "    topos_L     = dataset['topos_L']\n",
    "    topos_B     = dataset['topos_B']\n",
    "    ortho_T     = dataset['ortho_T']\n",
    "    ortho_L     = dataset['ortho_L']\n",
    "    ortho_B     = dataset['ortho_B']\n",
    "    lat         = dataset['lat_df']\n",
    "    lon         = dataset['new_lon_df']\n",
    "    # UNRAVEL : \n",
    "    cuu_T       = np.ravel(cuu_T)\n",
    "    cvv_T       = np.ravel(cvv_T)\n",
    "    cuv_T       = np.ravel(cuv_T)\n",
    "    cuu_L       = np.ravel(cuu_L)\n",
    "    cvv_L       = np.ravel(cvv_L)\n",
    "    cuv_L       = np.ravel(cuv_L)\n",
    "    cuu_B       = np.ravel(cuu_B)\n",
    "    cvv_B       = np.ravel(cvv_B)\n",
    "    cuv_B       = np.ravel(cuv_B)\n",
    "    trac_T      = np.ravel(trac_T)\n",
    "    trac_L      = np.ravel(trac_L)\n",
    "    trac_B      = np.ravel(trac_B)\n",
    "    tot_a       = np.ravel(tot_a)\n",
    "    loc_a       = np.ravel(loc_a)\n",
    "    bul_a       = np.ravel(bul_a)\n",
    "    tot_b       = np.ravel(tot_b)\n",
    "    loc_b       = np.ravel(loc_b)\n",
    "    bul_b       = np.ravel(bul_b)\n",
    "    tot_theta   = np.ravel(tot_theta)\n",
    "    loc_theta   = np.ravel(loc_theta)\n",
    "    bul_theta   = np.ravel(bul_theta)\n",
    "    tot_pol_x   = np.ravel(tot_pol_x)\n",
    "    tot_pol_y   = np.ravel(tot_pol_y)\n",
    "    loc_pol_x   = np.ravel(loc_pol_x)\n",
    "    loc_pol_y   = np.ravel(loc_pol_y)\n",
    "    bul_pol_x   = np.ravel(bul_pol_x)\n",
    "    bul_pol_y   = np.ravel(bul_pol_y)\n",
    "    grad_H_90_x = np.ravel(grad_H_90_x)\n",
    "    grad_H_90_y = np.ravel(grad_H_90_y)\n",
    "    dot_prod_T  = np.ravel(dot_prod_T)\n",
    "    dot_prod_L  = np.ravel(dot_prod_L)\n",
    "    dot_prod_B  = np.ravel(dot_prod_B)\n",
    "    mag_pol_T   = np.ravel(mag_pol_T)\n",
    "    mag_pol_L   = np.ravel(mag_pol_L)\n",
    "    mag_pol_B   = np.ravel(mag_pol_B)\n",
    "    mag_bathy   = np.ravel(mag_bathy)\n",
    "    ali_T       = np.ravel(ali_T)\n",
    "    ali_L       = np.ravel(ali_L)\n",
    "    ali_B       = np.ravel(ali_B)\n",
    "    topos_T     = np.ravel(topos_T)\n",
    "    topos_L     = np.ravel(topos_L)\n",
    "    topos_B     = np.ravel(topos_B)\n",
    "    ortho_T     = np.ravel(ortho_T)\n",
    "    ortho_L     = np.ravel(ortho_L)\n",
    "    ortho_B     = np.ravel(ortho_B)\n",
    "    lat         = np.ravel(lat)\n",
    "    lon         = np.ravel(lon)\n",
    "    # CREATE DATAFRAME :\n",
    "    dataset_tot = pd.DataFrame({'cuu_T'       : cuu_T,\n",
    "                                'cvv_T'       : cvv_T,\n",
    "                                'cuv_T'       : cuv_T,\n",
    "                                'trac_T'      : trac_T,\n",
    "                                'tot_a'       : tot_a,\n",
    "                                'tot_b'       : tot_b,\n",
    "                                'tot_theta'   : tot_theta,\n",
    "                                'tot_pol_x'   : tot_pol_x,\n",
    "                                'tot_pol_y'   : tot_pol_y,\n",
    "                                'grad_H_90_x' : grad_H_90_x,\n",
    "                                'grad_H_90_y' : grad_H_90_y,\n",
    "                                'dot_prod_T'  : dot_prod_T,\n",
    "                                'mag_pol_T'   : mag_pol_T,\n",
    "                                'mag_bathy'   : mag_bathy,\n",
    "                                'ali_T'       : ali_T,\n",
    "                                'topos_T'     : topos_T,\n",
    "                                'ortho_T'     : ortho_T,\n",
    "                                'latitude'    : lat,\n",
    "                                'longitude'   : lon})\n",
    "    dataset_loc = pd.DataFrame({'cuu_L'       : cuu_L,\n",
    "                                'cvv_L'       : cvv_L,\n",
    "                                'cuv_L'       : cuv_L,\n",
    "                                'trac_L'      : trac_L,\n",
    "                                'loc_a'       : loc_a,\n",
    "                                'loc_b'       : loc_b,\n",
    "                                'loc_theta'   : loc_theta,\n",
    "                                'loc_pol_x'   : loc_pol_x,\n",
    "                                'loc_pol_y'   : loc_pol_y,\n",
    "                                'grad_H_90_x' : grad_H_90_x,\n",
    "                                'grad_H_90_y' : grad_H_90_y,\n",
    "                                'dot_prod_L'  : dot_prod_L,\n",
    "                                'mag_pol_L'   : mag_pol_L,\n",
    "                                'mag_bathy'   : mag_bathy,\n",
    "                                'ali_L'       : ali_L,\n",
    "                                'topos_L'     : topos_L,\n",
    "                                'ortho_L'     : ortho_L,\n",
    "                                'latitude'    : lat,\n",
    "                                'longitude'   : lon})\n",
    "    dataset_bul = pd.DataFrame({'cuu_B'       : cuu_B,\n",
    "                                'cvv_B'       : cvv_B,\n",
    "                                'cuv_B'       : cuv_B,\n",
    "                                'trac_B'      : trac_B,\n",
    "                                'bul_a'       : bul_a,\n",
    "                                'bul_b'       : bul_b,\n",
    "                                'bul_theta'   : bul_theta,\n",
    "                                'bul_pol_x'   : bul_pol_x,\n",
    "                                'bul_pol_y'   : bul_pol_y,\n",
    "                                'grad_H_90_x' : grad_H_90_x,\n",
    "                                'grad_H_90_y' : grad_H_90_y,\n",
    "                                'dot_prod_B'  : dot_prod_B,\n",
    "                                'mag_pol_B'   : mag_pol_B,\n",
    "                                'mag_bathy'   : mag_bathy,\n",
    "                                'ali_B'       : ali_B,\n",
    "                                'topos_B'     : topos_B,\n",
    "                                'ortho_B'     : ortho_B,\n",
    "                                'latitude'    : lat,\n",
    "                                'longitude'   : lon})\n",
    "    # Return dataset :\n",
    "    return dataset_tot, dataset_loc, dataset_bul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_NETCDF_TOT_LOC_BUL_SOFIA = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/AA_Total_Local_bulk_new/SOFiA_NETCDF/smoothed_0'\n",
    "#save_CSV_TOT_LOC_BUL_SOFIA    = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/AA_Total_Local_bulk_new/SOFIA_CSV/smoothed_0'\n",
    "\n",
    "save_NETCDF_TOT_LOC_BUL_SOFIA = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/AA_Total_Local_bulk_new_2/SOFiA_NETCDF'\n",
    "save_CSV_TOT_LOC_BUL_SOFIA    = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/AA_Total_Local_bulk_new_2/SOFIA_CSV'\n",
    "\n",
    "for pass_number in list_of_passes:\n",
    "    print(pass_number)\n",
    "    dataset = xr.open_dataset(save_NETCDF_TOT_LOC_BUL_SOFIA + '/' + 'SOFiA_temp_spat_' + pass_number + '.nc')\n",
    "    dataset_tot, dataset_loc, dataset_bul = unravel_SOFiA_values_into_3_dataset(dataset)\n",
    "    # Save the dataframe to a csv file\n",
    "    save_csv_name_T = save_CSV_TOT_LOC_BUL_SOFIA + '/' + 'SOFiA_tota_' + pass_number + '.csv'\n",
    "    save_csv_name_L = save_CSV_TOT_LOC_BUL_SOFIA + '/' + 'SOFiA_loca_' + pass_number + '.csv'\n",
    "    save_csv_name_B = save_CSV_TOT_LOC_BUL_SOFIA + '/' + 'SOFiA_bulk_' + pass_number + '.csv'\n",
    "    # SAVE :\n",
    "    dataset_tot.to_csv(save_csv_name_T, index=False)\n",
    "    dataset_loc.to_csv(save_csv_name_L, index=False)\n",
    "    dataset_bul.to_csv(save_csv_name_B, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Calculate and save in files for each track eq. 14 vs eq. 1 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for raw files : \n",
    "path_n = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SWOT_1_RAW_MERGED/version_2'\n",
    "# Path to save calculation : \n",
    "original_path_for_files = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SOFiA/SOFiA_NETCDF/'\n",
    "original_path_for_files_csv = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SOFiA/SOFiA_CSV/'\n",
    "save_dataset_of_calc_folder = var_type+'_'+correction_type+'_'+mask_type+'_smoothing'+sm_name+'_delta_lat_lon_'+str(delta_lat_lon)+'_extent_of_area_'+str(lon1)+'_'+str(lon2)+'_'+str(lat1)+'_'+str(lat2)+'_'+passes_name#+'_'+seas_name\n",
    "\n",
    "#save_csv_name = original_path_for_files_csv + save_dataset_of_calc_folder + '/' + 'SOFiA_' + '*.csv'\n",
    "\n",
    "save_csv_name_T = original_path_for_files_csv + save_dataset_of_calc_folder + '/' + 'SOFiA_tota_' + '*.csv'\n",
    "save_csv_name_L = original_path_for_files_csv + save_dataset_of_calc_folder + '/' + 'SOFiA_loca_' + '*.csv'\n",
    "save_csv_name_B = original_path_for_files_csv + save_dataset_of_calc_folder + '/' + 'SOFiA_bulk_' + '*.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_L = pd.concat([pd.read_csv(f) for f in glob.glob(save_csv_name_L)], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuu_L_inval       = ~np.isnan(dataset_L['cuu_L'])\n",
    "cvv_L_inval       = ~np.isnan(dataset_L['cvv_L'])\n",
    "cuv_L_inval       = ~np.isnan(dataset_L['cuv_L'])\n",
    "trac_L_inval      = ~np.isnan(dataset_L['trac_L'])\n",
    "loc_a_inval       = ~np.isnan(dataset_L['loc_a'])\n",
    "loc_b_inval       = ~np.isnan(dataset_L['loc_b'])\n",
    "loc_theta_inval   = ~np.isnan(dataset_L['loc_theta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUU : \n",
    "cuu_L_binned, lonbins_L, latbins_L      = plt_s2.make_binned_statistics_from_unraveled_df_inval_winter_summer(dataset_L, 'cuu_L',       'latitude', 'longitude', cuu_L_inval,       lon1, lon2, lat1, lat2, delta_lat_lon=delta_lat_lon, statistic='mean')\n",
    "# CVV :\n",
    "cvv_L_binned, lonbins_L, latbins_L      = plt_s2.make_binned_statistics_from_unraveled_df_inval_winter_summer(dataset_L, 'cvv_L',       'latitude', 'longitude', cvv_L_inval,       lon1, lon2, lat1, lat2, delta_lat_lon=delta_lat_lon, statistic='mean')\n",
    "# CUV :\n",
    "cuv_L_binned, lonbins_L, latbins_L      = plt_s2.make_binned_statistics_from_unraveled_df_inval_winter_summer(dataset_L, 'cuv_L',       'latitude', 'longitude', cuv_L_inval,       lon1, lon2, lat1, lat2, delta_lat_lon=delta_lat_lon, statistic='mean')\n",
    "# Trace : \n",
    "trac_L_binned, lonbins_L, latbins_L     = plt_s2.make_binned_statistics_from_unraveled_df_inval_winter_summer(dataset_L, 'trac_L',      'latitude', 'longitude', trac_L_inval,      lon1, lon2, lat1, lat2, delta_lat_lon=delta_lat_lon, statistic='mean')\n",
    "# a :\n",
    "loc_a_binned, lonbins_L, latbins_L      = plt_s2.make_binned_statistics_from_unraveled_df_inval_winter_summer(dataset_L, 'loc_a',       'latitude', 'longitude', loc_a_inval,       lon1, lon2, lat1, lat2, delta_lat_lon=delta_lat_lon, statistic='mean')\n",
    "# b : \n",
    "loc_b_binned, lonbins_L, latbins_L      = plt_s2.make_binned_statistics_from_unraveled_df_inval_winter_summer(dataset_L, 'loc_b',       'latitude', 'longitude', loc_b_inval,       lon1, lon2, lat1, lat2, delta_lat_lon=delta_lat_lon, statistic='mean')\n",
    "# Theta : \n",
    "loc_theta_binned, lonbins_L, latbins_L  = plt_s2.make_binned_statistics_from_unraveled_df_inval_winter_summer(dataset_L, 'loc_theta',   'latitude', 'longitude', loc_theta_inval,   lon1, lon2, lat1, lat2, delta_lat_lon=delta_lat_lon, statistic='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and check : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinant of covariance matrix : \n",
    "detc_binned_man     = np.real(cuu_L_binned.statistic*cvv_L_binned.statistic-cuv_L_binned.statistic**2)\n",
    "# Trace of covariance matrix : \n",
    "trc_binned_man      = cuu_L_binned.statistic + cvv_L_binned.statistic \n",
    "# Semi-major axis : \n",
    "a_binned_man        = np.sqrt(trc_binned_man/2+np.sqrt(trc_binned_man**2-4*detc_binned_man)/2)\n",
    "# Semi-minor axis : \n",
    "b_binned_man        = np.sqrt(trc_binned_man/2-np.sqrt(trc_binned_man**2-4*detc_binned_man)/2)\n",
    "# Oientation angle :\n",
    "theta_binned_man    = np.arctan2(2*cuv_L_binned.statistic,cuu_L_binned.statistic-cvv_L_binned.statistic)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_s2.plot_binned_data(np.sqrt(trac_L_binned.statistic), lonbins_L, latbins_L, var_type, title='Local trace, before binning : ', d_lat_lon=delta_lat_lon, unit_t='', cmap='Spectral_r', vmin=None, vmax=1.5, cbar_aspect=40, figextent=[-80, 30, 43, 85], bath_color='black', projection=ccrs.NorthPolarStereo(central_longitude=-25), figsize=(20,15), bath2=None, bat_min=-2000, bat_max=-20, bat_lvl=4, grid_p=False, pre_def_lvls=False, bath_line_stye='solid', bath_line_width=1, bath_or_not=False)\n",
    "plt_s2.plot_binned_data(np.sqrt(trc_binned_man), lonbins_L, latbins_L, var_type, title='Local trace, after binning : ', d_lat_lon=delta_lat_lon, unit_t='', cmap='Spectral_r', vmin=None, vmax=1.5, cbar_aspect=40, figextent=[-80, 30, 43, 85], bath_color='black', projection=ccrs.NorthPolarStereo(central_longitude=-25), figsize=(20,15), bath2=None, bat_min=-2000, bat_max=-20, bat_lvl=4, grid_p=False, pre_def_lvls=False, bath_line_stye='solid', bath_line_width=1, bath_or_not=False)\n",
    "plt_s2.plot_binned_data(np.sqrt(trc_binned_man)-np.sqrt(trac_L_binned.statistic), lonbins_L, latbins_L, var_type, title='Local trace, after binning : ', d_lat_lon=delta_lat_lon, unit_t='', cmap=cmocean.cm.balance, vmin=-0.2e-15, vmax=0.2e-15, cbar_aspect=40, figextent=[-80, 30, 43, 85], bath_color='black', projection=ccrs.NorthPolarStereo(central_longitude=-25), figsize=(20,15), bath2=None, bat_min=-2000, bat_max=-20, bat_lvl=4, grid_p=False, pre_def_lvls=False, bath_line_stye='solid', bath_line_width=1, bath_or_not=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eccentricity(a_all, b_all):\n",
    "    Eccentricity = np.sqrt(1 - (b_all/a_all)**2)\n",
    "    return Eccentricity\n",
    "\n",
    "def eddy_anisotropy_jon_paper(a_all, b_all):\n",
    "    Eccentricity = (a_all**2 - b_all**2)/2\n",
    "    return Eccentricity\n",
    "\n",
    "def eddy_anisotropy_p(a_all, b_all):\n",
    "    Eccentricity = (a_all**2)/(a_all**2+b_all**2)\n",
    "    return Eccentricity\n",
    "\n",
    "def polarization_ratio(a_all, b_all):\n",
    "    P = (a_all**2 - b_all**2)/(a_all**2+b_all**2) #NDI - normalized difference index ? \n",
    "    return P\n",
    "\n",
    "#Eccentricity = eccentricity(a_binned_man.statistic, b_binned_man.statistic)\n",
    "#Eccentricity = np.array(Eccentricity)\n",
    "\n",
    "#Eddy_anisotropy_jon = eddy_anisotropy_jon_paper(a_binned_man.statistic, b_binned_man.statistic)\n",
    "#Eddy_anisotropy_jon = np.array(Eddy_anisotropy_jon)\n",
    "\n",
    "#Eddy_anisotropy_p = eddy_anisotropy_p(a_binned_man.statistic, b_binned_man.statistic)\n",
    "#Eddy_anisotropy_p = np.array(Eddy_anisotropy_p)\n",
    "\n",
    "Polarization_ratio = polarization_ratio(a_binned_man, b_binned_man)\n",
    "Polarization_ratio = np.array(Polarization_ratio)\n",
    "\n",
    "Polarization_ratio2 = polarization_ratio(loc_a_binned.statistic, loc_b_binned.statistic)\n",
    "Polarization_ratio2 = np.array(Polarization_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_s2.plot_binned_data(Polarization_ratio, lonbins_L, latbins_L, var_type, title='Local a, b, after binning : ', d_lat_lon=delta_lat_lon, unit_t='', cmap='Spectral_r', vmin=None, vmax=None, cbar_aspect=40, figextent=[-80, 30, 43, 85], bath_color='black', projection=ccrs.NorthPolarStereo(central_longitude=-25), figsize=(20,15), bath2=None, bat_min=-2000, bat_max=-20, bat_lvl=4, grid_p=False, pre_def_lvls=False, bath_line_stye='solid', bath_line_width=1, bath_or_not=False)\n",
    "plt_s2.plot_binned_data(Polarization_ratio2, lonbins_L, latbins_L, var_type, title='Local a, d, before binning : ', d_lat_lon=delta_lat_lon, unit_t='', cmap='Spectral_r', vmin=None, vmax=None, cbar_aspect=40, figextent=[-80, 30, 43, 85], bath_color='black', projection=ccrs.NorthPolarStereo(central_longitude=-25), figsize=(20,15), bath2=None, bat_min=-2000, bat_max=-20, bat_lvl=4, grid_p=False, pre_def_lvls=False, bath_line_stye='solid', bath_line_width=1, bath_or_not=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_binned_var_Earth_grid_to_satellite_pass_grid(binned_var, lat_bin, lon_bin, lat_sat_o, lon_sat_o):\n",
    "    ''' FUNCTION TO INTERPOLATE THE BINNED AVERAGED VARIABLES BACK TO SATELLITE-TRACK-GRID '''\n",
    "    mod_lat22 = lat_bin[:-1]\n",
    "    mod_lon22 = lon_bin[:-1]\n",
    "    lon_gridM, lat_gridM = np.meshgrid(mod_lon22, mod_lat22)\n",
    "    # GRID POINTS\n",
    "    lat_grid    = np.ravel(lat_gridM)\n",
    "    lon_grid    = np.ravel(lon_gridM)\n",
    "    grid_values = np.ravel(binned_var)\n",
    "    # SATELLITE-TRACK POINTS\n",
    "    lat_sat = np.ravel(lat_sat_o)\n",
    "    lon_sat = np.ravel(lon_sat_o)\n",
    "    # NB! Correct longitude, if not done before : \n",
    "    lon_sat = np.where(lon_sat>180.0,lon_sat-360,lon_sat)\n",
    "    # Make Earth-grid points :\n",
    "    original_points = np.column_stack((lon_grid, lat_grid))\n",
    "    original_values = grid_values\n",
    "    # Create a mask for valid satellite coordinates (where neither is NaN) :\n",
    "    valid_mask = ~np.isnan(lat_sat) & ~np.isnan(lon_sat)\n",
    "    valid_sat_coords = np.column_stack((lon_sat[valid_mask], lat_sat[valid_mask]))\n",
    "    # Perform griddata interpolation on valid satellite coordinates :\n",
    "    interpolated_values = griddata(original_points, original_values, valid_sat_coords, method='nearest')\n",
    "    # Initialize a full array with NaNs to represent the satellite's grid values :\n",
    "    satellite_values = np.full(lat_sat.shape, np.nan)\n",
    "    satellite_values[valid_mask] = interpolated_values\n",
    "    # Reshape back to original shape : \n",
    "    interpolated_values_original_shape = satellite_values.reshape((lat_sat_o.shape))\n",
    "    return interpolated_values_original_shape, interpolated_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bathymetry_from_SWOT_file_orig_grid_and_Earth_grid(ds):\n",
    "    # Get dept_or_elevation layer from the original SWOT dataset : \n",
    "    bath_swot = ds.depth_or_elevation # Should be the same for all cycles, so only need one\n",
    "    bath_swot = ds.isel(concat_dim=12).depth_or_elevation \n",
    "    bath_lat  = ds.isel(concat_dim=12).latitude\n",
    "    bath_lon  = ds.isel(concat_dim=12).longitude \n",
    "    # dx, dy 2 km : \n",
    "    dx = 2000\n",
    "    dy = 2000\n",
    "    # Get Grad_H\n",
    "    grad_H_num_lines  = np.gradient(bath_swot, axis=0)/dy # y-axis - local SWOT-grid\n",
    "    grad_H_num_pixels = np.gradient(bath_swot, axis=1)/dx # x-axis - local SWOT-grid\n",
    "    # Rotate to Earth-Grid : \n",
    "    grad_H_num_pixels_rotated, grad_H_num_lines_rotated, grad_H_num_pixels_rotated_by_phi =  SOFiA.rotate_dataset_original_grid_all_cycles(bath_lat, bath_lon, grad_H_num_pixels, grad_H_num_lines)\n",
    "    return grad_H_num_lines, grad_H_num_pixels, grad_H_num_pixels_rotated, grad_H_num_lines_rotated, grad_H_num_pixels_rotated_by_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_polarized_vector_and_rotated_bathymetry_vector(a, b, theta, ds_of_track_for_topogrphy, by_phi_or_not, rotated_to_eart_grid_or_not):\n",
    "    ''' 1 ) Get polarized part of variance : '''\n",
    "    # Polarized  part of velocity variance ellipse : \n",
    "    polarized_magnitude = np.sqrt(a**2 - b**2)\n",
    "    # Define the unit vector in the direction of the major axis\n",
    "    N = np.array([np.cos(theta), np.sin(theta)])\n",
    "    # Compute the polarized component\n",
    "    polarized_component = polarized_magnitude * N\n",
    "    ''' 2 ) Get barthymetry from the track : '''\n",
    "    grad_H_num_lines, grad_H_num_pixels, grad_H_num_pixels_rotated, grad_H_num_lines_rotated, grad_H_num_pixels_rotated_by_phi = get_bathymetry_from_SWOT_file_orig_grid_and_Earth_grid(ds_of_track_for_topogrphy)\n",
    "    # Dividen by phi or not:\n",
    "    if by_phi_or_not == 'yes':\n",
    "        grad_H_num_pixels_rotated = grad_H_num_pixels_rotated_by_phi\n",
    "    else: \n",
    "        grad_H_num_pixels_rotated = grad_H_num_pixels_rotated\n",
    "    # Gradient vector\n",
    "    if rotated_to_eart_grid_or_not == 'yes':\n",
    "        # Gradient vector\n",
    "        gradH = np.array([grad_H_num_pixels_rotated, grad_H_num_lines_rotated])\n",
    "        # Apply 90-degree rotation manually\n",
    "        grad_H_rot_90 = np.array([-grad_H_num_lines_rotated, grad_H_num_pixels_rotated])\n",
    "    elif rotated_to_eart_grid_or_not == 'no':\n",
    "        # Gradient vector\n",
    "        gradH = np.array([grad_H_num_pixels, grad_H_num_lines])\n",
    "        # Apply 90-degree rotation manually\n",
    "        grad_H_rot_90 = np.array([-grad_H_num_lines, grad_H_num_pixels])\n",
    "    ''' Another way to calculate : '''\n",
    "    # Rotation matrix\n",
    "    R_90 = np.array([[0, -1], [1, 0]])\n",
    "    # Rotate each vector in batch across provided dimensions [2,x,y]\n",
    "    # Reshape and permute to apply batch rotation per vector (2, dimensions)\n",
    "    gradH_rotated = np.einsum('ij,jkl->ikl', R_90, gradH)\n",
    "    # Ensure matching dimensions for computation\n",
    "    dot_product1 = np.einsum('ijm,ijm->jm', gradH_rotated, polarized_component)\n",
    "    return polarized_component, grad_H_rot_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_alignment_topostrophy_orthostrophy_of_variance(vector_a, topography_vector):\n",
    "    ''' FUNCTION TO CALCULATE THE ALIGNMENT, TOPOSTROPHY, AND ORTHOSTROPHY\n",
    "        This function do not normalize, can normalize at later scale, \n",
    "        when spatially averaged, or right away if looking at small scale. \n",
    "        Just a basic calculation ... '''\n",
    "    polarized_component = vector_a\n",
    "    grad_H_rot_90       = topography_vector\n",
    "    ''' 1 ) Calculate the dot-product between the two vectors : '''\n",
    "    # Compute dot product\n",
    "    #dot_product = np.dot(grad_H_rot_90, polarized_component)\n",
    "    dot_product2 = (grad_H_rot_90[0]*polarized_component[0] + grad_H_rot_90[1]*polarized_component[1])\n",
    "    magnitude_polarized  = (np.sqrt(polarized_component[0]**2+polarized_component[1]**2))\n",
    "    magnitude_bathymetry = (np.sqrt(grad_H_rot_90[0]**2+grad_H_rot_90[1]**2))\n",
    "    # Iterate over batches and compute dot product\n",
    "    #dot_product3 = np.sum(grad_H_rot_90 * polarized_component, axis=0)  # Axis to align sum if needed with dimensions\n",
    "    ''' ALIGNMENT :  '''\n",
    "    alignment   = np.abs(dot_product2)/(np.sqrt(polarized_component[0]**2+polarized_component[1]**2)*np.sqrt(grad_H_rot_90[0]**2+grad_H_rot_90[1]**2))\n",
    "    theta_cos = np.arccos(alignment)\n",
    "    #theta_cos_smooth = SOFiA.smooth_multiple_times(theta_cos,1)\n",
    "    ''' PROJECT A ONTO B : '''\n",
    "    flow_norm = polarized_component #/ (np.sqrt(polarized_component[0]**2+polarized_component[1]**2))\n",
    "    dot_product_a_b = (grad_H_rot_90[0]*flow_norm[0] + grad_H_rot_90[1]*flow_norm[1])\n",
    "    # Calculate squared norm safely by summing along the first axis (axis=0)\n",
    "    norm_squared = np.sum(grad_H_rot_90**2, axis=0)  # Shape should be (2062, 69)\n",
    "    # Element-wise condition to avoid division by zero\n",
    "    is_nonzero = norm_squared != 0  # Shape: (2062, 69)\n",
    "    # Modify norm_squared shape for broadcasting to ensure compatibility across operations\n",
    "    norm_squared = norm_squared[np.newaxis, :, :]  # Shape: (1, 2062, 69)\n",
    "    # Safe projection calculation using broadcasting\n",
    "    Proj_a_onto_b = np.where(is_nonzero[np.newaxis, :, :], (dot_product_a_b / norm_squared) * grad_H_rot_90, np.zeros_like(grad_H_rot_90))\n",
    "    #Proj_a_onto_b = np.abs(dot_product_a_b)/(grad_H_rot_90[0]*grad_H_rot_90[0] + grad_H_rot_90[1]*grad_H_rot_90[1])*grad_H_rot_90\n",
    "    non_topostrophy_a = flow_norm - Proj_a_onto_b\n",
    "    ''' ORTHOSTROPHY : '''\n",
    "    orthostrophy = np.sqrt(non_topostrophy_a[0]**2 + non_topostrophy_a[1]**2)\n",
    "    ''' TOPOSTROPHY : '''\n",
    "    topostrophy1 = np.abs(dot_product2)/(np.sqrt(grad_H_rot_90[0]**2+grad_H_rot_90[1]**2))\n",
    "    topostrophy  = np.sqrt(Proj_a_onto_b[0]**2 + Proj_a_onto_b[1]**2)\n",
    "    return dot_product2, magnitude_polarized, magnitude_bathymetry, alignment, topostrophy1, topostrophy, orthostrophy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing one one pass : pass_003 : Two ways : Interpolate back to satellite then calculate polarization-ratio, or first calculate polarization-ratio, then interpolate bakc to satellite track. --> Do the first now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for raw files : \n",
    "path_n = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SWOT_1_RAW_MERGED/version_2'\n",
    "ds = xr.open_dataset(path_n + '/' + 'pass_003' + '.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_d, lat, lon, month_list, DJF, MAM, JJA, SON, Winter, Summer = SOFiA.get_corrected_and_masked_variables(ds, var_type, correction_type, mask_type, smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuu_back_to_track, tmp = interpolate_binned_var_Earth_grid_to_satellite_pass_grid(cuu_L_binned.statistic, latbins_L, lonbins_L, lat_sat_o=lat, lon_sat_o=lon)\n",
    "cvv_back_to_track, tmp = interpolate_binned_var_Earth_grid_to_satellite_pass_grid(cvv_L_binned.statistic, latbins_L, lonbins_L, lat_sat_o=lat, lon_sat_o=lon)\n",
    "cuv_back_to_track, tmp = interpolate_binned_var_Earth_grid_to_satellite_pass_grid(cuv_L_binned.statistic, latbins_L, lonbins_L, lat_sat_o=lat, lon_sat_o=lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_lat22 = latbins_L[:-1]\n",
    "mod_lon22 = lonbins_L[:-1]\n",
    "lon_grid, lat_grid = np.meshgrid(mod_lon22, mod_lat22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_sat = np.where(lon>180.0,lon-360,lon)\n",
    "lat_sat = lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(lon_grid, lat_grid, c=np.ravel(cuu_L_binned.statistic), s=1, cmap=cmocean.cm.matter, vmin=0, vmax=0.2, marker='s', label='Grid Points')\n",
    "plt.scatter(lon_sat, lat_sat, c=cuu_back_to_track, s=1, cmap=cmocean.cm.matter_r, vmin=0, vmax=0.2, label='Track Points', alpha=0.7)\n",
    "plt.colorbar(label='Value')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "#plt.xlim(14,25)\n",
    "#plt.ylim(74,75)\n",
    "plt.xlim(-30,30)\n",
    "plt.ylim(44,79)\n",
    "plt.title('Interpolation of Grid Values onto Satellite Track using Nearest Neighbor')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(lon_grid, lat_grid, c=np.ravel(cuu_L_binned.statistic), s=50, cmap=cmocean.cm.speed, vmin=0, vmax=0.2, marker='s', label='Grid Points')\n",
    "plt.scatter(lon_sat, lat_sat, c=cuu_back_to_track, s=5, cmap=cmocean.cm.speed, vmin=0, vmax=0.2, marker='o', label='Track Points', alpha=0.99)\n",
    "plt.colorbar(label='Value')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.xlim(-10,-2)\n",
    "plt.ylim(61,63)\n",
    "plt.title('Interpolation of Grid Values onto Satellite Track using Nearest Neighbor')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinant of covariance matrix : \n",
    "detc_back_to_track     = np.real(cuu_back_to_track*cvv_back_to_track-cuv_back_to_track**2)\n",
    "# Trace of covariance matrix : \n",
    "trc_back_to_track      = cuu_back_to_track + cvv_back_to_track\n",
    "# Semi-major axis : \n",
    "a_back_to_track        = np.sqrt(trc_back_to_track/2+np.sqrt(trc_back_to_track**2-4*detc_back_to_track)/2)\n",
    "# Semi-minor axis : \n",
    "b_back_to_track        = np.sqrt(trc_back_to_track/2-np.sqrt(trc_back_to_track**2-4*detc_back_to_track)/2)\n",
    "# Oientation angle :\n",
    "theta_back_to_track    = np.arctan2(2*cuv_back_to_track,cuu_back_to_track-cvv_back_to_track)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarized_component, grad_H_rot_90 = calculate_polarized_vector_and_rotated_bathymetry_vector(a_back_to_track, b_back_to_track, theta_back_to_track, ds, None, 'yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product2, magnitude_polarized, magnitude_bathymetry, alignment, topostrophy1, topostrophy, orthostrophy = calculate_alignment_topostrophy_orthostrophy_of_variance(polarized_component, grad_H_rot_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "plt.imshow(alignment, cmap=cmocean.cm.matter, vmin=None, vmax=None, alpha=1, zorder=20)\n",
    "plt.colorbar()\n",
    "plt.contour(ds.isel(concat_dim=20)['depth_or_elevation'], levels=30, zorder=30, vmin=-2000, vmax=0, linewidths=0.5)\n",
    "plt.ylim(500,1500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(alignment, cmap=cmocean.cm.matter, vmin=None, vmax=None, alpha=.7, zorder=20)\n",
    "plt.colorbar()\n",
    "plt.contour(ds.isel(concat_dim=20)['depth_or_elevation'], levels=30, zorder=10, vmin=-2000, vmax=0, linewidth=0.5)\n",
    "plt.ylim(950,1050)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(alignment, cmap=cmocean.cm.matter, vmin=None, vmax=None, alpha=.7, zorder=20)\n",
    "plt.colorbar()\n",
    "plt.contour(ds.isel(concat_dim=20)['depth_or_elevation'], levels=30, zorder=10, vmin=-2000, vmax=0, linewidth=0.5)\n",
    "plt.ylim(650,750)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(dot_product2, vmin=0, vmax=0.01)\n",
    "plt.ylim(650,750)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make files of params in equation 8, from 14 instead of 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe_for_each_pass_loc_eq_14(path_for_pass, pass_number, var_type, correction_type, mask_type, smoothing, season_type, cuu_L_binned, cvv_L_binned, cuv_L_binned, latbins_L, lonbins_L, ug_by_phi=None, by_phi_or_not='no', rotated_to_eart_grid_or_not='yes'):\n",
    "    # Open passes : \n",
    "    #ds = xr.open_dataset(path_for_pass+pass_number)\n",
    "    ds = xr.open_dataset(path_for_pass + '/' + pass_number + '.nc')\n",
    "    # GET CORRECTED VAIABLE DATA : \n",
    "    #masked_d, lat, lon, month_list, DJF, MAM, JJA, SON, Winter, Summer = SOFiA.get_corrected_and_masked_variables(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables_C0_C2_all_own_ssha(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    # Use binned Local covariance ellipse and interpolat to satellite track again : \n",
    "    cuu_back_to_track, tmp = interpolate_binned_var_Earth_grid_to_satellite_pass_grid(cuu_L_binned.statistic, latbins_L, lonbins_L, lat_sat_o=lat, lon_sat_o=lon)\n",
    "    cvv_back_to_track, tmp = interpolate_binned_var_Earth_grid_to_satellite_pass_grid(cvv_L_binned.statistic, latbins_L, lonbins_L, lat_sat_o=lat, lon_sat_o=lon)\n",
    "    cuv_back_to_track, tmp = interpolate_binned_var_Earth_grid_to_satellite_pass_grid(cuv_L_binned.statistic, latbins_L, lonbins_L, lat_sat_o=lat, lon_sat_o=lon)\n",
    "    # Get a, b, theta from the binned covaiance ellipse : \n",
    "    # Determinant of covariance matrix : \n",
    "    detc_back_to_track     = np.real(cuu_back_to_track*cvv_back_to_track-cuv_back_to_track**2)\n",
    "    # Trace of covariance matrix : \n",
    "    trc_back_to_track      = cuu_back_to_track + cvv_back_to_track\n",
    "    # Semi-major axis : \n",
    "    a_back_to_track        = np.sqrt(trc_back_to_track/2+np.sqrt(trc_back_to_track**2-4*detc_back_to_track)/2)\n",
    "    # Semi-minor axis : \n",
    "    b_back_to_track        = np.sqrt(trc_back_to_track/2-np.sqrt(trc_back_to_track**2-4*detc_back_to_track)/2)\n",
    "    # Oientation angle :\n",
    "    theta_back_to_track    = np.arctan2(2*cuv_back_to_track,cuu_back_to_track-cvv_back_to_track)/2\n",
    "    # Get polarized and bath vectors : \n",
    "    polarized_component, grad_H_rot_90 = calculate_polarized_vector_and_rotated_bathymetry_vector(a_back_to_track, b_back_to_track, theta_back_to_track, ds, None, 'yes')\n",
    "    # Calculate alignment, dotprod, etc. : \n",
    "    dot_product2, magnitude_polarized, magnitude_bathymetry, alignment, topostrophy1, topostrophy, orthostrophy = calculate_alignment_topostrophy_orthostrophy_of_variance(polarized_component, grad_H_rot_90)\n",
    "    ''' MAKE DATAFRAME : '''\n",
    "    # Fix the Longitude : \n",
    "    new_lon = np.where(lon > 180, lon - 360, lon)\n",
    "    # Num_lines, Num_pixels :\n",
    "    num_lines  = lat.shape[0]\n",
    "    num_pixels = lat.shape[1]\n",
    "    ''' CREATE DataArrays '''\n",
    "    loc_a       = xr.DataArray(a_back_to_track,        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    loc_b       = xr.DataArray(b_back_to_track,        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    loc_theta   = xr.DataArray(theta_back_to_track,    dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    trac_L      = xr.DataArray(trc_back_to_track,      dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    loc_pol_x   = xr.DataArray(polarized_component[0], dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    loc_pol_y   = xr.DataArray(polarized_component[1], dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    grad_H_90_x = xr.DataArray(grad_H_rot_90[0],       dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    grad_H_90_y = xr.DataArray(grad_H_rot_90[1],       dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    dot_prod_L  = xr.DataArray(dot_product2,           dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    mag_pol_L   = xr.DataArray(magnitude_polarized,    dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    mag_bathy   = xr.DataArray(magnitude_bathymetry,   dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ali_L       = xr.DataArray(alignment,              dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    topos_L     = xr.DataArray(topostrophy1,           dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ortho_L     = xr.DataArray(orthostrophy,           dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    lat_df      = xr.DataArray(lat,                    dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    new_lon_df  = xr.DataArray(new_lon,                dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ''' CREATE DataSet '''\n",
    "    dataset  = xr.Dataset({ 'trac_L'      : trac_L,\n",
    "                            'loc_a'       : loc_a,\n",
    "                            'loc_b'       : loc_b,\n",
    "                            'loc_theta'   : loc_theta,\n",
    "                            'loc_pol_x'   : loc_pol_x,\n",
    "                            'loc_pol_y'   : loc_pol_y,\n",
    "                            'grad_H_90_x' : grad_H_90_x,\n",
    "                            'grad_H_90_y' : grad_H_90_y,\n",
    "                            'dot_prod_L'  : dot_prod_L,\n",
    "                            'mag_pol_L'   : mag_pol_L,\n",
    "                            'mag_bathy'   : mag_bathy,\n",
    "                            'ali_L'       : ali_L,\n",
    "                            'topos_L'     : topos_L,\n",
    "                            'ortho_L'     : ortho_L,\n",
    "                            'lat_df'      : lat_df,\n",
    "                            'new_lon_df'  : new_lon_df})\n",
    "    ''' RETURN THE DataSet '''\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_SOFiA_values_into_dataset_eq14(dataset):\n",
    "    # GET VALUES : \n",
    "    trac_L      = dataset['trac_L']\n",
    "    loc_a       = dataset['loc_a']\n",
    "    loc_b       = dataset['loc_b']\n",
    "    loc_theta   = dataset['loc_theta']\n",
    "    loc_pol_x   = dataset['loc_pol_x']\n",
    "    loc_pol_y   = dataset['loc_pol_y']\n",
    "    grad_H_90_x = dataset['grad_H_90_x']\n",
    "    grad_H_90_y = dataset['grad_H_90_y']\n",
    "    dot_prod_L  = dataset['dot_prod_L']\n",
    "    mag_pol_L   = dataset['mag_pol_L']\n",
    "    mag_bathy   = dataset['mag_bathy']\n",
    "    ali_L       = dataset['ali_L']\n",
    "    topos_L     = dataset['topos_L']\n",
    "    ortho_L     = dataset['ortho_L']\n",
    "    lat         = dataset['lat_df']\n",
    "    lon         = dataset['new_lon_df']\n",
    "    # UNRAVEL : \n",
    "    trac_L      = np.ravel(trac_L)\n",
    "    loc_a       = np.ravel(loc_a)\n",
    "    loc_b       = np.ravel(loc_b)\n",
    "    loc_theta   = np.ravel(loc_theta)\n",
    "    loc_pol_x   = np.ravel(loc_pol_x)\n",
    "    loc_pol_y   = np.ravel(loc_pol_y)\n",
    "    grad_H_90_x = np.ravel(grad_H_90_x)\n",
    "    grad_H_90_y = np.ravel(grad_H_90_y)\n",
    "    dot_prod_L  = np.ravel(dot_prod_L)\n",
    "    mag_pol_L   = np.ravel(mag_pol_L)\n",
    "    mag_bathy   = np.ravel(mag_bathy)\n",
    "    ali_L       = np.ravel(ali_L)\n",
    "    topos_L     = np.ravel(topos_L)\n",
    "    ortho_L     = np.ravel(ortho_L)\n",
    "    lat         = np.ravel(lat)\n",
    "    lon         = np.ravel(lon)\n",
    "    # CREATE DATAFRAME :\n",
    "    dataset = pd.DataFrame({'trac_L'      : trac_L,\n",
    "                            'loc_a'       : loc_a,\n",
    "                            'loc_b'       : loc_b,\n",
    "                            'loc_theta'   : loc_theta,\n",
    "                            'loc_pol_x'   : loc_pol_x,\n",
    "                            'loc_pol_y'   : loc_pol_y,\n",
    "                            'grad_H_90_x' : grad_H_90_x,\n",
    "                            'grad_H_90_y' : grad_H_90_y,\n",
    "                            'dot_prod_L'  : dot_prod_L,\n",
    "                            'mag_pol_L'   : mag_pol_L,\n",
    "                            'mag_bathy'   : mag_bathy,\n",
    "                            'ali_L'       : ali_L,\n",
    "                            'topos_L'     : topos_L,\n",
    "                            'ortho_L'     : ortho_L,\n",
    "                            'latitude'    : lat,\n",
    "                            'longitude'   : lon})\n",
    "    # Return dataset :\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for raw files : \n",
    "path_n = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SWOT_1_RAW_MERGED/version_2'\n",
    "# Path to save calculation : \n",
    "original_path_for_files = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SOFiA/SOFiA_NETCDF/'\n",
    "save_dataset_of_calc_folder = var_type+'_'+correction_type+'_'+mask_type+'_smoothing'+sm_name+'_delta_lat_lon_'+str(delta_lat_lon)+'_extent_of_area_'+str(lon1)+'_'+str(lon2)+'_'+str(lat1)+'_'+str(lat2)+'_'+passes_name#+'_'+seas_name\n",
    "\n",
    "for pass_number in list_of_passes:\n",
    "    print(pass_number)\n",
    "    save_dataset_of_calc_NETCDF = original_path_for_files + save_dataset_of_calc_folder + '/' + 'SOFiA_eq_14' + pass_number + '.nc'\n",
    "    dataset = create_dataframe_for_each_pass_loc_eq_14(path_n, pass_number, var_type, correction_type, mask_type, smoothing, season_type, cuu_L_binned, cvv_L_binned, cuv_L_binned, latbins_L, lonbins_L, ug_by_phi=None, by_phi_or_not='no', rotated_to_eart_grid_or_not='yes')\n",
    "    dataset.to_netcdf(save_dataset_of_calc_NETCDF)\n",
    "    print(save_dataset_of_calc_NETCDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for raw files : \n",
    "path_n = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SWOT_1_RAW_MERGED/version_2'\n",
    "# Path to save calculation : \n",
    "original_path_for_files = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SOFiA/SOFiA_NETCDF/'\n",
    "original_path_for_files_csv = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SOFiA/SOFiA_CSV/'\n",
    "save_dataset_of_calc_folder = var_type+'_'+correction_type+'_'+mask_type+'_smoothing'+sm_name+'_delta_lat_lon_'+str(delta_lat_lon)+'_extent_of_area_'+str(lon1)+'_'+str(lon2)+'_'+str(lat1)+'_'+str(lat2)+'_'+passes_name#+'_'+seas_name\n",
    "\n",
    "for pass_number in list_of_passes:\n",
    "    print(pass_number)\n",
    "    dataset = xr.open_dataset(original_path_for_files + save_dataset_of_calc_folder + '/' + 'SOFiA_' + pass_number + '.nc')\n",
    "    dataset_loc = unravel_SOFiA_values_into_dataset_eq14(dataset)\n",
    "    # Save the dataframe to a csv file\n",
    "    save_csv_name_L = original_path_for_files_csv + save_dataset_of_calc_folder + '/' + 'SOFiA_eq_14_loca_' + pass_number + '.csv'\n",
    "    # SAVE :\n",
    "    dataset_loc.to_csv(save_csv_name_L, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) OLD - Calculate and save in files for each track Mean-flow alignment, eddy-vs-mean-flow-alignment, etc. ... : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES INFORMATION :\n",
    "var_type         = 'ssha_karin_2'  # 'ssh_karin' , 'ssh_karin_2' , 'ssha_karin' , 'ssha_karin_2'\n",
    "correction_type  = 'ssha'          # 'ssh' , 'ssh_no_corr' , 'ssh_n_geoid' , 'ssha' , 'ssha_no_corr' , 'ssh_tide'\n",
    "mask_type        = 'all'           # 'all' , 'height_and_qual_flag' , 'qual_flag'\n",
    "smoothing        = None            #  None, 1, 3, ...\n",
    "season_type      = None            #  None, Winter, Summer, DFJ, ... --> NB! Correct this !!!\n",
    "\n",
    "if smoothing != None:\n",
    "    sm_name = str(smoothing)\n",
    "else:\n",
    "    sm_name = '0'\n",
    "\n",
    "if season_type != None:\n",
    "    seas_name = season_type\n",
    "else:\n",
    "    seas_name = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' SAVE THE BINNED DATASET, WITH CORRECT NAME : '''\n",
    "dataset_svae_path = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SOFiA/Spat_and_Temp_average'\n",
    "dataset_save_name = 'spat_temp_avg_'+var_type+'_'+correction_type+'_'+mask_type+'_smoothing'+sm_name+'_delta_lat_lon_'+str(delta_lat_lon)+'_extent_of_area_'+str(lon1)+'_'+str(lon2)+'_'+str(lat1)+'_'+str(lat2)+'_'+passes_name+'_'+seas_name+'.csv'\n",
    "dataset_save_full = dataset_svae_path + '/' + dataset_save_name\n",
    "print(dataset_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spat_temp_avg_nonan = pd.read_csv(dataset_save_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and check : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for raw files : \n",
    "path_n = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SWOT_1_RAW_MERGED/version_2'\n",
    "ds = xr.open_dataset(path_n + '/' + 'pass_003' + '.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ds.isel(concat_dim=20)['geoid'], vmin=55, vmax=58)#, cmap=cmocean.cm.balance, vmin=-0.4, vmax=0.01)\n",
    "plt.ylim(200,300)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(ds.isel(concat_dim=20)['mean_sea_surface_cnescls'], vmin=55, vmax=58)#, cmap=cmocean.cm.balance, vmin=-0.4, vmax=0.01)\n",
    "plt.ylim(200,300)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(ds.isel(concat_dim=20)['solid_earth_tide'], vmin=0.07, vmax=0.09, cmap=cmocean.cm.balance)\n",
    "plt.ylim(200,300)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(ds.isel(concat_dim=20)['ocean_tide_fes'], vmin=-0.4, vmax=-0.2, cmap=cmocean.cm.balance)\n",
    "plt.ylim(200,300)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(ds.isel(concat_dim=20)['internal_tide_hret'], vmin=-0.02, vmax=0.02, cmap=cmocean.cm.balance)\n",
    "plt.ylim(200,300)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(ds.isel(concat_dim=20)['pole_tide'], vmin=-0.0095, vmax=-0.009, cmap=cmocean.cm.balance)\n",
    "plt.ylim(200,300)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(ds.isel(concat_dim=20)['dac'], vmin=-0.1, vmax=-0.04, cmap=cmocean.cm.balance)\n",
    "plt.ylim(200,300)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_tide_corr = ds.isel(concat_dim=20)['solid_earth_tide']+ds.isel(concat_dim=20)['ocean_tide_fes']+ds.isel(concat_dim=20)['internal_tide_hret']+ds.isel(concat_dim=20)['pole_tide']+ds.isel(concat_dim=20)['dac']\n",
    "plt.imshow(ssh_tide_corr, vmin=-0.4, vmax=-0.1, cmap=cmocean.cm.balance)\n",
    "plt.ylim(200,300)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_tide_corr = ds['solid_earth_tide']+ds['ocean_tide_fes']+ds['internal_tide_hret']+ds['pole_tide']+ds['dac']\n",
    "plt.imshow(np.nanmean(ssh_tide_corr, axis=0), vmin=-0.1, vmax=-0.05, cmap=cmocean.cm.balance)\n",
    "plt.ylim(200,300)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_d, lat, lon, month_list, DJF, MAM, JJA, SON, Winter, Summer = SOFiA.get_corrected_and_masked_variables(ds, var_type, correction_type, mask_type, smoothing)\n",
    "masked_d_MEAN, latM, lonM, month_listM, DJFM, MAMM, JJAM, SONM, WinterM, SummerM = SOFiA.get_corrected_and_masked_variables(ds, 'ssh_karin_2', 'ssh_tide', mask_type, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.nanmean(masked_d_MEAN, axis=0), cmap=cmocean.cm.balance,vmin=-0.4, vmax=0)\n",
    "plt.ylim(200,300)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.nanmean(masked_d_MEAN, axis=0), cmap=cmocean.cm.balance,vmin=-0.4, vmax=0)\n",
    "plt.ylim(200,300)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masked_d_MEAN[20]-ssh_tide_corr, cmap=cmocean.cm.balance, vmin=-0.4, vmax=0.01)\n",
    "plt.ylim(200,300)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(masked_d[20], cmap=cmocean.cm.balance, vmin=-0.1, vmax=0.2)\n",
    "plt.ylim(200,300)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_local_covariance_for_each_track(masked_d, lat, lon, season_type, ug_by_phi):#ds, var_type, correction_type, mask_type, smoothing):\n",
    "    ''' GET CORRECTED VAIABLE DATA : '''\n",
    "    #masked_d, lat, lon, month_list, DJF, MAM, JJA, SON, Winter, Summer = SOFiA.get_corrected_and_masked_variables(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    ''' CALCULATE UG AND VG : '''\n",
    "    ug, vg = SOFiA.calculate_ug_vg(masked_d, lat, lon)\n",
    "    ''' CALCULATE UG AND VG ROTATED : '''\n",
    "    ug_rotated, vg_rotated, ug_rotated_by_phi = SOFiA.rotate_dataset_original_grid_all_cycles(lat, lon, ug, vg)\n",
    "    if season_type != None: \n",
    "        ''' GET THE SEASONAL INDICES : '''  \n",
    "        # ---> NB! Change season to mathc dec, jan, feb, mar, and jun, jul, aug, sep ...\n",
    "        ug_rotated        = ug_rotated[season_type]\n",
    "        vg_rotated        = vg_rotated[season_type]\n",
    "        ug_rotated_by_phi = ug_rotated_by_phi[season_type]\n",
    "    ''' 4 ) Calculate Temporal mean ug and vg (for each track) : '''\n",
    "    ug_temp_mean        = np.nanmean(ug_rotated, axis=0)\n",
    "    vg_temp_mean        = np.nanmean(vg_rotated, axis=0)\n",
    "    ug_by_phi_temp_mean = np.nanmean(ug_rotated_by_phi, axis=0)\n",
    "    ''' 6 ) CALCULATE COVARIANCE MATRICES : '''\n",
    "    # Add choice for ug by phi\n",
    "    if ug_by_phi == 'yes':\n",
    "        ug_temp_mean = ug_by_phi_temp_mean\n",
    "    else: \n",
    "        ug_temp_mean = ug_temp_mean\n",
    "    # LOCAL : \n",
    "    local_covariance_cuu = np.nanmean(np.multiply(ug_rotated - ug_temp_mean, ug_rotated - ug_temp_mean), axis=0)\n",
    "    local_covariance_cvv = np.nanmean(np.multiply(vg_rotated - vg_temp_mean, vg_rotated - vg_temp_mean), axis=0)\n",
    "    local_covariance_cuv = np.nanmean(np.multiply(ug_rotated - ug_temp_mean, vg_rotated - vg_temp_mean), axis=0)\n",
    "    # NEEDS SPATIAL AVERAGE OR BINNING FOR ALL TO BE COMPLETE !!!\n",
    "    return local_covariance_cuu, local_covariance_cvv, local_covariance_cuv\n",
    "\n",
    "def calculate_mean_ssh_ug_vg(masked_d_MEAN, lat, lon, season_type, ug_by_phi):\n",
    "    ''' CALCULATE UG AND VG : '''\n",
    "    ug, vg = SOFiA.calculate_ug_vg(masked_d_MEAN, lat, lon)\n",
    "    ''' CALCULATE UG AND VG ROTATED : '''\n",
    "    ug_rotated, vg_rotated, ug_rotated_by_phi = SOFiA.rotate_dataset_original_grid_all_cycles(lat, lon, ug, vg)\n",
    "    if season_type != None: \n",
    "        ''' GET THE SEASONAL INDICES : '''  \n",
    "        # ---> NB! Change season to mathc dec, jan, feb, mar, and jun, jul, aug, sep ...\n",
    "        ug_rotated        = ug_rotated[season_type]\n",
    "        vg_rotated        = vg_rotated[season_type]\n",
    "        ug_rotated_by_phi = ug_rotated_by_phi[season_type]\n",
    "    ''' 4 ) Calculate Temporal mean ug and vg (for each track) : '''\n",
    "    ug_temp_mean        = np.nanmean(ug_rotated, axis=0)\n",
    "    vg_temp_mean        = np.nanmean(vg_rotated, axis=0)\n",
    "    ug_by_phi_temp_mean = np.nanmean(ug_rotated_by_phi, axis=0)\n",
    "    ''' 6 ) CALCULATE COVARIANCE MATRICES : '''\n",
    "    # Add choice for ug by phi\n",
    "    if ug_by_phi == 'yes':\n",
    "        ug_temp_mean = ug_by_phi_temp_mean\n",
    "    else: \n",
    "        ug_temp_mean = ug_temp_mean\n",
    "    return ug_temp_mean, vg_temp_mean\n",
    "\n",
    "def calculate_det_trc_a_b_theta(cuu,cvv,cuv):\n",
    "    ''' FUNCTION TO CALCULATE DETERMINANT, TRACE, A, B, AND THETHA\n",
    "        Can be on track-level or on binned-level.'''\n",
    "    ''' CALCULATE DETC, TRC : '''\n",
    "    detc = np.real(cuu*cvv-cuv**2)   # determinant of covariance matrix\n",
    "    trc = cuu + cvv                  # trace of covariance matrix\n",
    "    ''' CALCULATE A, B, THETA : '''\n",
    "    a     = np.sqrt(trc/2+np.sqrt(trc**2-4*detc)/2)  # semi-major axis\n",
    "    b     = np.sqrt(trc/2-np.sqrt(trc**2-4*detc)/2)  # semi-minor axis\n",
    "    theta = np.arctan2(2*cuv,cuu-cvv)/2              # orientation angle\n",
    "    return detc, trc, a, b, theta\n",
    "\n",
    "# (a_L, b_L, theta_L, ds, 'no', 'yes')\n",
    "def calculate_polarized_vector_and_rotated_bathymetry_vector_and_mean_ssh_vector(mean_ssh_ds, a, b, theta, ds_of_track_for_topogrphy, by_phi_or_not, rotated_to_eart_grid_or_not):\n",
    "    ''' 1 ) Get polarized part of variance : '''\n",
    "    # Polarized  part of velocity variance ellipse : \n",
    "    polarized_magnitude = np.sqrt(a**2 - b**2)\n",
    "    # Define the unit vector in the direction of the major axis\n",
    "    N = np.array([np.cos(theta), np.sin(theta)])\n",
    "    # Compute the polarized component\n",
    "    polarized_component = polarized_magnitude * N\n",
    "    ''' 2 ) Get barthymetry from the track : '''\n",
    "    grad_H_num_lines, grad_H_num_pixels, grad_H_num_pixels_rotated, grad_H_num_lines_rotated, grad_H_num_pixels_rotated_by_phi = get_bathymetry_from_SWOT_file_orig_grid_and_Earth_grid(ds_of_track_for_topogrphy)\n",
    "    # Dividen by phi or not:\n",
    "    if by_phi_or_not == 'yes':\n",
    "        grad_H_num_pixels_rotated = grad_H_num_pixels_rotated_by_phi\n",
    "    else: \n",
    "        grad_H_num_pixels_rotated = grad_H_num_pixels_rotated\n",
    "    # Gradient vector\n",
    "    if rotated_to_eart_grid_or_not == 'yes':\n",
    "        # Gradient vector\n",
    "        gradH = np.array([grad_H_num_pixels_rotated, grad_H_num_lines_rotated])\n",
    "        # Apply 90-degree rotation manually\n",
    "        grad_H_rot_90 = np.array([-grad_H_num_lines_rotated, grad_H_num_pixels_rotated])\n",
    "    elif rotated_to_eart_grid_or_not == 'no':\n",
    "        # Gradient vector\n",
    "        gradH = np.array([grad_H_num_pixels, grad_H_num_lines])\n",
    "        # Apply 90-degree rotation manually\n",
    "        grad_H_rot_90 = np.array([-grad_H_num_lines, grad_H_num_pixels])\n",
    "    ''' Another way to calculate : '''\n",
    "    # Rotation matrix\n",
    "    R_90 = np.array([[0, -1], [1, 0]])\n",
    "    # Rotate each vector in batch across provided dimensions [2,x,y]\n",
    "    # Reshape and permute to apply batch rotation per vector (2, dimensions)\n",
    "    gradH_rotated = np.einsum('ij,jkl->ikl', R_90, gradH)\n",
    "    # Ensure matching dimensions for computation\n",
    "    dot_product1 = np.einsum('ijm,ijm->jm', gradH_rotated, polarized_component)\n",
    "    ''' 3 ) Get MEAN vector (rotated): '''\n",
    "    ug_temp_mean, vg_temp_mean = calculate_mean_ssh_ug_vg(mean_ssh_ds, lat, lon, season_type, by_phi_or_not)\n",
    "    mean_ug_vg_vector = np.array([ug_temp_mean,vg_temp_mean])\n",
    "    return polarized_component, grad_H_rot_90, mean_ug_vg_vector\n",
    "\n",
    "def calculate_alignment(vector_a, vector_b):\n",
    "    ''' FUNCTION TO CALCULATE THE ALIGNMENT, TOPOSTROPHY, AND ORTHOSTROPHY\n",
    "        This function do not normalize, can normalize at later scale, \n",
    "        when spatially averaged, or right away if looking at small scale. \n",
    "        Just a basic calculation ... '''\n",
    "    ''' 1 ) Calculate the dot-product between the two vectors : '''\n",
    "    dot_product2 = (vector_a[0]*vector_b[0] + vector_a[1]*vector_b[1])\n",
    "    magnitude_a = (np.sqrt(vector_a[0]**2+vector_a[1]**2))\n",
    "    magnitude_b = (np.sqrt(vector_b[0]**2+vector_b[1]**2))\n",
    "    ''' 2 ) ALIGNMENT :  '''\n",
    "    alignment   = np.abs(dot_product2)/(np.sqrt(vector_a[0]**2+vector_a[1]**2)*np.sqrt(vector_b[0]**2+vector_b[1]**2))\n",
    "    \n",
    "    return dot_product2, magnitude_a, magnitude_b, alignment\n",
    "\n",
    "def calculate_alignment_old(vector_a, vector_b):\n",
    "    ''' FUNCTION TO CALCULATE THE ALIGNMENT, TOPOSTROPHY, AND ORTHOSTROPHY\n",
    "        This function do not normalize, can normalize at later scale, \n",
    "        when spatially averaged, or right away if looking at small scale. \n",
    "        Just a basic calculation ... '''\n",
    "    polarized_component = vector_a\n",
    "    grad_H_rot_90       = vector_b\n",
    "    ''' 1 ) Calculate the dot-product between the two vectors : '''\n",
    "    dot_product2 = (grad_H_rot_90[0]*polarized_component[0] + grad_H_rot_90[1]*polarized_component[1])\n",
    "    magnitude_a  = (np.sqrt(polarized_component[0]**2+polarized_component[1]**2))\n",
    "    magnitude_b = (np.sqrt(grad_H_rot_90[0]**2+grad_H_rot_90[1]**2))\n",
    "    ''' 2 ) ALIGNMENT :  '''\n",
    "    alignment   = np.abs(dot_product2)/(np.sqrt(polarized_component[0]**2+polarized_component[1]**2)*np.sqrt(grad_H_rot_90[0]**2+grad_H_rot_90[1]**2))\n",
    "    \n",
    "    return dot_product2, magnitude_a, magnitude_b, alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product2, magnitude_polarized, magnitude_bathymetry, alignment, topostrophy1, topostrophy, orthostrophy = calculate_alignment_topostrophy_orthostrophy_of_variance(polarized_component, grad_H_rot_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_covariance_cuu, local_covariance_cvv, local_covariance_cuv = calculate_local_covariance_for_each_track(masked_d, lat, lon, season_type, None)\n",
    "detc_L, trc_L, a_L, b_L, theta_L = calculate_det_trc_a_b_theta(local_covariance_cuu,local_covariance_cvv,local_covariance_cuv)\n",
    "polarized_component, grad_H_rot_90, mean_ug_vg_vector = calculate_polarized_vector_and_rotated_bathymetry_vector_and_mean_ssh_vector(masked_d_MEAN, a_L, b_L, theta_L, ds, 'no', 'yes')\n",
    "\n",
    "dot_product2, magnitude_a, magnitude_b, alignment = calculate_alignment(mean_ug_vg_vector, polarized_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(alignment, cmap=cmocean.cm.matter, vmin=None, vmax=None, alpha=.7, zorder=20)\n",
    "plt.colorbar()\n",
    "plt.contour(ds.isel(concat_dim=20)['depth_or_elevation'], levels=30, zorder=10, vmin=-2000, vmax=0, linewidth=0.5)\n",
    "plt.ylim(650,750)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "#plt.contourf(np.nanmean(masked_d_MEAN, axis=0), levels=60, cmap=cmocean.cm.balance,vmin=-0.4, vmax=0, alpha=.7, zorder=20)#\n",
    "plt.imshow(alignment, cmap=cmocean.cm.speed, vmin=None, vmax=None, alpha=.7, zorder=20)\n",
    "plt.colorbar()\n",
    "#plt.contour(ds.isel(concat_dim=20)['depth_or_elevation'], levels=40, zorder=10, vmin=-2500, vmax=0, linewidth=2)\n",
    "plt.ylim(700,770)\n",
    "plt.quiver(mean_ug_vg_vector[0], mean_ug_vg_vector[1], scale=20, zorder=30, color='red', width=0.0015)\n",
    "plt.quiver(polarized_component[0], polarized_component[1], scale=5, zorder=30, color='yellow', width=0.0015, headlength=0, pivot='middle', headwidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "#plt.contourf(np.nanmean(masked_d_MEAN, axis=0), levels=60, cmap=cmocean.cm.balance,vmin=-0.4, vmax=0, alpha=.7, zorder=20)\n",
    "plt.imshow(alignment, cmap=cmocean.cm.matter_r, vmin=None, vmax=None, alpha=.7, zorder=20)\n",
    "plt.colorbar()\n",
    "plt.contour(ds.isel(concat_dim=20)['depth_or_elevation'], levels=40, zorder=10, vmin=-2500, vmax=0, linewidth=2)\n",
    "plt.ylim(650,750)\n",
    "plt.quiver(mean_ug_vg_vector[0], mean_ug_vg_vector[1], scale=30, zorder=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "plt.contourf(np.nanmean(masked_d_MEAN, axis=0), levels=60, cmap=cmocean.cm.balance,vmin=-0.4, vmax=0, alpha=.7, zorder=20)\n",
    "plt.contour(ds.isel(concat_dim=20)['depth_or_elevation'], levels=30, zorder=10, vmin=-2000, vmax=0, linewidth=0.5)\n",
    "plt.ylim(650,900)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.quiver(mean_ug_vg_vector[0],mean_ug_vg_vector[1], scale=20)\n",
    "plt.imshow(ds.isel(concat_dim=20)['depth_or_elevation'])#, cmap=cmocean.cm.balance,vmin=-0.4, vmax=0)\n",
    "plt.ylim(650,750)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.quiver(mean_ug_vg_vector[0],mean_ug_vg_vector[1], scale=20)\n",
    "plt.imshow(np.nanmean(masked_d_MEAN, axis=0), cmap=cmocean.cm.balance,vmin=-0.4, vmax=0)\n",
    "plt.ylim(650,750)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.quiver(mean_ug_vg_vector[0],mean_ug_vg_vector[1], scale=20)\n",
    "plt.imshow(magnitude_b)#, cmap=cmocean.cm.balance,vmin=-0.4, vmax=0)\n",
    "plt.ylim(650,750)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_cos = np.arccos(alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(magnitude_b)#, vmin=0, vmax=0.001)#, cmap='Spectral_r')#, vmin=0, vmax=0.5)\n",
    "plt.ylim(650,750)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(magnitude_a, vmin=0, vmax=0.2)\n",
    "plt.ylim(650,750)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(dot_product2, vmin=0, vmax=0.5)\n",
    "plt.ylim(650,750)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(alignment)#, vmin=0, vmax=1.57)\n",
    "plt.ylim(650,750)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(theta_cos)#, vmin=0, vmax=0.02)\n",
    "plt.ylim(650,750)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(alignment, cmap='Spectral_r')#, vmin=0, vmax=0.5)\n",
    "#plt.ylim(200,300)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe_for_each_pass_tot_loc_bulk_a_b_ali_topo_ortho(path_for_pass, pass_number, var_type, correction_type, mask_type, smoothing, season_type, df_spat_temp_avg_nonan, ug_by_phi=None, by_phi_or_not='no', rotated_to_eart_grid_or_not='yes'):\n",
    "    # Open passes : \n",
    "    #ds = xr.open_dataset(path_for_pass+pass_number)\n",
    "    ds = xr.open_dataset(path_for_pass + '/' + pass_number + '.nc')\n",
    "    # GET CORRECTED VAIABLE DATA : \n",
    "    masked_d, lat, lon, month_list, DJF, MAM, JJA, SON, Winter, Summer = SOFiA.get_corrected_and_masked_variables(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    masked_d_MEAN, latM, lonM, month_listM, DJFM, MAMM, JJAM, SONM, WinterM, SummerM = SOFiA.get_corrected_and_masked_variables(ds, 'ssh_karin_2', 'ssh_tide', mask_type, smoothing)\n",
    "    # Calculate Total, Local and Bulk covariance :\n",
    "    total_covariance_cuu, total_covariance_cvv, total_covariance_cuv, local_covariance_cuu, local_covariance_cvv, local_covariance_cuv, bulk_covariance_cuu, bulk_covariance_cvv, bulk_covariance_cuv = calculate_total_local_bulk_covariance_for_each_track(masked_d, lat, lon, season_type, df_spat_temp_avg_nonan, ug_by_phi)#(ds, var_type, correction_type, mask_type, smoothing, season_type, df_spat_temp_avg_nonan, ug_by_phi)\n",
    "    # Calculate detc, trace, a, b, and theta for Local covariance : \n",
    "    detc_L, trc_L, a_L, b_L, theta_L = calculate_det_trc_a_b_theta(local_covariance_cuu,local_covariance_cvv,local_covariance_cuv)\n",
    "    # Calculate Polarized vector and Rotated Bathymetry vector :\n",
    "    local_polarized_component, grad_H_rot_90 = calculate_polarized_vector_and_rotated_bathymetry_vector(a_L, b_L, theta_L, ds, 'no', 'yes')\n",
    "    # Calculate small scale alignment, topostrophy, orthostrophy, and dot-poduct between polarized and grad_H (and magnitude) : \n",
    "    dot_product2_L, magnitude_polarized_L, magnitude_bathymetry_L, alignment_L, topostrophy1_L, topostrophy_L, orthostrophy_L = calculate_alignment_topostrophy_orthostrophy_of_variance(local_polarized_component, grad_H_rot_90)\n",
    "    ''' MAKE DATAFRAME : '''\n",
    "    # Fix the Longitude : \n",
    "    new_lon = np.where(lon > 180, lon - 360, lon)\n",
    "    # Num_lines, Num_pixels :\n",
    "    num_lines  = lat.shape[0]\n",
    "    num_pixels = lat.shape[1]\n",
    "    ''' CREATE DataArrays '''\n",
    "    cuu_T       = xr.DataArray(total_covariance_cuu,         dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    cvv_T       = xr.DataArray(total_covariance_cvv,         dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    cuv_T       = xr.DataArray(total_covariance_cuv,         dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    cuu_L       = xr.DataArray(local_covariance_cuu,         dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    cvv_L       = xr.DataArray(local_covariance_cvv,         dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    cuv_L       = xr.DataArray(local_covariance_cuv,         dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    cuu_B       = xr.DataArray(bulk_covariance_cuu,          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    cvv_B       = xr.DataArray(bulk_covariance_cvv,          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    cuv_B       = xr.DataArray(bulk_covariance_cuv,          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    trac_T      = xr.DataArray(trc_T,                        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    trac_L      = xr.DataArray(trc_L,                        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    trac_B      = xr.DataArray(trc_B,                        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    tot_a       = xr.DataArray(a_T,                          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    loc_a       = xr.DataArray(a_L,                          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    bul_a       = xr.DataArray(a_B,                          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    tot_b       = xr.DataArray(b_T,                          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    loc_b       = xr.DataArray(b_L,                          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    bul_b       = xr.DataArray(b_B,                          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    tot_theta   = xr.DataArray(theta_T,                      dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    loc_theta   = xr.DataArray(theta_L,                      dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    bul_theta   = xr.DataArray(theta_B,                      dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    tot_pol_x   = xr.DataArray(total_polarized_component[0], dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    tot_pol_y   = xr.DataArray(total_polarized_component[1], dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    loc_pol_x   = xr.DataArray(local_polarized_component[0], dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    loc_pol_y   = xr.DataArray(local_polarized_component[1], dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    bul_pol_x   = xr.DataArray(bulk_polarized_component[0],  dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    bul_pol_y   = xr.DataArray(bulk_polarized_component[1],  dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    grad_H_90_x = xr.DataArray(grad_H_rot_90[0],             dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    grad_H_90_y = xr.DataArray(grad_H_rot_90[1],             dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    dot_prod_T  = xr.DataArray(dot_product2_T,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    dot_prod_L  = xr.DataArray(dot_product2_L,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    dot_prod_B  = xr.DataArray(dot_product2_B,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    mag_pol_T   = xr.DataArray(magnitude_polarized_T,        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    mag_pol_L   = xr.DataArray(magnitude_polarized_L,        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    mag_pol_B   = xr.DataArray(magnitude_polarized_B,        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    mag_bathy   = xr.DataArray(magnitude_bathymetry_L,       dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ali_T       = xr.DataArray(alignment_T,                  dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ali_L       = xr.DataArray(alignment_L,                  dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ali_B       = xr.DataArray(alignment_B,                  dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    topos1_T    = xr.DataArray(topostrophy1_T,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    topos1_L    = xr.DataArray(topostrophy1_L,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    topos1_B    = xr.DataArray(topostrophy1_B,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    topos_T     = xr.DataArray(topostrophy_T,                dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    topos_L     = xr.DataArray(topostrophy_L,                dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    topos_B     = xr.DataArray(topostrophy_B,                dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ortho_T     = xr.DataArray(orthostrophy_T,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ortho_L     = xr.DataArray(orthostrophy_L,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ortho_B     = xr.DataArray(orthostrophy_B,               dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    lat_df      = xr.DataArray(lat,                          dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    new_lon_df  = xr.DataArray(new_lon,                      dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ''' CREATE DataSet '''\n",
    "    dataset  = xr.Dataset({ 'cuu_T'       : cuu_T,\n",
    "                            'cvv_T'       : cvv_T,\n",
    "                            'cuv_T'       : cuv_T,\n",
    "                            'cuu_L'       : cuu_L,\n",
    "                            'cvv_L'       : cvv_L,\n",
    "                            'cuv_L'       : cuv_L,\n",
    "                            'cuu_B'       : cuu_B,\n",
    "                            'cvv_B'       : cvv_B,\n",
    "                            'cuv_B'       : cuv_B,\n",
    "                            'trac_T'      : trac_T,\n",
    "                            'trac_L'      : trac_L,\n",
    "                            'trac_B'      : trac_B,\n",
    "                            'tot_a'       : tot_a,\n",
    "                            'loc_a'       : loc_a,\n",
    "                            'bul_a'       : bul_a,\n",
    "                            'tot_b'       : tot_b,\n",
    "                            'loc_b'       : loc_b,\n",
    "                            'bul_b'       : bul_b,\n",
    "                            'tot_theta'   : tot_theta,\n",
    "                            'loc_theta'   : loc_theta,\n",
    "                            'bul_theta'   : bul_theta,\n",
    "                            'tot_pol_x'   : tot_pol_x,\n",
    "                            'tot_pol_y'   : tot_pol_y,\n",
    "                            'loc_pol_x'   : loc_pol_x,\n",
    "                            'loc_pol_y'   : loc_pol_y,\n",
    "                            'bul_pol_x'   : bul_pol_x,\n",
    "                            'bul_pol_y'   : bul_pol_y,\n",
    "                            'grad_H_90_x' : grad_H_90_x,\n",
    "                            'grad_H_90_y' : grad_H_90_y,\n",
    "                            'dot_prod_T'  : dot_prod_T,\n",
    "                            'dot_prod_L'  : dot_prod_L,\n",
    "                            'dot_prod_B'  : dot_prod_B,\n",
    "                            'mag_pol_T'   : mag_pol_T,\n",
    "                            'mag_pol_L'   : mag_pol_L,\n",
    "                            'mag_pol_B'   : mag_pol_B,\n",
    "                            'mag_bathy'   : mag_bathy,\n",
    "                            'ali_T'       : ali_T,\n",
    "                            'ali_L'       : ali_L,\n",
    "                            'ali_B'       : ali_B,\n",
    "                            'topos_T'     : topos1_T,\n",
    "                            'topos_L'     : topos1_L,\n",
    "                            'topos_B'     : topos1_B,\n",
    "                            'ortho_T'     : ortho_T,\n",
    "                            'ortho_L'     : ortho_L,\n",
    "                            'ortho_B'     : ortho_B,\n",
    "                            'lat_df'      : lat_df,\n",
    "                            'new_lon_df'  : new_lon_df})\n",
    "    ''' RETURN THE DataSet '''\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_local_covariance_for_each_track(masked_d, lat, lon, season_type, ug_by_phi):#ds, var_type, correction_type, mask_type, smoothing):\n",
    "    ''' GET CORRECTED VAIABLE DATA : '''\n",
    "    #masked_d, lat, lon, month_list, DJF, MAM, JJA, SON, Winter, Summer = SOFiA.get_corrected_and_masked_variables(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    ''' CALCULATE UG AND VG : '''\n",
    "    ug, vg = SOFiA.calculate_ug_vg(masked_d, lat, lon)\n",
    "    ''' CALCULATE UG AND VG ROTATED : '''\n",
    "    ug_rotated, vg_rotated, ug_rotated_by_phi = SOFiA.rotate_dataset_original_grid_all_cycles(lat, lon, ug, vg)\n",
    "    if season_type != None: \n",
    "        ''' GET THE SEASONAL INDICES : '''  \n",
    "        # ---> NB! Change season to mathc dec, jan, feb, mar, and jun, jul, aug, sep ...\n",
    "        ug_rotated        = ug_rotated[season_type]\n",
    "        vg_rotated        = vg_rotated[season_type]\n",
    "        ug_rotated_by_phi = ug_rotated_by_phi[season_type]\n",
    "    ''' 4 ) Calculate Temporal mean ug and vg (for each track) : '''\n",
    "    ug_temp_mean        = np.nanmean(ug_rotated, axis=0)\n",
    "    vg_temp_mean        = np.nanmean(vg_rotated, axis=0)\n",
    "    ug_by_phi_temp_mean = np.nanmean(ug_rotated_by_phi, axis=0)\n",
    "    ''' 6 ) CALCULATE COVARIANCE MATRICES : '''\n",
    "    # Add choice for ug by phi\n",
    "    if ug_by_phi == 'yes':\n",
    "        ug_temp_mean = ug_by_phi_temp_mean\n",
    "    else: \n",
    "        ug_temp_mean = ug_temp_mean\n",
    "    # LOCAL : \n",
    "    local_covariance_cuu = np.nanmean(np.multiply(ug_rotated - ug_temp_mean, ug_rotated - ug_temp_mean), axis=0)\n",
    "    local_covariance_cvv = np.nanmean(np.multiply(vg_rotated - vg_temp_mean, vg_rotated - vg_temp_mean), axis=0)\n",
    "    local_covariance_cuv = np.nanmean(np.multiply(ug_rotated - ug_temp_mean, vg_rotated - vg_temp_mean), axis=0)\n",
    "    # NEEDS SPATIAL AVERAGE OR BINNING FOR ALL TO BE COMPLETE !!!\n",
    "    return local_covariance_cuu, local_covariance_cvv, local_covariance_cuv\n",
    "\n",
    "def calculate_mean_ssh_ug_vg(masked_d_MEAN, lat, lon, season_type, ug_by_phi):\n",
    "    ''' CALCULATE UG AND VG : '''\n",
    "    ug, vg = SOFiA.calculate_ug_vg(masked_d_MEAN, lat, lon)\n",
    "    ''' CALCULATE UG AND VG ROTATED : '''\n",
    "    ug_rotated, vg_rotated, ug_rotated_by_phi = SOFiA.rotate_dataset_original_grid_all_cycles(lat, lon, ug, vg)\n",
    "    if season_type != None: \n",
    "        ''' GET THE SEASONAL INDICES : '''  \n",
    "        # ---> NB! Change season to mathc dec, jan, feb, mar, and jun, jul, aug, sep ...\n",
    "        ug_rotated        = ug_rotated[season_type]\n",
    "        vg_rotated        = vg_rotated[season_type]\n",
    "        ug_rotated_by_phi = ug_rotated_by_phi[season_type]\n",
    "    ''' 4 ) Calculate Temporal mean ug and vg (for each track) : '''\n",
    "    ug_temp_mean        = np.nanmean(ug_rotated, axis=0)\n",
    "    vg_temp_mean        = np.nanmean(vg_rotated, axis=0)\n",
    "    ug_by_phi_temp_mean = np.nanmean(ug_rotated_by_phi, axis=0)\n",
    "    ''' 6 ) CALCULATE COVARIANCE MATRICES : '''\n",
    "    # Add choice for ug by phi\n",
    "    if ug_by_phi == 'yes':\n",
    "        ug_temp_mean = ug_by_phi_temp_mean\n",
    "    else: \n",
    "        ug_temp_mean = ug_temp_mean\n",
    "    return ug_temp_mean, vg_temp_mean\n",
    "\n",
    "def calculate_det_trc_a_b_theta(cuu,cvv,cuv):\n",
    "    ''' FUNCTION TO CALCULATE DETERMINANT, TRACE, A, B, AND THETHA\n",
    "        Can be on track-level or on binned-level.'''\n",
    "    ''' CALCULATE DETC, TRC : '''\n",
    "    detc = np.real(cuu*cvv-cuv**2)   # determinant of covariance matrix\n",
    "    trc = cuu + cvv                  # trace of covariance matrix\n",
    "    ''' CALCULATE A, B, THETA : '''\n",
    "    a     = np.sqrt(trc/2+np.sqrt(trc**2-4*detc)/2)  # semi-major axis\n",
    "    b     = np.sqrt(trc/2-np.sqrt(trc**2-4*detc)/2)  # semi-minor axis\n",
    "    theta = np.arctan2(2*cuv,cuu-cvv)/2              # orientation angle\n",
    "    return detc, trc, a, b, theta\n",
    "\n",
    "# (a_L, b_L, theta_L, ds, 'no', 'yes')\n",
    "def calculate_polarized_vector_and_rotated_bathymetry_vector_and_mean_ssh_vector(mean_ssh_ds, lat, lon, season_type, a, b, theta, ds_of_track_for_topogrphy, by_phi_or_not, rotated_to_eart_grid_or_not):\n",
    "    ''' 1 ) Get polarized part of variance : '''\n",
    "    # Polarized  part of velocity variance ellipse : \n",
    "    polarized_magnitude = np.sqrt(a**2 - b**2)\n",
    "    # Define the unit vector in the direction of the major axis\n",
    "    N = np.array([np.cos(theta), np.sin(theta)])\n",
    "    # Compute the polarized component\n",
    "    polarized_component = polarized_magnitude * N\n",
    "    ''' 2 ) Get barthymetry from the track : '''\n",
    "    grad_H_num_lines, grad_H_num_pixels, grad_H_num_pixels_rotated, grad_H_num_lines_rotated, grad_H_num_pixels_rotated_by_phi = get_bathymetry_from_SWOT_file_orig_grid_and_Earth_grid(ds_of_track_for_topogrphy)\n",
    "    # Dividen by phi or not:\n",
    "    if by_phi_or_not == 'yes':\n",
    "        grad_H_num_pixels_rotated = grad_H_num_pixels_rotated_by_phi\n",
    "    else: \n",
    "        grad_H_num_pixels_rotated = grad_H_num_pixels_rotated\n",
    "    # Gradient vector\n",
    "    if rotated_to_eart_grid_or_not == 'yes':\n",
    "        # Gradient vector\n",
    "        gradH = np.array([grad_H_num_pixels_rotated, grad_H_num_lines_rotated])\n",
    "        # Apply 90-degree rotation manually\n",
    "        grad_H_rot_90 = np.array([-grad_H_num_lines_rotated, grad_H_num_pixels_rotated])\n",
    "    elif rotated_to_eart_grid_or_not == 'no':\n",
    "        # Gradient vector\n",
    "        gradH = np.array([grad_H_num_pixels, grad_H_num_lines])\n",
    "        # Apply 90-degree rotation manually\n",
    "        grad_H_rot_90 = np.array([-grad_H_num_lines, grad_H_num_pixels])\n",
    "    ''' Another way to calculate : '''\n",
    "    # Rotation matrix\n",
    "    R_90 = np.array([[0, -1], [1, 0]])\n",
    "    # Rotate each vector in batch across provided dimensions [2,x,y]\n",
    "    # Reshape and permute to apply batch rotation per vector (2, dimensions)\n",
    "    gradH_rotated = np.einsum('ij,jkl->ikl', R_90, gradH)\n",
    "    # Ensure matching dimensions for computation\n",
    "    dot_product1 = np.einsum('ijm,ijm->jm', gradH_rotated, polarized_component)\n",
    "    ''' 3 ) Get MEAN vector (rotated): '''\n",
    "    ug_temp_mean, vg_temp_mean = calculate_mean_ssh_ug_vg(mean_ssh_ds, lat, lon, season_type, by_phi_or_not)\n",
    "    mean_ug_vg_vector = np.array([ug_temp_mean,vg_temp_mean])\n",
    "    return polarized_component, grad_H_rot_90, mean_ug_vg_vector\n",
    "\n",
    "def calculate_alignment(vector_a, vector_b):\n",
    "    ''' FUNCTION TO CALCULATE THE ALIGNMENT, TOPOSTROPHY, AND ORTHOSTROPHY\n",
    "        This function do not normalize, can normalize at later scale, \n",
    "        when spatially averaged, or right away if looking at small scale. \n",
    "        Just a basic calculation ... '''\n",
    "    ''' 1 ) Calculate the dot-product between the two vectors : '''\n",
    "    dot_product2 = (vector_a[0]*vector_b[0] + vector_a[1]*vector_b[1])\n",
    "    magnitude_a = (np.sqrt(vector_a[0]**2+vector_a[1]**2))\n",
    "    magnitude_b = (np.sqrt(vector_b[0]**2+vector_b[1]**2))\n",
    "    ''' 2 ) ALIGNMENT :  '''\n",
    "    alignment   = np.abs(dot_product2)/(np.sqrt(vector_a[0]**2+vector_a[1]**2)*np.sqrt(vector_b[0]**2+vector_b[1]**2))\n",
    "    \n",
    "    return dot_product2, magnitude_a, magnitude_b, alignment\n",
    "\n",
    "def calculate_alignment_old(vector_a, vector_b):\n",
    "    ''' FUNCTION TO CALCULATE THE ALIGNMENT, TOPOSTROPHY, AND ORTHOSTROPHY\n",
    "        This function do not normalize, can normalize at later scale, \n",
    "        when spatially averaged, or right away if looking at small scale. \n",
    "        Just a basic calculation ... '''\n",
    "    polarized_component = vector_a\n",
    "    grad_H_rot_90       = vector_b\n",
    "    ''' 1 ) Calculate the dot-product between the two vectors : '''\n",
    "    dot_product2 = (grad_H_rot_90[0]*polarized_component[0] + grad_H_rot_90[1]*polarized_component[1])\n",
    "    magnitude_a  = (np.sqrt(polarized_component[0]**2+polarized_component[1]**2))\n",
    "    magnitude_b = (np.sqrt(grad_H_rot_90[0]**2+grad_H_rot_90[1]**2))\n",
    "    ''' 2 ) ALIGNMENT :  '''\n",
    "    alignment   = np.abs(dot_product2)/(np.sqrt(polarized_component[0]**2+polarized_component[1]**2)*np.sqrt(grad_H_rot_90[0]**2+grad_H_rot_90[1]**2))\n",
    "    \n",
    "    return dot_product2, magnitude_a, magnitude_b, alignment\n",
    "\n",
    "def calculate_topostrophy_orthostrophy_of_variance(vector_a, topography_vector):\n",
    "    ''' FUNCTION TO CALCULATE THE ALIGNMENT, TOPOSTROPHY, AND ORTHOSTROPHY\n",
    "        This function do not normalize, can normalize at later scale, \n",
    "        when spatially averaged, or right away if looking at small scale. \n",
    "        Just a basic calculation ... '''\n",
    "    polarized_component = vector_a\n",
    "    grad_H_rot_90       = topography_vector\n",
    "    ''' 1 ) Calculate the dot-product between the two vectors : '''\n",
    "    # Compute dot product\n",
    "    #dot_product = np.dot(grad_H_rot_90, polarized_component)\n",
    "    dot_product2 = (grad_H_rot_90[0]*polarized_component[0] + grad_H_rot_90[1]*polarized_component[1])\n",
    "    magnitude_polarized  = (np.sqrt(polarized_component[0]**2+polarized_component[1]**2))\n",
    "    magnitude_bathymetry = (np.sqrt(grad_H_rot_90[0]**2+grad_H_rot_90[1]**2))\n",
    "    ''' PROJECT A ONTO B : '''\n",
    "    flow_norm = polarized_component #/ (np.sqrt(polarized_component[0]**2+polarized_component[1]**2))\n",
    "    dot_product_a_b = (grad_H_rot_90[0]*flow_norm[0] + grad_H_rot_90[1]*flow_norm[1])\n",
    "    # Calculate squared norm safely by summing along the first axis (axis=0)\n",
    "    norm_squared = np.sum(grad_H_rot_90**2, axis=0)  # Shape should be (2062, 69)\n",
    "    # Element-wise condition to avoid division by zero\n",
    "    is_nonzero = norm_squared != 0  # Shape: (2062, 69)\n",
    "    # Modify norm_squared shape for broadcasting to ensure compatibility across operations\n",
    "    norm_squared = norm_squared[np.newaxis, :, :]  # Shape: (1, 2062, 69)\n",
    "    # Safe projection calculation using broadcasting\n",
    "    Proj_a_onto_b = np.where(is_nonzero[np.newaxis, :, :], (dot_product_a_b / norm_squared) * grad_H_rot_90, np.zeros_like(grad_H_rot_90))\n",
    "    #Proj_a_onto_b = np.abs(dot_product_a_b)/(grad_H_rot_90[0]*grad_H_rot_90[0] + grad_H_rot_90[1]*grad_H_rot_90[1])*grad_H_rot_90\n",
    "    non_topostrophy_a = flow_norm - Proj_a_onto_b\n",
    "    ''' ORTHOSTROPHY : '''\n",
    "    orthostrophy = np.sqrt(non_topostrophy_a[0]**2 + non_topostrophy_a[1]**2)\n",
    "    ''' TOPOSTROPHY : '''\n",
    "    topostrophy1 = np.abs(dot_product2)/(np.sqrt(grad_H_rot_90[0]**2+grad_H_rot_90[1]**2))\n",
    "    topostrophy  = np.sqrt(Proj_a_onto_b[0]**2 + Proj_a_onto_b[1]**2)\n",
    "    return topostrophy1, topostrophy, orthostrophy\n",
    "\n",
    "def get_bathymetry_from_SWOT_file_orig_grid_and_Earth_grid(ds):\n",
    "    # Get dept_or_elevation layer from the original SWOT dataset : \n",
    "    bath_swot = ds.depth_or_elevation # Should be the same for all cycles, so only need one\n",
    "    bath_swot = ds.isel(concat_dim=12).depth_or_elevation \n",
    "    bath_lat  = ds.isel(concat_dim=12).latitude\n",
    "    bath_lon  = ds.isel(concat_dim=12).longitude \n",
    "    # dx, dy 2 km : \n",
    "    dx = 2000\n",
    "    dy = 2000\n",
    "    # Get Grad_H\n",
    "    grad_H_num_lines  = np.gradient(bath_swot, axis=0)/dy # y-axis - local SWOT-grid\n",
    "    grad_H_num_pixels = np.gradient(bath_swot, axis=1)/dx # x-axis - local SWOT-grid\n",
    "    # Rotate to Earth-Grid : \n",
    "    grad_H_num_pixels_rotated, grad_H_num_lines_rotated, grad_H_num_pixels_rotated_by_phi =  SOFiA.rotate_dataset_original_grid_all_cycles(bath_lat, bath_lon, grad_H_num_pixels, grad_H_num_lines)\n",
    "    return grad_H_num_lines, grad_H_num_pixels, grad_H_num_pixels_rotated, grad_H_num_lines_rotated, grad_H_num_pixels_rotated_by_phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for raw files : \n",
    "path_n = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SWOT_1_RAW_MERGED/version_2'\n",
    "ds = xr.open_dataset(path_n + '/' + 'pass_003' + '.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_d, lat, lon, month_list, DJF, MAM, JJA, SON, Winter, Summer = SOFiA.get_corrected_and_masked_variables(ds, var_type, correction_type, mask_type, smoothing)\n",
    "masked_d_MEAN, latM, lonM, month_listM, DJFM, MAMM, JJAM, SONM, WinterM, SummerM = SOFiA.get_corrected_and_masked_variables(ds, 'ssh_karin_2', 'ssh_tide', mask_type, smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ug_mean, vg_mean = SOFiA.calculate_ug_vg(masked_d_MEAN, lat, lon)\n",
    "ug_mean2, vg_mean2 = SOFiA.calculate_ug_vg(masked_d, lat, lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ug_mean_M = np.nanmean(ug_mean, axis=0)\n",
    "vg_mean_M = np.nanmean(vg_mean, axis=0)\n",
    "\n",
    "ug_mean_M2 = np.nanmean(ug_mean2, axis=0)\n",
    "vg_mean_M2 = np.nanmean(vg_mean2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ug_diff_mean = ug_mean - ug_mean_M\n",
    "vg_diff_mean = vg_mean - vg_mean_M\n",
    "ug_mean_M = np.nanmean(ug_diff_mean, axis=0)\n",
    "vg_mean_M = np.nanmean(vg_diff_mean, axis=0)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_mean_M = np.sqrt(ug_mean_M**2 + vg_mean_M**2)\n",
    "speed_mean_M2 = np.sqrt(ug_mean_M2**2 + vg_mean_M2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "#plt.imshow(alignment, vmin=0, vmax=1, cmap='Grays')\n",
    "plt.imshow(alignment, vmin=None, vmax=None, cmap=cmocean.cm.balance)\n",
    "plt.colorbar()\n",
    "#plt.contour(ds.isel(concat_dim=20)['depth_or_elevation'], levels=100, zorder=10, vmin=-2500, vmax=0, linewidths=2)\n",
    "#plt.ylim(715,735)\n",
    "#plt.xlim(5,25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "#plt.imshow(alignment, vmin=0, vmax=1, cmap='Grays')\n",
    "plt.imshow(alignment, vmin=None, vmax=None, cmap=cmocean.cm.balance)\n",
    "plt.colorbar()\n",
    "plt.quiver(ug_mean_M,vg_mean_M, scale=10, zorder=30, color='yellow', width=0.0025)\n",
    "plt.quiver(ug_mean_M2,vg_mean_M2, scale=5, zorder=30, color='magenta', width=0.0025)\n",
    "plt.contour(ds.isel(concat_dim=20)['depth_or_elevation'], levels=100, zorder=10, vmin=-2500, vmax=0, linewidths=2)\n",
    "plt.ylim(715,735)\n",
    "plt.xlim(5,25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(alignment, vmin=0, vmax=1, cmap='Grays')\n",
    "plt.quiver(ug_mean_M,vg_mean_M, scale=20, zorder=30, color='blue', width=0.0015)\n",
    "plt.quiver(ug_mean_M2,vg_mean_M2, scale=10, zorder=30, color='red', width=0.0015)\n",
    "plt.ylim(700,770)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "#plt.imshow(speed_mean_M, vmin=0, vmax=0.5, cmap=cmocean.cm.speed)#, vmin=0, vmax=0.5)\n",
    "plt.imshow(alignment, vmin=0, vmax=1, cmap='YlGnBu')#, vmin=0, vmax=0.5)\n",
    "plt.colorbar()\n",
    "#plt.quiver(ug_mean_M,vg_mean_M, scale=10, zorder=30, color='red', width=0.0015)\n",
    "plt.quiver(ug_mean_M,vg_mean_M, scale=15, zorder=30, color='yellow', width=0.0015)\n",
    "plt.quiver(ug_mean_M2,vg_mean_M2, scale=10, zorder=30, color='red', width=0.0015)\n",
    "plt.contour(ds.isel(concat_dim=20)['depth_or_elevation'], levels=50, zorder=10, vmin=-2000, vmax=0, linewidths=1)\n",
    "plt.ylim(700,770)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_covariance_cuu, local_covariance_cvv, local_covariance_cuv = calculate_local_covariance_for_each_track(masked_d, lat, lon, season_type, None)\n",
    "detc_L, trc_L, a_L, b_L, theta_L = calculate_det_trc_a_b_theta(local_covariance_cuu,local_covariance_cvv,local_covariance_cuv)\n",
    "polarized_component, grad_H_rot_90, mean_ug_vg_vector = calculate_polarized_vector_and_rotated_bathymetry_vector_and_mean_ssh_vector(masked_d_MEAN, a_L, b_L, theta_L, ds, 'no', 'yes')\n",
    "polarized_component, grad_H_rot_90, mean_ug_vg_vector_ssha = calculate_polarized_vector_and_rotated_bathymetry_vector_and_mean_ssh_vector(masked_d, a_L, b_L, theta_L, ds, 'no', 'yes')\n",
    "\n",
    "dot_product2, magnitude_a, magnitude_b, alignment = calculate_alignment(mean_ug_vg_vector, mean_ug_vg_vector_ssha)\n",
    "\n",
    "topostrophy1, topostrophy, orthostrophy = calculate_topostrophy_orthostrophy_of_variance(polarized_component, grad_H_rot_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment = dot_product2/(magnitude_a * magnitude_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "plt.imshow(alignment, vmin=0, vmax=1, cmap='YlGnBu')#, vmin=0, vmax=0.5)\n",
    "#plt.ylim(600,1000)\n",
    "#plt.ylim(650,750)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "plt.imshow(alignment, vmin=0, vmax=1, cmap='YlGnBu')#, vmin=0, vmax=0.5)\n",
    "plt.ylim(600,1000)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(topostrophy1, vmin=0, vmax=1, cmap='Spectral_r')#, vmin=0, vmax=0.5)\n",
    "#plt.ylim(650,750)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_cos = np.arccos(alignment)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(magnitude_b, vmin=0, vmax=1)#, cmap='Spectral_r')#, vmin=0, vmax=0.5)\n",
    "plt.ylim(650,750)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(magnitude_a, vmin=0, vmax=0.2)\n",
    "plt.ylim(650,750)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(dot_product2, vmin=0, vmax=0.5)\n",
    "plt.ylim(650,750)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(alignment)#, vmin=0, vmax=1.57)\n",
    "plt.ylim(650,750)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(theta_cos)#, vmin=0, vmax=0.02)\n",
    "plt.ylim(650,750)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cycles = len(dataset_merged.concat_dim.data)\n",
    "''' FINDING THE MONTH OF THE CYCLE : '''\n",
    "month_list = []\n",
    "for i in range(num_cycles):\n",
    "    year, month, day = finding_year_month_day_of_cycle(dataset_merged, i, 'int')\n",
    "    month_list.append(month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_int, month_int, day_int = SOFiA.finding_year_month_day_of_cycle(ds, 1, 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES INFORMATION :\n",
    "var_type         = 'ssha_karin_2'  # 'ssh_karin' , 'ssh_karin_2' , 'ssha_karin' , 'ssha_karin_2'\n",
    "correction_type  = 'ssha'          # 'ssh' , 'ssh_no_corr' , 'ssh_n_geoid' , 'ssha' , 'ssha_no_corr' , 'ssh_tide'\n",
    "mask_type        = 'all'           # 'all' , 'height_and_qual_flag' , 'qual_flag'\n",
    "smoothing        = None            #  None, 1, 3, ...\n",
    "season_type      = None            #  None, Winter, Summer, DFJ, ... --> NB! Correct this !!!\n",
    "\n",
    "if smoothing != None:\n",
    "    sm_name = str(smoothing)\n",
    "else:\n",
    "    sm_name = '0'\n",
    "\n",
    "if season_type != None:\n",
    "    seas_name = season_type\n",
    "else:\n",
    "    seas_name = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for raw files : \n",
    "path_n = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SWOT_1_RAW_MERGED/version_2'\n",
    "ds = xr.open_dataset(path_n + '/' + 'pass_003' + '.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_d, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type, correction_type, mask_type, smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DJF, MAM, JJA, SON, Winter, Summer, Winter_24, Winter_25, Summer_24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Files : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe_for_each_pass_mean_flow(path_for_pass, pass_number, var_type, correction_type, mask_type, smoothing, season_type, ug_by_phi=None, by_phi_or_not='no', rotated_to_eart_grid_or_not='yes'):\n",
    "    # Open passes : \n",
    "    #ds = xr.open_dataset(path_for_pass+pass_number)\n",
    "    ds = xr.open_dataset(path_for_pass + '/' + pass_number + '.nc')\n",
    "    # GET CORRECTED VAIABLE DATA : \n",
    "    #masked_d_ssha, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    masked_d_ssha, lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables_C0_C2_all_own_ssha(ds, var_type, correction_type, mask_type, smoothing)\n",
    "    #masked_d_ssh,  lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables2(ds, 'ssh_karin_2', 'ssh_tide', mask_type, smoothing)\n",
    "    masked_d_ssh,  lat, lon, month_list, year_list, Winter_24, Winter_25, Summer_24 = SOFiA.get_corrected_and_masked_variables_C0_C2_all_own_ssha(ds, 'ssh_karin_2', 'ssh_tide', mask_type, smoothing)\n",
    "    # Get cuu, cvv, cuv : \n",
    "    local_covariance_cuu, local_covariance_cvv, local_covariance_cuv = calculate_local_covariance_for_each_track(masked_d_ssha, lat, lon, season_type, None)\n",
    "    # Get a, b, theta : \n",
    "    detc_L, trc_L, a_L, b_L, theta_L = calculate_det_trc_a_b_theta(local_covariance_cuu,local_covariance_cvv,local_covariance_cuv)\n",
    "    # Get polarized component and mean ug/vg vector : \n",
    "    polarized_component, grad_H_rot_90, mean_ug_vg_vector      = calculate_polarized_vector_and_rotated_bathymetry_vector_and_mean_ssh_vector(masked_d_ssh, lat, lon, season_type, a_L, b_L, theta_L, ds, 'no', 'yes')\n",
    "    polarized_component, grad_H_rot_90, mean_ug_vg_vector_ssha = calculate_polarized_vector_and_rotated_bathymetry_vector_and_mean_ssh_vector(masked_d_ssha, lat, lon, season_type, a_L, b_L, theta_L, ds, 'no', 'yes')\n",
    "    ''' 1 ) MEAN SSH (TIDE) VS. TOPOGRAPHY : '''\n",
    "    dot_product1, magnitude_a1, magnitude_b1, alignment1 = calculate_alignment(mean_ug_vg_vector, grad_H_rot_90)\n",
    "    ''' 2 ) MEAN SSHA       VS. TOPOGRAPHY : '''\n",
    "    dot_product2, magnitude_a2, magnitude_b2, alignment2 = calculate_alignment(mean_ug_vg_vector_ssha, grad_H_rot_90)\n",
    "    ''' 3 ) MEAN SSH (TIDE) VS. MEAN SSHA  : '''\n",
    "    dot_product3, magnitude_a3, magnitude_b3, alignment3 = calculate_alignment(mean_ug_vg_vector, mean_ug_vg_vector_ssha)\n",
    "    ''' 4 ) MEAN SSH (TIDE) VS. POLA-RATIO : '''\n",
    "    dot_product4, magnitude_a4, magnitude_b4, alignment4 = calculate_alignment(mean_ug_vg_vector, polarized_component)\n",
    "    ''' 5 ) MEAN SSHA       VS. POLA-RATIO : '''\n",
    "    dot_product5, magnitude_a5, magnitude_b5, alignment5 = calculate_alignment(mean_ug_vg_vector_ssha, polarized_component)\n",
    "    ''' MAKE DATAFRAME : '''\n",
    "    # Fix the Longitude : \n",
    "    new_lon = np.where(lon > 180, lon - 360, lon)\n",
    "    # Num_lines, Num_pixels :\n",
    "    num_lines  = lat.shape[0]\n",
    "    num_pixels = lat.shape[1]\n",
    "    ''' CREATE DataArrays '''\n",
    "    df1_dot_product1 = xr.DataArray(dot_product1, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    df1_magnitude_a1 = xr.DataArray(magnitude_a1, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    df1_magnitude_b1 = xr.DataArray(magnitude_b1, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    df1_alignment1   = xr.DataArray(alignment1,   dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "\n",
    "    df2_dot_product2 = xr.DataArray(dot_product2, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    df2_magnitude_a2 = xr.DataArray(magnitude_a2, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    df2_magnitude_b2 = xr.DataArray(magnitude_b2, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    df2_alignment2   = xr.DataArray(alignment2,   dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "\n",
    "    df3_dot_product3 = xr.DataArray(dot_product3, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    df3_magnitude_a3 = xr.DataArray(magnitude_a3, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    df3_magnitude_b3 = xr.DataArray(magnitude_b3, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    df3_alignment3   = xr.DataArray(alignment3,   dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "\n",
    "    df4_dot_product4 = xr.DataArray(dot_product4, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    df4_magnitude_a4 = xr.DataArray(magnitude_a4, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    df4_magnitude_b4 = xr.DataArray(magnitude_b4, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    df4_alignment4   = xr.DataArray(alignment4,   dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "\n",
    "    df5_dot_product5 = xr.DataArray(dot_product5, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    df5_magnitude_a5 = xr.DataArray(magnitude_a5, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    df5_magnitude_b5 = xr.DataArray(magnitude_b5, dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    df5_alignment5   = xr.DataArray(alignment5,   dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "\n",
    "    loc_pol_x   = xr.DataArray(polarized_component[0],      dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    loc_pol_y   = xr.DataArray(polarized_component[1],      dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ssh_x       = xr.DataArray(mean_ug_vg_vector[0],        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ssh_y       = xr.DataArray(mean_ug_vg_vector[1],        dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ssha_x      = xr.DataArray(mean_ug_vg_vector_ssha[0],   dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ssha_y      = xr.DataArray(mean_ug_vg_vector_ssha[1],   dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    grad_H_90_x = xr.DataArray(grad_H_rot_90[0],            dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    grad_H_90_y = xr.DataArray(grad_H_rot_90[1],            dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "\n",
    "    lat_df      = xr.DataArray(lat,                    dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    new_lon_df  = xr.DataArray(new_lon,                dims=('num_lines', 'num_pixels'), coords={'num_lines': np.arange(num_lines), 'num_pixels': np.arange(num_pixels)})\n",
    "    ''' CREATE DataSet '''\n",
    "    dataset1  = xr.Dataset({'dot_product'    : df1_dot_product1,\n",
    "                            'magnitude_SSHT' : df1_magnitude_a1,\n",
    "                            'magnitude_TOPO' : df1_magnitude_b1,\n",
    "                            'alignment'      : df1_alignment1,\n",
    "                            'lat_df'         : lat_df,\n",
    "                            'new_lon_df'     : new_lon_df})\n",
    "    dataset2  = xr.Dataset({'dot_product'    : df2_dot_product2,\n",
    "                            'magnitude_SSHA' : df2_magnitude_a2,\n",
    "                            'magnitude_TOPO' : df2_magnitude_b2,\n",
    "                            'alignment'      : df2_alignment2,\n",
    "                            'lat_df'         : lat_df,\n",
    "                            'new_lon_df'     : new_lon_df})\n",
    "    dataset3  = xr.Dataset({'dot_product'    : df3_dot_product3,\n",
    "                            'magnitude_SSHT' : df3_magnitude_a3,\n",
    "                            'magnitude_SSHA' : df3_magnitude_b3,\n",
    "                            'alignment'      : df3_alignment3,\n",
    "                            'lat_df'         : lat_df,\n",
    "                            'new_lon_df'     : new_lon_df})\n",
    "    dataset4  = xr.Dataset({'dot_product'    : df4_dot_product4,\n",
    "                            'magnitude_SSHT' : df4_magnitude_a4,\n",
    "                            'magnitude_PORA' : df4_magnitude_b4,\n",
    "                            'alignment'      : df4_alignment4,\n",
    "                            'lat_df'         : lat_df,\n",
    "                            'new_lon_df'     : new_lon_df})\n",
    "    dataset5  = xr.Dataset({'dot_product'    : df5_dot_product5,\n",
    "                            'magnitude_SSHA' : df5_magnitude_a5,\n",
    "                            'magnitude_PORA' : df5_magnitude_b5,\n",
    "                            'alignment'      : df5_alignment5,\n",
    "                            'lat_df'         : lat_df,\n",
    "                            'new_lon_df'     : new_lon_df})\n",
    "    dataset6  = xr.Dataset({'loc_pol_x'   : loc_pol_x,\n",
    "                            'loc_pol_y'   : loc_pol_y,\n",
    "                            'ssh_x'       : ssh_x,\n",
    "                            'ssh_y'       : ssh_y,\n",
    "                            'ssha_x'      : ssha_x,\n",
    "                            'ssha_y'      : ssha_y,\n",
    "                            'grad_H_90_x' : grad_H_90_x,\n",
    "                            'grad_H_90_y' : grad_H_90_y,\n",
    "                            'lat_df'      : lat_df,\n",
    "                            'new_lon_df'  : new_lon_df})\n",
    "    ''' RETURN THE DataSet '''\n",
    "    return dataset1, dataset2, dataset3, dataset4, dataset5, dataset6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_SOFiA_values_into_dataset_MEAN_ali(dataset, var_a_name, var_b_name):\n",
    "    # GET VALUES : \n",
    "    dot = dataset['dot_product']\n",
    "    vra = dataset[var_a_name]\n",
    "    vrb = dataset[var_b_name]\n",
    "    ali = dataset['alignment']\n",
    "    lat = dataset['lat_df']\n",
    "    lon = dataset['new_lon_df']\n",
    "    # UNRAVEL : \n",
    "    dot = np.ravel(dot)\n",
    "    vra = np.ravel(vra)\n",
    "    vrb = np.ravel(vrb)\n",
    "    ali = np.ravel(ali)\n",
    "    lat = np.ravel(lat)\n",
    "    lon = np.ravel(lon)\n",
    "    # CREATE DATAFRAME :\n",
    "    dataset = pd.DataFrame({'dot_product' : dot,\n",
    "                            var_a_name    : vra,\n",
    "                            var_b_name    : vrb,\n",
    "                            'alignment'   : ali,\n",
    "                            'latitude'    : lat,\n",
    "                            'longitude'   : lon})\n",
    "    # Return dataset :\n",
    "    return dataset\n",
    "\n",
    "def unravel_SOFiA_values_into_dataset_MEAN_vector(dataset):\n",
    "    # GET VALUES : \n",
    "    polx  = dataset['loc_pol_x']\n",
    "    poly  = dataset['loc_pol_y']\n",
    "    sshx  = dataset['ssh_x']\n",
    "    sshy  = dataset['ssh_y']\n",
    "    sshax = dataset['ssha_x']\n",
    "    sshay = dataset['ssha_y']\n",
    "    gradx = dataset['grad_H_90_x']\n",
    "    grady = dataset['grad_H_90_y']\n",
    "    lat   = dataset['lat_df']\n",
    "    lon   = dataset['new_lon_df']\n",
    "    # UNRAVEL : \n",
    "    polx  = np.ravel(polx)\n",
    "    poly  = np.ravel(poly)\n",
    "    sshx  = np.ravel(sshx)\n",
    "    sshy  = np.ravel(sshy)\n",
    "    sshax = np.ravel(sshax)\n",
    "    sshay = np.ravel(sshay)\n",
    "    gradx = np.ravel(gradx)\n",
    "    grady = np.ravel(grady)\n",
    "    lat   = np.ravel(lat)\n",
    "    lon   = np.ravel(lon)\n",
    "    # CREATE DATAFRAME :\n",
    "    dataset = pd.DataFrame({'loc_pol_x'   : polx,\n",
    "                            'loc_pol_y'   : poly,\n",
    "                            'ssh_x'       : sshx,\n",
    "                            'ssh_y'       : sshy,\n",
    "                            'ssha_x'      : sshax,\n",
    "                            'ssha_y'      : sshay,\n",
    "                            'grad_H_90_x' : gradx,\n",
    "                            'grad_H_90_y' : grady,\n",
    "                            'latitude'    : lat,\n",
    "                            'longitude'   : lon})\n",
    "    # Return dataset :\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for raw files : \n",
    "path_n = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SWOT_1_RAW_MERGED/version_2'\n",
    "# Path to save calculation : \n",
    "original_path_for_files = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SOFiA/SOFiA_NETCDF/mean_datasets_karin_2/smoothing_0'\n",
    "\n",
    "for pass_number in list_of_passes:\n",
    "    print(pass_number)\n",
    "    save_dataset_of_calc_NETCDF1 = original_path_for_files +  '/' + 'SOFiA_mean_1_' + pass_number + '.nc'\n",
    "    save_dataset_of_calc_NETCDF2 = original_path_for_files +  '/' + 'SOFiA_mean_2_' + pass_number + '.nc'\n",
    "    save_dataset_of_calc_NETCDF3 = original_path_for_files +  '/' + 'SOFiA_mean_3_' + pass_number + '.nc'\n",
    "    save_dataset_of_calc_NETCDF4 = original_path_for_files +  '/' + 'SOFiA_mean_4_' + pass_number + '.nc'\n",
    "    save_dataset_of_calc_NETCDF5 = original_path_for_files +  '/' + 'SOFiA_mean_5_' + pass_number + '.nc'\n",
    "    save_dataset_of_calc_NETCDF6 = original_path_for_files +  '/' + 'SOFiA_mean_6_' + pass_number + '.nc'\n",
    "\n",
    "    dataset1, dataset2, dataset3, dataset4, dataset5, dataset6 = create_dataframe_for_each_pass_mean_flow(path_n, pass_number, var_type, correction_type, mask_type, smoothing, season_type, ug_by_phi=None, by_phi_or_not='no', rotated_to_eart_grid_or_not='yes')\n",
    "    \n",
    "    dataset1.to_netcdf(save_dataset_of_calc_NETCDF1)\n",
    "    dataset2.to_netcdf(save_dataset_of_calc_NETCDF2)\n",
    "    dataset3.to_netcdf(save_dataset_of_calc_NETCDF3)\n",
    "    dataset4.to_netcdf(save_dataset_of_calc_NETCDF4)\n",
    "    dataset5.to_netcdf(save_dataset_of_calc_NETCDF5)\n",
    "    dataset6.to_netcdf(save_dataset_of_calc_NETCDF6)\n",
    "    \n",
    "    print(save_dataset_of_calc_NETCDF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for raw files : \n",
    "path_n = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SWOT_1_RAW_MERGED/version_2'\n",
    "# Path to save calculation : \n",
    "original_path_for_files = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SOFiA/SOFiA_NETCDF/mean_datasets_karin_2/smoothing_0'\n",
    "original_path_for_files_csv = '/uio/hypatia/geofag-personlig/students/metos/ingvilob/SOFiA/SOFiA_CSV/mean_datasets_karin_2/smoothing_0'\n",
    "\n",
    "for pass_number in list_of_passes:\n",
    "    print(pass_number)\n",
    "    dataset1 = xr.open_dataset(original_path_for_files + '/' + 'SOFiA_mean_1_' + pass_number + '.nc')\n",
    "    dataset2 = xr.open_dataset(original_path_for_files + '/' + 'SOFiA_mean_2_' + pass_number + '.nc')\n",
    "    dataset3 = xr.open_dataset(original_path_for_files + '/' + 'SOFiA_mean_3_' + pass_number + '.nc')\n",
    "    dataset4 = xr.open_dataset(original_path_for_files + '/' + 'SOFiA_mean_4_' + pass_number + '.nc')\n",
    "    dataset5 = xr.open_dataset(original_path_for_files + '/' + 'SOFiA_mean_5_' + pass_number + '.nc')\n",
    "    dataset6 = xr.open_dataset(original_path_for_files + '/' + 'SOFiA_mean_6_' + pass_number + '.nc')\n",
    "\n",
    "    dataset_1 = unravel_SOFiA_values_into_dataset_MEAN_ali(dataset1, 'magnitude_SSHT', 'magnitude_TOPO')\n",
    "    dataset_2 = unravel_SOFiA_values_into_dataset_MEAN_ali(dataset2, 'magnitude_SSHA', 'magnitude_TOPO')\n",
    "    dataset_3 = unravel_SOFiA_values_into_dataset_MEAN_ali(dataset3, 'magnitude_SSHT', 'magnitude_SSHA')\n",
    "    dataset_4 = unravel_SOFiA_values_into_dataset_MEAN_ali(dataset4, 'magnitude_SSHT', 'magnitude_PORA')\n",
    "    dataset_5 = unravel_SOFiA_values_into_dataset_MEAN_ali(dataset5, 'magnitude_SSHA', 'magnitude_PORA')\n",
    "    dataset_6 = unravel_SOFiA_values_into_dataset_MEAN_vector(dataset6)\n",
    "\n",
    "    # Save the dataframe to a csv file\n",
    "    save_csv_name_1 = original_path_for_files_csv + '/' + 'SOFiA_mean_1_' + pass_number + '.csv'\n",
    "    save_csv_name_2 = original_path_for_files_csv + '/' + 'SOFiA_mean_2_' + pass_number + '.csv'\n",
    "    save_csv_name_3 = original_path_for_files_csv + '/' + 'SOFiA_mean_3_' + pass_number + '.csv'\n",
    "    save_csv_name_4 = original_path_for_files_csv + '/' + 'SOFiA_mean_4_' + pass_number + '.csv'\n",
    "    save_csv_name_5 = original_path_for_files_csv + '/' + 'SOFiA_mean_5_' + pass_number + '.csv'\n",
    "    save_csv_name_6 = original_path_for_files_csv + '/' + 'SOFiA_mean_6_' + pass_number + '.csv'\n",
    "\n",
    "    # SAVE :\n",
    "    dataset_1.to_csv(save_csv_name_1, index=False)\n",
    "    dataset_2.to_csv(save_csv_name_2, index=False)\n",
    "    dataset_3.to_csv(save_csv_name_3, index=False)\n",
    "    dataset_4.to_csv(save_csv_name_4, index=False)\n",
    "    dataset_5.to_csv(save_csv_name_5, index=False)\n",
    "    dataset_6.to_csv(save_csv_name_6, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
